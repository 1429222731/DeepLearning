#  第一章 深度学习简介：为什么应该学习深度学习

##  一、为什么要学习深度学习？

1.它是智能增量自动化的强大工具

2.深度学习具有使用熟练劳动力实现重大自动化的潜力（代替人类的劳动）

3.它有趣又有创意。通过对智力和创造力的模拟，你会发现许多人类的秘密



**你得付出多少努力才能获得“乐趣”？**

**我对获得“乐趣”作为汇报的定义是能够亲眼看到我学到的东西变成现实。**



深度学习的教学材料大致有两类：

一类是关注如何使用流行的框架和代码库，如：Torch、TensorFlow、Keras等

另一类是专注讲解深度学习本身，也就是这些框架之下的科学原理

## 二、准备工作

1.安装Jupyter Notebook和NumPy、Python库

安装说明（http://Jupyter.org以及http://numpy.org）

推荐使用Anaconda框架（http://docs.continuum.io/anaconda/install）如果你是小白，那么建议你通过安装Anaconda来解决Jupyter Notebook的安装问题，因为Anaconda已经自动为你安装了Jupter Notebook及其他工具，还有python中超过180个科学包及其依赖项

2.掌握一部分python知识

本书中的案例源码在（www.manning.com/books/grokking-deep-learning和http://github.com/iamtrask/Grokking-Deep_Learning）

#  第二章 基本概念：机器该如何学习？

## 一、什么是深度学习？

1.深度学习是机器学习的一个子集。

2.深度学习被用于解决多个领域的实际任务，如计算机视觉（图像）、自然语言处理（文本）、自动语音识别（音频）。

##  二、什么是机器学习？

1.机器能够执行那些没有被显式编程的任务。（即：机器观察某项任务中存在的模式，并试图以某种直接或间接的方式模仿它。）

2.机器学习  ~≈  先观察，后照做

##  三、监督机器学习

1.监督机器学习对数据集进行转换。即监督学习是一种将一个数据集转换成另一个数据集的方法。（进行预测）

2.监督机器学习属于应用人工智能（也被成为弱人工智能）领域的一部分。对于将你所知道的东西作为输入，并快速地将其转换成你想知道的东西这一场景来说，它使很有用的。（预测）

3.你知道的----->监督学习----->你想知道的（主要就是预测）

## 四、无监督机器学习

1.无监督机器学习对数据进行分组。

2.可以把聚类作为无监督机器学习的定义记在脑子中。（同一类别进行分组）

3.小狗                                                                                                        1

   披萨                                                                                                        2

   小猫           ----------------->         **无监督机器学习**     ---------------->       1

   热狗                                                                                                        2

   汉堡                                                                                                        2

   聚类（1==可爱的，2==美味的）

##  五、参数学习和非参数学习

1.简化表述：   试错学习  **VS**  计数和概率

## 六、监督参数学习、无监督参数学习、非参数学习

1.监督参数学习

简化描述：使用旋钮进行试错学习（机器根据旋钮的角度对输入数据进行处理，并转换为预测结果）

三个步骤：预测--->与真值进行比较---->学习模式

2.无监督参数学习

简化描述：使用旋钮对数据进行分组。通常为每个类别都设置了几个旋钮，每个旋钮都反映了输入数据到那个特定类别的相似度。

3.非参数学习

简化描述：基于计数的方法

非参数学习是一类参数个数以数据为基础（而不是预先定义好）的算法

#  第三章 神经网络预测导论：前向传播

## 一、神经网络

1.神经网络：用权重乘以输入数据进行预测。（预测是神经网络对输入数据进行“思考”之后告诉你结果。预测不一定总是对的，它会调整自己的权重，以获得更准确的预测值。）

2.神经网络做了什么：它将输入乘以权重，将输入“缩放”一定的比例。

3.神经网络的交互界面很简单。它接受输入变量，并以此作为信息来源；拥有权重变量，以此作为知识；然后，融合信息和知识，输出预测结果。

4.神经网络不仅可以预测正数，还可以预测复数，甚至还可以将复数作为输入。

5.First_neural.ipynb   第一个神经网络（使用Jupyter Notebook）

```python
#网络代码
weight=0.1
def neural_network(input,weight):
    prediction=input*weight
    return prediction

#使用训练好的神经网络进行预测
number_of_toes=[8.5,9.5,10,9]
input=number_of_toes[0]
pred=neural_network(input,weight)
print(pred)

#结果
0.8500000000000001
```

##  二、进行多输入的预测

1.神经网络可以融合多个数据点的智能。

2.多个输入：这个神经网络做了什么？------->它将每个输入乘以其各自的权重，然后对所有局部预测结果进行求和，称为：输入的加权和或加权和或点积。

3.点积（加权和）的工作原理是真正理解神经网络如何做出预测的最重要部分之一。

4.挑战向量操作：**（待完成）**

```python
def elementwise_multiplication(vec_a,vec_b)
def elementwise_addtion(vec_a,vec_b)
def vector_sum(vec_a)
def vector_average(vec_a)
```

5.Second_neural.ipynb  第二个神经网络（多个输入）

```python
#网络代码
def w_sum(a,b):
    assert(len(a)==len(b))
    output=0
    for i in range((len(a))):
        output+=(a[i]*b[i])
    return output

weights=[0.1,0.2,0]

def neural_network(input,weights):
    pred=w_sum(input,weights)
    return pred


#使用训练好的神经网络进行预测
toes=[8.5,9.5,9.9,9.0]
wlrec=[0.65,0.8,0.8,0.9]
nfans=[1.2,1.3,0.5,1.0]

input=[toes[0],wlrec[0],nfans[0]]
pred=neural_network(input,weights)
print(pred)

#结果
0.9800000000000001
```



**Numpy代码：**其中Numpy中有一个dot（dot product）函数，指的是点乘

```python
import numpy as np
weights=np.array([0.1,0.2,0])
def neural_network(input,weight):
    pred=input.dot(weights)
    return pred

toes=np.array([8.5,9.5,9.9,9.0])
wlrec=np.array([0.65,0.8,0.8,0.9])
nfans=np.array([1.2,1.3,0.5,1.0])

input=np.array([toes[0],wlrec[0],nfans[0]])

pred=neural_network(input,weights)
print(pred)

#结果
0.9800000000000001
```

##  三、进行多输出的预测

1.神经网络可以只用一个输入做出多个预测。

2.表现为多个独立的子网络；每个子网络接收相同的数据。

3.Third_neural.ipynb  第三个神经网络（一个输入来预测多个输出）

```python
#网络代码
def ele_mul(number,vector):
    output=[0,0,0]
    assert(len(output)==len(vector))
    for i in range(len(vector)):
        output[i]=number*vector[i]
    return output

weights=[0.3,0.2,0.9]
def neural_network(input,weights):
    pred=ele_mul(input,weights)
    return pred


#使用训练好的神经网络进行预测
wlrec=[0.65,0.8,0.8,0.9]
input=wlrec[0]
pred=neural_network(input,weights)
print(pred)

#结果
[0.195, 0.13, 0.5850000000000001]
```

##  四、进行多输入和多输出的预测

1.基于给定的多个输入，神经网络可以对多个输出进行预测。

2.向量的数组称为矩阵。

3.将权重处理/存储为列向量。

4.Fourth_neural.ipynb  第四个神经网络（多个输入和多个输出）

   **对输入数据执行三次独立的加权和操作，产生是三个预测结果**

```python
#网络代码
def w_sum(a,b):
    assert(len(a)==len(b))
    output=0
    for i in range(len(a)):
        output+=(a[i]*b[i])
    return output

def vect_mat_mul(vect,matrix):
    assert(len(vect)==len(matrix))
    output=[0,0,0]
    for i in range(len(vect)):
        output[i]=w_sum(vect,matrix[i])
    return output

weights=[[0.1,0.1,-0.3],  #是否受伤
         [0.1,0.2,0.0],   #是否胜利
         [0.0,1.3,0.1]]   #是否难过

def neural_network(input,weights):
    pred=vect_mat_mul(input,weights)
    return pred

#使用训练好的神经网络进行预测
toes=[8.5,9.5,9.9,9.0]
wlrec=[0.65,0.8,0.8,0.9]
nfans=[1.2,1.3,0.5,1.0]

input=[toes[0],wlrec[0],nfans[0]]
pred=neural_network(input,weights)
print(pred)

#结果
[0.555, 0.9800000000000001, 0.9650000000000001]
```

##  五、对预测进行预测

1.神经网络可以堆叠。

2.可以将一个网络的输出提供给另一个网络作为输入。（相当于两个连续的向量矩阵乘法）

3.Fifth_neural.ipynb   第五个神经网络

```python
#网络代码
def w_sum(a,b):
    assert(len(a)==len(b))
    output=0
    for i in range(len(a)):
        output+=(a[i]*b[i])
    return output

def vect_mat_mul(vect,matrix):
    assert(len(vect)==len(matrix))
    output=[0,0,0]
    for i in range(len(vect)):
        output[i]=w_sum(vect,matrix[i])
    return output

           #toes %win #fans
ih_wgt = [ [0.1, 0.2, -0.1], #hid[0]
           [-0.1,0.1, 0.9],  #hid[1]
           [0.1, 0.4, 0.1]]  #hid[2]

           #hid[0] hid[1] hid[2]
hp_wgt = [ [0.3, 1.1, -0.3], #hurt?
           [0.1, 0.2, 0.0],  #win?
           [0.0, 1.3, 0.1]]  #sad?

weights = [ih_wgt, hp_wgt]

def neural_network(input, weights):
    hid = vect_mat_mul(input,weights[0])
    pred = vect_mat_mul(hid,weights[1])
    return pred

#使用训练好的神经网络进行预测
toes =  [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65,0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

input = [toes[0],wlrec[0],nfans[0]]
pred = neural_network(input,weights)
print(pred)

#结果
[0.21350000000000002, 0.14500000000000002, 0.5065]
```



**Numpy版本:**（T操作对矩阵进行转置）

```python
import numpy as np

#toes %win #fans
ih_wgt = np.array([ 
            [0.1, 0.2, -0.1], #hid[0]
            [-0.1,0.1, 0.9], #hid[1]
            [0.1, 0.4, 0.1]]).T #hid[2]

# hid[0] hid[1] hid[2]
hp_wgt = np.array([  
            [0.3, 1.1, -0.3], #hurt?
            [0.1, 0.2, 0.0], #win?
            [0.0, 1.3, 0.1] ]).T #sad?

weights = [ih_wgt, hp_wgt]

def neural_network(input, weights):

    hid = input.dot(weights[0])
    pred = hid.dot(weights[1])
    return pred


toes =  np.array([8.5, 9.5, 9.9, 9.0])
wlrec = np.array([0.65,0.8, 0.8, 0.9])
nfans = np.array([1.2, 1.3, 0.5, 1.0])

input = np.array([toes[0],wlrec[0],nfans[0]])
pred = neural_network(input,weights)
print(pred)

#结果
[0.2135 0.145  0.5065]
```

##  六、Numpy快速入门

1.做任何逐元操作（包括+、-、*、/）的两个变量必须具有相同数量的列，或者其中一个变量只有一列。

2.T操作是对矩阵进行转置。

3.例子一：

```python
import numpy as np
a=np.array([0,1,2,3]) #一个向量
b=np.array([4,5,6,7]) #另一个向量
c=np.array([[0,1,2,3],#一个矩阵
           [4,5,6,7]])

d=np.zeros((2,4))     #2*4的全零矩阵
e=np.random.rand(2,5) #2*5的随机矩阵，元素的值在0到1之间

print(a)
print()

print(b)
print()

print(c)
print()

print(d)
print()

print(e)

#结果
[0 1 2 3]

[4 5 6 7]

[[0 1 2 3]
 [4 5 6 7]]

[[0. 0. 0. 0.]
 [0. 0. 0. 0.]]

[[0.57098209 0.13800791 0.7065916  0.77809562 0.8438687 ]
 [0.8953357  0.66488207 0.00939334 0.10181642 0.09276199]]
```

#  第四章：神经网络学习导论：梯度下降

##  一、预测、比较和学习

1.比较：比较为预测的“误差”提供度量。(本章中只介绍一种测量误差方法：均方误差)

2.学习：“学习”告诉权重应该如何改变以降低误差。（本章中学习最流行的“踢皮球”方法：梯度下降）

3.测量误差能够简化问题。（训练神经网络的目的就是做出正确的预测）

4.在测量误差的不同方法中，误差的优先级不同。（训练过程能够修正你对误差的认识，放大那些较大的误差，并忽略那些较小的误差）

5.误差只需要正的。（如果有负的误差，那么它会和正的相互抵消，会让人认为是自己预测很完美。）

6.Neural_Network_01.ipynb    (对“误差的量”进行平方运算的主要原因是使原始误差的值变为正数，值为负数的误差没有意义。)

```python
knod_weight=0.5
input=0.5
goal_pred=0.8

pred=input*knod_weight

error=(pred-goal_pred)**2
print(error)

#结果
0.30250000000000005
```

##  二、冷热法学习

1.冷热学习：冷热学习指的是通过扰动权重来确定向哪个方向调整可以使得误差的降低幅度最大，基于此将权重的值向那个方向移动，不断重复这个过程，指导误差降趋于0。

2.神经网络学习的本质：搜索。

3.Neural_Network_02.ipynb    (冷热学习)

```python
weight=0.5
input=0.5
goal_prediction=0.8

#对每一个迭代，权重应该进行多大幅度的调节
step_amount=0.001

for i in range(1101):
    prediction=input*weight
    error=(prediction-goal_prediction)**2
    
    print("Error:"+str(error)+"    "+"Prediction:"+str(prediction))
    print()
    
    #试一下提升权重
    up_prediction=input*(weight+step_amount)
    up_error=(goal_prediction-up_prediction)**2
    
    #试一下降低权重
    down_prediction=input*(weight-step_amount)
    down_error=(goal_prediction-down_prediction)**2
    
    #如果降低权重的结果更好，那么将权重的结果调低
    if (down_error<up_error):
        weight=weight-step_amount
        
    #如果升高权重的结果更好，那么将权重的结果升高
    if (down_error>up_error):
        weight=weight+step_amount
        
        
#结果  趋于0.8
Error:0.30250000000000005    Prediction:0.25

Error:0.3019502500000001    Prediction:0.2505

Error:0.30140100000000003    Prediction:0.251

Error:0.30085225    Prediction:0.2515
...........................................................
Error:1.000000000065505e-06    Prediction:0.7989999999999673

Error:2.5000000003280753e-07    Prediction:0.7994999999999672

Error:1.0799505792475652e-27    Prediction:0.7999999999999672
```

4.冷热学习：

- 简单（每次模型进行三次预测，选取好的权重，然后继续迭代重复这一过程，直到最终误差降为0）
- 效率低下:多次预测才能进行一次权重的更新
- 有时准确预测出目标是不可能的。
- 即使知道调节权重的正确方向，也没办法指导正确的幅度。
- step_amount是任意选择的，可能学习不到正确的权重值。

##  三、基于误差计算方向和幅度

1.Neural_Network_03.ipynb      

```python
#direction_and_amount=(pred-goal_pred)*input  这一行同时进行方向和幅度的计算

weight=0.5
input=0.5
goal_pred=0.8

for iteration in range(20):
    pred=input*weight
    error=(pred-goal_pred)**2
    
    direction_and_amount=(pred-goal_pred)*input
    weight=weight-direction_and_amount
    print("Error:"+str(error)+"   "+"Prediction:"+str(pred))

#结果
Error:0.30250000000000005   Prediction:0.25
Error:0.17015625000000004   Prediction:0.3875
Error:0.095712890625   Prediction:0.49062500000000003
Error:0.05383850097656251   Prediction:0.56796875
Error:0.03028415679931642   Prediction:0.6259765625
Error:0.0170348381996155   Prediction:0.669482421875
Error:0.00958209648728372   Prediction:0.70211181640625
Error:0.005389929274097089   Prediction:0.7265838623046875
Error:0.0030318352166796153   Prediction:0.7449378967285156
Error:0.0017054073093822882   Prediction:0.7587034225463867
Error:0.0009592916115275371   Prediction:0.76902756690979
Error:0.0005396015314842384   Prediction:0.7767706751823426
Error:0.000303525861459885   Prediction:0.7825780063867569
Error:0.00017073329707118678   Prediction:0.7869335047900676
Error:9.603747960254256e-05   Prediction:0.7902001285925507
Error:5.402108227642978e-05   Prediction:0.7926500964444131
Error:3.038685878049206e-05   Prediction:0.7944875723333098
Error:1.7092608064027242e-05   Prediction:0.7958656792499823
Error:9.614592036015323e-06   Prediction:0.7968992594374867
Error:5.408208020258491e-06   Prediction:0.7976744445781151      
```

direction_and_amount(方向和数量)是什么？（direction_and_amount指的是我们希望如何更改权重。）

**由两部分组成：**

1. **纯误差**：(pred-goal_pred)。也就是预测和真实值之间的差值。----->表示当前错误的原始方向和幅度。如果是正数，预测就太高了；反之亦然。如果它使一个很大的数字，那么你就错了很多。
2. **与输入的相乘操作**：(pred-goal_pred)*input。用于执行缩放、负值反转和停止调节，对纯误差进行修正以调节权重。（缩放、负值反转和停止调节这三个属性的共同作用就是将纯误差转换为我们需要的权重调节的绝对幅度。）

##  四、梯度下降

1.Neural_Network_04.ipynb   (对单个训练示例执行权重更新)

```python
# 1) An Empty Network

weight = 0.1 
alpha = 0.01

def neural_network(input, weight):
    prediction = input * weight
    return prediction

# 2) PREDICT: Making A Prediction And Evaluating Error

number_of_toes = [8.5]
win_or_lose_binary = [1] # (won!!!)

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input,weight)
error = (pred - goal_pred) ** 2

# 3) COMPARE: Calculating "Node Delta" and Putting it on the Output Node

delta = pred - goal_pred

# 4) LEARN: Calculating "Weight Delta" and Putting it on the Weight
#weight_delta是一个用于度量权重所导致的网络犯错的指标。
weight_delta = input * delta

# 5) LEARN: Updating the Weight

alpha = 0.01 # fixed before training
weight -= weight_delta * alpha


#此种情况下 input=0.5   weight, goal_pred, input = (0.0, 0.8, 0.5)
weight, goal_pred, input = (0.0, 0.8, 0.5)

for iteration in range(4):
    
    pred = input * weight
    error = (pred - goal_pred) ** 2
    delta = pred - goal_pred
    weight_delta = delta * input
    weight = weight - weight_delta
    print("Error:" + str(error) + " Prediction:" + str(pred))
    
#结果
Error:0.6400000000000001 Prediction:0.0
Error:0.3600000000000001 Prediction:0.2
Error:0.2025 Prediction:0.35000000000000003
Error:0.11390625000000001 Prediction:0.4625
        
        
#此种情况下 input=1.1   weight, goal_pred, input = (0.0, 0.8, 1.1)
weight, goal_pred, input = (0.0, 0.8, 1.1)

for iteration in range(4):
    print("-----\nWeight:" + str(weight))
    
    pred = input * weight
    error = (pred - goal_pred) ** 2
    delta = pred - goal_pred
    weight_delta = delta * input
    weight = weight - weight_delta
    print("Error:" + str(error) + " Prediction:" + str(pred))
    
    print("Delta:" + str(delta) + " Weight Delta:" + str(weight_delta))
    
#结果
-----
Weight:0.0
Error:0.6400000000000001 Prediction:0.0
Delta:-0.8 Weight Delta:-0.8800000000000001
-----
Weight:0.8800000000000001
Error:0.02822400000000005 Prediction:0.9680000000000002
Delta:0.16800000000000015 Weight Delta:0.1848000000000002
-----
Weight:0.6951999999999999
Error:0.0012446784000000064 Prediction:0.76472
Delta:-0.03528000000000009 Weight Delta:-0.0388080000000001
-----
Weight:0.734008
Error:5.4890317439999896e-05 Prediction:0.8074088
Delta:0.007408799999999993 Weight Delta:0.008149679999999992
```

##  五、权重增量到底是什么？

1.权重。

- 调整它不会改变你对世界的感知,不会改变你的目标,也不会破坏你的误差测量方法。

- 改变权重表示函数正在试图匹配数据中的模式。
- 通过强迫函数剩下的部分保持不变,我们需要使函数能够正确地对数据中存在的某些模式建模。
- 它只允许修改网络进行预测的方式。

2.学习就是调整权重，将误差减小到0。

3.敏感度是方向和数量的另一个名称。

4.error=((input-weight)-goal_pred)**2     这是一个公式。它告诉error和weight之间存在的关系。这个关系是精确的。它是可计算的，也是普遍存在的。现在如此，将来也永远如此。

##  六、导数与如何用导数学习

1.给定一个函数，如果改变其中一个变量，则它的导数代表了另一个变量发生变化的方向和幅度。

2.神经网络就是一件事情：一堆你可以用来计算误差函数的权重。（通过改变神经网络中的每一个weight，将error减小到0。）

3.导数：表示某个函数中两个变量之间的关系，当你改变其中一个变量的时候，你据此知道另一个将会如何改变。它只是两个变量之间的影响的敏感度。

4.weight_delta(误差增量)就是你的导数。

5.**需要记住:**为使得误差减小,我们尽力去寻找权重 应该变化的方向和幅度。导数给出了函数中任意两个变量之间的关系。我们可以 用导数来确定权重和误差之间的关系。然后把权重的值向导数的相反方向移动, 就能得到使误差更小的权重值。瞧!这就是神经网络学习。
**这种学习(寻找误差最小值)的方法称为梯度下降法**。即将权重值往梯度(导数)值相反方向移动,可以使error趋向于0。

6.Neural_Network_05.ipynb  (破坏梯度下降对比)

```python
weight = 0.1 
alpha = 0.01

def neural_network(input, weight):
    prediction = input * weight
    return prediction

number_of_toes = [8.5]
win_or_lose_binary = [1] # (won!!!)

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input,weight)
error = (pred - goal_pred) ** 2

delta = pred - goal_pred

weight_delta = input * delta


alpha = 0.01 
weight -= weight_delta * alpha


#没破坏之前
# weight = 0.5
# goal_pred = 0.8
# input = 0.5

# for iteration in range(20):
#     pred = input * weight
#     error = (pred - goal_pred) ** 2
#     delta = pred - goal_pred
#     weight_delta = input * delta
#     weight = weight - weight_delta
#     print("Error:" + str(error) + " Prediction:" + str(pred))

#没破坏之前结果
Error:0.30250000000000005 Prediction:0.25
Error:0.17015625000000004 Prediction:0.3875
Error:0.095712890625 Prediction:0.49062500000000003
Error:0.05383850097656251 Prediction:0.56796875
Error:0.03028415679931642 Prediction:0.6259765625
Error:0.0170348381996155 Prediction:0.669482421875
Error:0.00958209648728372 Prediction:0.70211181640625
Error:0.005389929274097089 Prediction:0.7265838623046875
Error:0.0030318352166796153 Prediction:0.7449378967285156
Error:0.0017054073093822882 Prediction:0.7587034225463867
Error:0.0009592916115275371 Prediction:0.76902756690979
Error:0.0005396015314842384 Prediction:0.7767706751823426
Error:0.000303525861459885 Prediction:0.7825780063867569
Error:0.00017073329707118678 Prediction:0.7869335047900676
Error:9.603747960254256e-05 Prediction:0.7902001285925507
Error:5.402108227642978e-05 Prediction:0.7926500964444131
Error:3.038685878049206e-05 Prediction:0.7944875723333098
Error:1.7092608064027242e-05 Prediction:0.7958656792499823
Error:9.614592036015323e-06 Prediction:0.7968992594374867
Error:5.408208020258491e-06 Prediction:0.7976744445781151

        

#破坏梯度下降
weight = 0.5
goal_pred = 0.8
input = 2

for iteration in range(20):
    pred = input * weight
    error = (pred - goal_pred) ** 2
    delta = pred - goal_pred
    weight_delta = input * delta
    weight = weight - weight_delta
    print("Error:" + str(error) + " Prediction:" + str(pred))
    
#破坏梯度下降后结果
Error:0.03999999999999998 Prediction:1.0
Error:0.3599999999999998 Prediction:0.20000000000000018
Error:3.2399999999999984 Prediction:2.5999999999999996
Error:29.159999999999986 Prediction:-4.599999999999999
Error:262.4399999999999 Prediction:16.999999999999996
Error:2361.959999999998 Prediction:-47.79999999999998
Error:21257.639999999978 Prediction:146.59999999999994
Error:191318.75999999983 Prediction:-436.5999999999998
Error:1721868.839999999 Prediction:1312.9999999999995
Error:15496819.559999991 Prediction:-3935.799999999999
Error:139471376.03999993 Prediction:11810.599999999997
Error:1255242384.3599997 Prediction:-35428.59999999999
Error:11297181459.239996 Prediction:106288.99999999999
Error:101674633133.15994 Prediction:-318863.79999999993
Error:915071698198.4395 Prediction:956594.5999999997
Error:8235645283785.954 Prediction:-2869780.599999999
Error:74120807554073.56 Prediction:8609344.999999996
Error:667087267986662.1 Prediction:-25828031.799999986
Error:6003785411879960.0 Prediction:77484098.59999996
Error:5.403406870691965e+16 Prediction:-232452292.5999999

        
#只是把input设为2，但并没有出现预测的0.8。并且这个预测结果爆炸，有正有负，来回往复。
```

##  七、发散和学习率

1.Neural_Network_06.ipynb   （引入α------->使小的神经网络也能做出很好的预测）

```python
weight = 0.5
goal_pred = 0.8
input = 2
alpha = 0.1#alpha值非常小（或者非常大）时，会发生什么？成负数时会发生什么？   可以自己多长时间来寻找合适的alpha

for iteration in range(20):
    pred  = input * weight
    error = (pred - goal_pred) ** 2
    derivative = input * (pred - goal_pred)
    weight = weight - (alpha * derivative)
    
    print("Error:" + str(error) + "   "+" Prediction:" + str(pred))

#结果
Error:0.03999999999999998    Prediction:1.0
Error:0.0144    Prediction:0.92
Error:0.005183999999999993    Prediction:0.872
Error:0.0018662400000000014    Prediction:0.8432000000000001
Error:0.0006718464000000028    Prediction:0.8259200000000001
Error:0.00024186470400000033    Prediction:0.815552
Error:8.70712934399997e-05    Prediction:0.8093312
Error:3.134566563839939e-05    Prediction:0.80559872
Error:1.1284439629823931e-05    Prediction:0.803359232
Error:4.062398266736526e-06    Prediction:0.8020155392
Error:1.4624633760252567e-06    Prediction:0.8012093235200001
Error:5.264868153690924e-07    Prediction:0.8007255941120001
Error:1.8953525353291194e-07    Prediction:0.8004353564672001
Error:6.82326912718715e-08    Prediction:0.8002612138803201
Error:2.456376885786678e-08    Prediction:0.8001567283281921
Error:8.842956788836216e-09    Prediction:0.8000940369969153
Error:3.1834644439835434e-09    Prediction:0.8000564221981492
Error:1.1460471998340758e-09    Prediction:0.8000338533188895
Error:4.125769919393652e-10    Prediction:0.8000203119913337
Error:1.485277170987127e-10    Prediction:0.8000121871948003
```

2.**引入alpha（防止过度修正权重）**

- 合适的alpha值常常是靠猜测找到的
- **在更新权重的时候**：用一个较小的数字alpha乘以weight_delta，然后用它对权重进行更新。这项操作让你能够控制神经网络学习的速度。如果你学习太快，权重更新的幅度会过大，以至于出现矫枉过正的现象。

#  第五章：通用梯度下降：一次学习多个权重

##  一、多输入梯度下降学习

1.Neural_Network_01.ipynb     (多输入梯度下降)

```python
def neural_network(input,weights):
    out=0
    for i in range(len(input)):
        out+=(input[i]*weights[i])
    return out

#计算权重增量
def ele_mul(scalar,vector):
    out=[0,0,0]
    for i in range(len(out)):
        out[i]=vector[i]*scalar
    return out

toes=[8.5,9.5,9.9,9.0]
wlrec=[0.65,0.8,0.8,0.9]
nfans=[1.2,1.3,0.5,1.0]

win_or_lose_binary=[1,1,0,1]
true=win_or_lose_binary[0]

alpha=0.1
weights=[0.1,0.2,-0.1]
input=[toes[0],wlrec[0],nfans[0]]

for iter in range(3):
    pred=neural_network(input,weights)
    
    error=(pred-true)**2
    delta=pred-true
    
    weight_deltas=ele_mul(delta,input)
    
    print("Iteration:"+str(iter+1))
    print("Pred:"+str(pred))
    print("Error:"+str(error))
    print("Delta:"+str(delta))
    print("Weights:"+str(weights))
    print("Weight_Deltas:")
    print(str(weight_deltas))
    print()
    
    #更新权重
    for i in range(len(weights)):
        weights[i]-=alpha*weight_deltas[i]
        
        
 #结果：
Iteration:1
Pred:0.8600000000000001
Error:0.01959999999999997
Delta:-0.1399999999999999
Weights:[0.1, 0.2, -0.1]
Weight_Deltas:
[-1.189999999999999, -0.09099999999999994, -0.16799999999999987]

Iteration:2
Pred:1.8975749999999993
Error:0.8056408806249988
Delta:0.8975749999999993
Weights:[0.21899999999999992, 0.2091, -0.08320000000000002]
Weight_Deltas:
[7.629387499999995, 0.5834237499999996, 1.0770899999999992]

Iteration:3
Pred:-4.754577718749997
Error:33.115164721133915
Delta:-5.754577718749997
Weights:[-0.5439387499999997, 0.15075762500000006, -0.19090899999999994]
Weight_Deltas:
[-48.91391060937497, -3.740475517187498, -6.905493262499996]
```

2.delta：在当前的训练示例中，用于衡量你所希望的当前节点的值的变化，以便完美的预测结果

3.weight_delta：一项基于导数的对权重移动的方向和数量的估计，目标是降低node_delta,需要考虑关于缩放、负值反转、条件停止的处理。（每项weight_delta都是delta基于输入数据的一种修正版本）

4.**在更新权重的时候**：用一个较小的数字alpha乘以weight_delta，然后用它对权重进行更新。这项操作让你能够控制神经网络学习的速度。如果你学习太快，权重更新的幅度会过大，以至于出现矫枉过正的现象。

5.大部分的学习（权重改变）都是在输入值最大的权重上进行的，因为输入能够显著的改变斜率。基于斜率的显著差异，我们必须将alpha设置得比预想中的还要低。

##  二、冻结权重的意义和用途

1.Neural_Network_02.ipynb   (单项权重冻结：此处冻结toes[0]的权重增量：weight_deltas[0],使其为0)

```python
def neural_network(input,weights):
    out=0
    for i in range(len(input)):
        out+=(input[i]*weights[i])
    return out

#计算权重增量
def ele_mul(scalar,vector):
    out=[0,0,0]
    for i in range(len(out)):
        out[i]=vector[i]*scalar
    return out

toes=[8.5,9.5,9.9,9.0]
wlrec=[0.65,0.8,0.8,0.9]
nfans=[1.2,1.3,0.5,1.0]

win_or_lose_binary=[1,1,0,1]
true=win_or_lose_binary[0]

#alpha修改变大,改变学习率
alpha=0.3
weights=[0.1,0.2,-0.1]
input=[toes[0],wlrec[0],nfans[0]]

for iter in range(3):
    pred=neural_network(input,weights)
    
    error=(pred-true)**2
    delta=pred-true
    
    weight_deltas=ele_mul(delta,input)
    #将第一个权重增量设置为0，使第一项权重永远不会改变
    weight_deltas[0]=0
    
    print("Iteration:"+str(iter+1))
    print("Pred:"+str(pred))
    print("Error:"+str(error))
    print("Delta:"+str(delta))
    print("Weights:"+str(weights))
    print("Weight_Deltas:")
    print(str(weight_deltas))
    print()
    
    #更新权重
    for i in range(len(weights)):
        weights[i]-=alpha*weight_deltas[i]
        
 #结果：
Iteration:1
Pred:0.8600000000000001
Error:0.01959999999999997
Delta:-0.1399999999999999
Weights:[0.1, 0.2, -0.1]
Weight_Deltas:
[0, -0.09099999999999994, -0.16799999999999987]

Iteration:2
Pred:0.9382250000000001
Error:0.003816150624999989
Delta:-0.06177499999999991
Weights:[0.1, 0.2273, -0.04960000000000005]
Weight_Deltas:
[0, -0.040153749999999946, -0.07412999999999989]

Iteration:3
Pred:0.97274178125
Error:0.000743010489422852
Delta:-0.027258218750000007
Weights:[0.1, 0.239346125, -0.02736100000000008]
Weight_Deltas:
[0, -0.017717842187500006, -0.032709862500000006]
```

2.因为权重是共享的，当一项权重到了梯度的底部（导数为0）所有权重都会到达梯度的底部。

3.神经网络潜在的一项负面特性：第一项权重   a  ----->  (toes[0])
可能对应着·重要的输入数据，会对预测结果产生举足轻重的影响，但如果网络在训练数据集中意外的发现了一种不需要a也可以准确预测（error=0)的情况，那么权重a将不会对预测结果产生任何影响。

4.误差由训练数据决定。任何网络的权重都可以任意取值，但给定任意特定权重后，误差完全由数据决定。

##  三、多输出梯度下降学习

1.Neural_Network_03.ipynb    (通过一个输入  输赢率  来预测多个结果：是否受伤、是否获胜、是否难过)

```python
#通过一个输入  输赢率  来预测多个结果：是否受伤、是否获胜、是否难过
weights = [0.3, 0.2, 0.9] 

#计算权重增量
def ele_mul(scalar,vector):
    out=[0,0,0]
    for i in range(len(out)):
        out[i]=vector[i]*scalar
    return out

def neural_network(input, weights):
    pred = ele_mul(input,weights)
    return pred

wlrec = [0.65, 1.0, 1.0, 0.9]

hurt  = [0.1, 0.0, 0.0, 0.1]
win   = [  1,   1,   0,   1]
sad   = [0.1, 0.0, 0.1, 0.2]

input = wlrec[0]
true = [hurt[0], win[0], sad[0]]

pred = neural_network(input,weights)

error = [0, 0, 0] 
delta = [0, 0, 0]

for i in range(len(true)):
    #计算出每一个想要预测结果的误差和增量
    error[i] = (pred[i] - true[i]) ** 2
    delta[i] = pred[i] - true[i]
    
def scalar_ele_mul(number,vector):
    output = [0,0,0]
    assert(len(output) == len(vector))
    for i in range(len(vector)):
        output[i] = number * vector[i]
    return output

weight_deltas = scalar_ele_mul(input,delta)

alpha = 0.1

for i in range(len(weights)):
    weights[i] -= (weight_deltas[i] * alpha)
    
print("Weights:" + str(weights))
print("Weight Deltas:" + str(weight_deltas))

#结果：
Weights:[0.293825, 0.25655, 0.868475]
Weight Deltas:[0.061750000000000006, -0.5655, 0.3152500000000001]
```

##  四、多输入多输出梯度下降学习

1.Neural_Network_04.ipynb  (多个输入，多个输出的梯度下降)  

```python
import numpy as np
            #toes %win #fans
weights = [ [0.1, 0.1, -0.3],#hurt?
            [0.1, 0.2, 0.0], #win?
            [0.0, 1.3, 0.1] ]#sad?

def w_sum(a,b):
    assert(len(a) == len(b))
    output = 0
    for i in range(len(a)):
        output += (a[i] * b[i])
    return output


#注意：
            #toes %win #fans
#weights = [ [0.1, 0.1, -0.3],#hurt?
#            [0.1, 0.2, 0.0], #win?
#            [0.0, 1.3, 0.1],#sad?
#            [1,2,3]]
#1.print(len(weights))------>4    表示矩阵的行数
#2. for i in range(len(weights)):
#       print(weights[i])    表示每一个行向量
#结果：[0.1, 0.1, -0.3]
#      [0.1, 0.2, 0.0]
#      [0.0, 1.3, 0.1]
#      [1, 2, 3]      
def vect_mat_mul(vect,matrix):
    assert(len(vect) == len(matrix))
    output = [0,0,0]
    for i in range(len(vect)):
        output[i] = w_sum(vect,matrix[i])
    return output

def neural_network(input, weights):
    pred = vect_mat_mul(input,weights)
    return pred

toes  = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65,0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

hurt  = [0.1, 0.0, 0.0, 0.1]
win   = [  1,   1,   0,   1]
sad   = [0.1, 0.0, 0.1, 0.2]

alpha = 0.01

input = [toes[0],wlrec[0],nfans[0]]
true  = [hurt[0], win[0], sad[0]]

pred = neural_network(input,weights)

error = [0, 0, 0] 
delta = [0, 0, 0]

for i in range(len(true)):
    error[i] = (pred[i] - true[i]) ** 2
    delta[i] = pred[i] - true[i]

def outer_prod(vec_a,vec_b):
    out = np.zeros((len(vec_a),len(vec_b)))
    for i in range(len(vec_a)):
        for j in range(len(vec_b)): 
            out[i][j]=vec_a[i]*vec_b[j]
    return out

weight_deltas = outer_prod(input,delta)

for i in range(len(weights)):
    for j in range(len(weights[0])):
        weights[i][j] -= alpha * weight_deltas[i][j]
        
        
print("Weights:"+str(weights))
print("Weight_deltas:"+str(weight_deltas))

#结果：
Weights:[[0.061325, 0.1017, -0.373525], [0.0970425, 0.20013, -0.005622500000000002], [-0.0054600000000000004, 1.30024, 0.08962]]
Weight_deltas:[[ 3.8675  -0.17     7.3525 ]
 [ 0.29575 -0.013    0.56225]
 [ 0.546   -0.024    1.038  ]]
```

2.**数据集：MNIST**      ----------------->学习有关这个数据集的东西

##  五、可视化权重

1.在图像领域，将权重以图片的形式进行可视化。

2.如果权重值比较高，说明模型对应的像素和需要预测的值的相关度会比较高。如果权重值非常低（负数），说明神经网络认为对应的像素和需要预测的值的相关度非常低，甚至可能是负相关。

##  六、可视化点积

1.点积是对两个向量之间相似性的松散度量。

#  第六章：建立你的第一个深度神经网络：反向传播

##  一、交通信号灯问题

1.通过解读交通信号的含义来知道什么时候过马路是安全的。（神经网络不能识别交通信号灯）

2.如何训练一个监督神经网络呢？---->有两个数据集：一是六组信号灯状态（已经知道的）；二是六组人们是否通行的观察记录（想要知道的）

##  二、矩阵和矩阵关系

1.将交通信号灯转换成数学表达。以数字模式1和0的形式模拟信号灯的模式。

2.在数据矩阵中，惯例是用一行来表示每个样例记录，并将每一项（属性）对应记录为一列（矩阵的一列包含对某事物所有状态的记录）。

3.好的数据矩阵能够完美的模拟外部世界。

4.Neural_Network_01.ipynb     ---------->只学习一条记录

```python
import numpy as np
weights = np.array([0.5,0.48,-0.7])
alpha = 0.1

streetlights = np.array( [ [ 1, 0, 1 ],
                           [ 0, 1, 1 ],
                           [ 0, 0, 1 ],
                           [ 1, 1, 1 ],
                           [ 0, 1, 1 ],
                           [ 1, 0, 1 ] ] )

walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )

input = streetlights[0] # [1,0,1]
goal_prediction = walk_vs_stop[0] # 等于0（停止）

for iteration in range(20):
    prediction = input.dot(weights)
    error = (goal_prediction - prediction) ** 2
    delta = prediction - goal_prediction
    weights = weights - (alpha * (input * delta))

    print("Error:" + str(error) +"    "+ " Prediction:" + str(prediction))
    
 #结果
Error:0.03999999999999998     Prediction:-0.19999999999999996
Error:0.025599999999999973     Prediction:-0.15999999999999992
Error:0.01638399999999997     Prediction:-0.1279999999999999
Error:0.010485759999999964     Prediction:-0.10239999999999982
Error:0.006710886399999962     Prediction:-0.08191999999999977
Error:0.004294967295999976     Prediction:-0.06553599999999982
Error:0.002748779069439994     Prediction:-0.05242879999999994
Error:0.0017592186044416036     Prediction:-0.04194304000000004
Error:0.0011258999068426293     Prediction:-0.03355443200000008
Error:0.0007205759403792803     Prediction:-0.02684354560000002
Error:0.0004611686018427356     Prediction:-0.021474836479999926
Error:0.0002951479051793508     Prediction:-0.01717986918399994
Error:0.00018889465931478573     Prediction:-0.013743895347199997
Error:0.00012089258196146188     Prediction:-0.010995116277759953
Error:7.737125245533561e-05     Prediction:-0.008796093022207963
Error:4.951760157141604e-05     Prediction:-0.007036874417766459
Error:3.169126500570676e-05     Prediction:-0.0056294995342132115
Error:2.028240960365233e-05     Prediction:-0.004503599627370569
Error:1.298074214633813e-05     Prediction:-0.003602879701896544
Error:8.307674973656916e-06     Prediction:-0.002882303761517324
```

5.Neural_Network_02.ipynb    ------>学习所有的记录（此处是六条记录），然后进行多次迭代（此处是40次迭代）看误差结果。

```python
import numpy as np

weights = np.array([0.5,0.48,-0.7])
alpha = 0.1

streetlights = np.array( [[ 1, 0, 1 ],
                          [ 0, 1, 1 ],
                          [ 0, 0, 1 ],
                          [ 1, 1, 1 ],
                          [ 0, 1, 1 ],
                          [ 1, 0, 1 ] ] )

walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )

input = streetlights[0] # [1,0,1]
goal_prediction = walk_vs_stop[0] #等于0（停止）

for iteration in range(40):#迭代40次，来看误差结果越来越小
    error_for_all_lights = 0
    for row_index in range(len(walk_vs_stop)):#此处是通过你想要知道的结果集中结果个数来对每一个记录进行学习（此处是6个记录）
        input = streetlights[row_index]
        goal_prediction = walk_vs_stop[row_index]
        
        prediction = input.dot(weights)
        
        error = (goal_prediction - prediction) ** 2
        error_for_all_lights += error
        
        delta = prediction - goal_prediction
        weights = weights - (alpha * (input * delta))
        print("Prediction:" + str(prediction))
    print("Error:" + str(error_for_all_lights) + "\n")
 #结果
Prediction:-0.19999999999999996
Prediction:-0.19999999999999996
Prediction:-0.5599999999999999
Prediction:0.6160000000000001
Prediction:0.17279999999999995
Prediction:0.17552
Error:2.6561231104

Prediction:0.14041599999999999
Prediction:0.3066464
Prediction:-0.34513824
Prediction:1.006637344
Prediction:0.4785034751999999
Prediction:0.26700416768
Error:0.9628701776715985
.......................
.......................
Prediction:-0.002388697618122871
Prediction:0.9977021355600483
Prediction:-0.01793930655497516
Prediction:1.0162137740080082
Prediction:0.9967128843019345
Prediction:-0.0028012842268006904
Error:0.0006143435674831474

Prediction:-0.0022410273814405524
Prediction:0.9978745386023716
Prediction:-0.016721264429884947
Prediction:1.0151127459893812
Prediction:0.9969492081270097
Prediction:-0.0026256193329783125
Error:0.00053373677328488
```

##  三、完全、批量和随机梯度下降

1.随机梯度下降每次对一个样例更新权重。（循环遍历整个数据集多次，直到找到所有训练用例的权重配置）

2.完全梯度下降每次对整个数据集更新权重。

3.批量梯度下降对每n个样例更新权重。（先选择确定批次大小[通常是8到256之间]的样例，然后更新权重）

##  四、神经网络对相关性的学习

1.所有复杂的迭代学习过程最终完成了相当简单的事情：让神经网络确认了输入和输出的相关性。任何权重数值高的位置都具有高的相关性。

2.网络如何确认相关性？在梯度下降的过程中，每个训练示例都会对权重施加向上或向下的压力。（权重数值大，受到的向上的压力越大；权重数值小，受到的向下的压力越大）

3.神经网络能够学习的关键部分是**误差归因**，它意味着给定**共享误差**，神经网络需要找出哪些权重对误差产生了影响（可以据此进行调整），哪些权重没有（可以不管它们）。

4.底线：预测结果是输入数据的加权和。学习算法对与输出相关的输入所对应的权重以向上（向1的方向）压力作为奖励，而对于与输出不想管的输入所对应的权重以向下（向0的方向）压力作为惩罚。输入数据的加权和通过将无关输入所对应的权重项推向0，可以找到输入和输出之间存在的完美相关性。

##  五、过拟合

1.深度学习最大的弱点：过拟合。（所有的权值都有误差。如果某个特定的权重配置意外的在预测数据集和输出数据集之间创建了完美的吻合（比如使得=0），而此时并没有给真正最重要的输入赋予最大的权重，那么神经网络将停止学习。）

2.深度学习所面临的的最大挑战是训练你的神经网络去泛化而不仅仅是记忆。

3.正则化是有利的。从本质上来讲，正则化的目的是使得只有真正具有强相关性的权重才能保持不变，其他的一切都应该被压制，因为他们会产生噪声。与此同时，它会导致神经网络的训练加快（迭代次数更少）。

##  六、创建属于自己的相关性

1.如果数据没有相关性，那么创建具有相关性的中间数据。

##  七、反向传播：远程错误归因

 1.加权平均误差

 2.加权平均增量

##  八、线性与非线性

1.要知道：对于任何两个乘法，都可以用一个乘法完成同样的事情。（这样做不好）

2.任何一个三层的神经网络，都存在着一个有着相同的行为模式的两层神经网络。

3.两个连续的加权和运算等价于一个加权和运算，只是代价更昂贵。

4.条件相关（选择相关）:对于中间层，不仅仅总是X%与一个输入节点相关，Y%与另一个输入节点相关；相反，它可以只在想要和某个输入节点产生X%的关联的时候，才关联到它，而其他时候根本不关联。

5.当值低于0时“关闭”节点。（通过在任何中间节点为负时关闭它，你能够允许神经网络选择性接受来自不同输入的相关性。这对于两层神经网络来说是不可能的，因此，三层神经网络具有更强的能力。）

6.“如果节点为负，则将其设置为0”的逻辑有一个漂亮的术语：非线性。

7.非线性变换有很多中，此处讨论许多情况下最好的方法，也是最简单的方法：relu。

8.为什么选择非线性而不是线性的模型----

7.非线性变换有很多种，此处讨论许多情况下最好的，也是最简单的方法：relu。

8.为什么会选择非线性而不是线性的模型----------->“如果没有非线性变换，那么两个矩阵相乘可能等价于一个矩阵。”

9.神经网络寻找输入和输出之间的相关性。

##  九、第一个深度神经网络：反向传播

1.**numpy.random.seed()的使用**

seed( ) 用于指定随机数生成时所用算法开始的整数值。

- 如果使用相同的seed( )值，则每次生成的随即数都相同；
-  如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。
- 设置的seed()值仅一次有效

```python
#代码一：使用seed()设置随机数生成时所用算法开始的整数值
import random

num=0
while(num<5):
    random.seed(5)
    print(random.random())
    num+=1
    
#结果：
0.6229016948897019
0.6229016948897019
0.6229016948897019
0.6229016948897019
0.6229016948897019


#代码二：不使用seed()，相当于生成没有任何限制的随机数
import random

num=0
random.seed(5)
while(num<5):
    print(random.random())
    num+=1
    
#结果：
0.6229016948897019
0.7417869892607294
0.7951935655656966
0.9424502837770503
0.7398985747399307
```

2.np.random.random((num1,num2))用法

```python
np.random.random((6, 7))#代表生成6行 7列的随机浮点数，浮点数范围 : (0,1)
#生成结果
[[0.96826158 0.31342418 0.69232262 0.87638915 0.89460666 0.08504421
  0.03905478]
 [0.16983042 0.8781425  0.09834683 0.42110763 0.95788953 0.53316528
  0.69187711]
 [0.31551563 0.68650093 0.83462567 0.01828828 0.75014431 0.98886109
  0.74816565]
 [0.28044399 0.78927933 0.10322601 0.44789353 0.9085955  0.29361415
  0.28777534]
 [0.13002857 0.01936696 0.67883553 0.21162812 0.26554666 0.49157316
  0.05336255]
 [0.57411761 0.14672857 0.58930554 0.69975836 0.10233443 0.41405599
  0.69440016]]
```

3.Neural_Network_03.ipynb      第一个深度神经网络：反向传播代码整合

```python
import numpy as np

np.random.seed(1)#设置随机数生成时的整数值

#当x>0时，返回x；在其他条件下，返回0
def relu(x):
    return (x>0)*x

#当input输入大于0时，返回1；在其他条件下,返回0
def relu2deriv(output):
    return output>0

alpha=0.2
hidden_size=4  #中间层节点个数

streetlights = np.array( [ [ 1, 0, 1 ],
                           [ 0, 1, 1 ],
                           [ 0, 0, 1 ],
                           [ 1, 1, 1 ] ] )

walk_vs_stop = np.array( [[ 1, 1, 0, 0 ]] ).T #转置为列

weights_0_1=2*np.random.random((3,hidden_size))-1  #3行4列
weights_1_2=2*np.random.random((hidden_size,1))-1  #4行1列

for iteration in range(60):
    layer_2_error=0
    for i in range(len(streetlights)):
        layer_0=streetlights[i:i+1]   #streetlights[i:i+1] 结果为[[1 0 1]]   [[0 1 1]]   [[0 0 1]]  [[1 1 1]]
        layer_1=relu(np.dot(layer_0,weights_0_1))
        layer_2=np.dot(layer_1,weights_1_2)
        
        layer_2_error+=np.sum((layer_2-walk_vs_stop[i:i+1])**2)
        
        layer_2_delta=(walk_vs_stop[i:i+1]-layer_2)
        layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)
        #从这里可以看出反向传播能够计算中间层的增量。对layer_1取layer_2增量的加权平均值（用它们之间的权重计算得出）。然后关闭不参与前向预测的节点（将它们设置为0），因为它们对误差没有贡献
        
        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)
    
    if(iteration%10==9):
        print("Error:"+str(layer_2_error))
        
 #结果:
Error:0.6342311598444467
Error:0.35838407676317513
Error:0.0830183113303298
Error:0.006467054957103705
Error:0.0003292669000750734
Error:1.5055622665134859e-05
```

#  第七章：如何描绘神经网络：在脑海里，在白纸上...

##  一、相关性的概括

1.对于深度学习而言，重要的是掌握相关性的观点（即：神经网络能够发现并创建相关性），这个观念命名为：关联抽象。

2.关联抽象：神经网络试图寻找在输入层和输出层之间的直接和间接关联，这种关联性由输入和输出数据集决定。

3.局部关联抽象：任何给定的权重集合都通过优化来学习如何关联输入层与输出层的要求。

4.全局关联抽象：前一层神经网络所应该得到的值，可以通过后一层神经网络（所应该得到的值）的值和它们之间的权重的乘积来确定。这样，后面的层就可以告诉全面的层它们需要什么样的信号，从而最终找到与输出的相关性。这种交叉通信称为反向传播。

5.神经网络就是一系列权重矩阵。当使用神经网络时，也是在创建对应每一层的向量。（权重矩阵是从节点和节点的连线，向量就是一层层的节点）

## 二、简化版可视化

如图：



<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\01.jpg"  style="zoom: 25%;" />

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\02.jpg" style="zoom: 25%;" />

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\03.jpg" style="zoom: 25%;" />

1.神经网络就像乐高积木，每块积木都是一个向量或矩阵。

2.关联抽象的好处是，所有导致它的细节（包括反向传播、梯度下降、alpha、dropout、小型批处理等）都不依赖于某一块乐高积木的特定形状。无论你如何将一系列矩阵拼接在一起，用神经元把它们“粘“在一起，神经网络都会通过调整输入层和输出层之间的权重来尝试学习数据中存在的模式。

3.权重矩阵的纬度由各网络层决定。（如图：权重矩阵weights_0_1是一个（3×4）的矩阵，因为前一层（layer_0）的维度是3，后一层（layer_1）的维度是4）

4.神经网络的权重矩阵和神经元层的特定配置称为网络结构。良好的神经网络结构能够引导信号，使相关性易于发现（神经网络通过调整权重来发现输入层和输出层之间的相关性，有时甚至借助隐含层创造相关性）。优秀的神经网络还可以过滤噪声以防止过拟合。

## 三、使用字母而不是图片进行可视化

如图：用数学可视化表达

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\04.jpg" style="zoom:25%;" />

1.连接变量

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\05.jpg" style="zoom: 50%;" />

2.Python代码、代数公式和可视化放在一起看（四种不同的方式）

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\06.jpg" style="zoom:50%;" />

3.神经网络的结构能够控制信号在网络中的流动方式。

4.不同的数据集和不同的领域具有不同的特征。例如，图像数据的信号和噪声的类型与文本数据不同。即使同一种神经网络结构可以在很多情况下使用，但不同结构会适用于不同的问题，因为它们定位特定类型相关性的能力不同。

# 第八章：学习信号，忽略噪声：正则化和批处理

## 一、过拟合

1.过拟合，在神经网络中极常见。它使神经网络的宿敌；神经网络的表达能力越强大（层数和权重数量越多），网络就越容易过拟合。

2.Neural_Network_01.ipynb     (带有relu隐藏层的三层网络)

```python
import sys, numpy as np
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])

one_hot_labels = np.zeros((len(labels),10))
for i,l in enumerate(labels):
    one_hot_labels[i][l] = 1
labels = one_hot_labels

test_images = x_test.reshape(len(x_test),28*28) / 255
test_labels = np.zeros((len(y_test),10))
for i,l in enumerate(y_test):
    test_labels[i][l] = 1
    
np.random.seed(1)
relu = lambda x:(x>=0) * x # returns x if x > 0, return 0 otherwise
relu2deriv = lambda x: x>=0 # returns 1 for input > 0, return 0 otherwise
alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)

weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1
weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1

for j in range(iterations):
    error, correct_cnt = (0.0, 0)
    
    for i in range(len(images)):
        layer_0 = images[i:i+1]
        layer_1 = relu(np.dot(layer_0,weights_0_1))
        layer_2 = np.dot(layer_1,weights_1_2)

        error += np.sum((labels[i:i+1] - layer_2) ** 2)
        correct_cnt += int(np.argmax(layer_2) == \
                                        np.argmax(labels[i:i+1]))

        layer_2_delta = (labels[i:i+1] - layer_2)
        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\
                                    * relu2deriv(layer_1)
        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)

    sys.stdout.write("\r I:"+str(j)+ \
                     " Train-Err:" + str(error/float(len(images)))[0:5] +\
                     " Train-Acc:" + str(correct_cnt/float(len(images))))
    
#结果：
I:349 Train-Err:0.108 Train-Acc:1.099
```



3.Neural_Network_02.ipynb     （根据代码中的两个变量：test_images和test_labels，对测试集中的数据进行评估分类。测试准确率为70.7%，它是在没有训练过得数据集上的准确率。其中测试准确率的峰值达到81.14%）

```python
import sys, numpy as np
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])

one_hot_labels = np.zeros((len(labels),10))
for i,l in enumerate(labels):
    one_hot_labels[i][l] = 1
labels = one_hot_labels

test_images = x_test.reshape(len(x_test),28*28) / 255
test_labels = np.zeros((len(y_test),10))
for i,l in enumerate(y_test):
    test_labels[i][l] = 1

np.random.seed(1)
relu = lambda x:(x>=0) * x # returns x if x > 0, return 0 otherwise
relu2deriv = lambda x: x>=0 # returns 1 for input > 0, return 0 otherwise
alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)

weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1
weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1

for j in range(iterations):
    error, correct_cnt = (0.0, 0)
    
    for i in range(len(images)):
        layer_0 = images[i:i+1]
        layer_1 = relu(np.dot(layer_0,weights_0_1))
        layer_2 = np.dot(layer_1,weights_1_2)

        error += np.sum((labels[i:i+1] - layer_2) ** 2)
        correct_cnt += int(np.argmax(layer_2) == \
                                        np.argmax(labels[i:i+1]))

        layer_2_delta = (labels[i:i+1] - layer_2)
        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\
                                    * relu2deriv(layer_1)
        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)

    sys.stdout.write("\r I:"+str(j)+ \
                     " Train-Err:" + str(error/float(len(images)))[0:5] +\
                     " Train-Acc:" + str(correct_cnt/float(len(images))))
    
    #添加以下代码，将在这些图像上运行神经网络，并评估网络对它们进行分类的效果。
    if(j % 10 == 0 or j == iterations-1):
        error, correct_cnt = (0.0, 0)

        for i in range(len(test_images)):

            layer_0 = test_images[i:i+1]
            layer_1 = relu(np.dot(layer_0,weights_0_1))
            layer_2 = np.dot(layer_1,weights_1_2)

            error += np.sum((test_labels[i:i+1] - layer_2) ** 2)
            correct_cnt += int(np.argmax(layer_2) == \
                                            np.argmax(test_labels[i:i+1]))
        sys.stdout.write(" Test-Err:" + str(error/float(len(test_images)))[0:5] +\
                         " Test-Acc:" + str(correct_cnt/float(len(test_images))))
        print()
    
 #结果：
I:0 Train-Err:0.722 Train-Acc:0.537 Test-Err:0.601 Test-Acc:0.6488
 I:10 Train-Err:0.312 Train-Acc:0.901 Test-Err:0.420 Test-Acc:0.8114
 I:20 Train-Err:0.260 Train-Acc:0.937 Test-Err:0.414 Test-Acc:0.8111
 I:30 Train-Err:0.232 Train-Acc:0.946 Test-Err:0.417 Test-Acc:0.8066
 I:40 Train-Err:0.215 Train-Acc:0.956 Test-Err:0.426 Test-Acc:0.8019
 I:50 Train-Err:0.204 Train-Acc:0.966 Test-Err:0.437 Test-Acc:0.7982
 I:60 Train-Err:0.194 Train-Acc:0.967 Test-Err:0.448 Test-Acc:0.7921
 I:70 Train-Err:0.186 Train-Acc:0.975 Test-Err:0.458 Test-Acc:0.7864
 I:80 Train-Err:0.179 Train-Acc:0.979 Test-Err:0.466 Test-Acc:0.7817
 I:90 Train-Err:0.172 Train-Acc:0.981 Test-Err:0.474 Test-Acc:0.7758
 I:100 Train-Err:0.166 Train-Acc:0.984 Test-Err:0.482 Test-Acc:0.7706
 I:110 Train-Err:0.161 Train-Acc:0.984 Test-Err:0.489 Test-Acc:0.7686
 I:120 Train-Err:0.157 Train-Acc:0.986 Test-Err:0.496 Test-Acc:0.766
 I:130 Train-Err:0.153 Train-Acc:0.999 Test-Err:0.502 Test-Acc:0.7622
 I:140 Train-Err:0.149 Train-Acc:0.991 Test-Err:0.508 Test-Acc:0.758
 I:150 Train-Err:0.145 Train-Acc:0.991 Test-Err:0.513 Test-Acc:0.7558
 I:160 Train-Err:0.141 Train-Acc:0.992 Test-Err:0.518 Test-Acc:0.7553
 I:170 Train-Err:0.138 Train-Acc:0.992 Test-Err:0.524 Test-Acc:0.751
 I:180 Train-Err:0.135 Train-Acc:0.995 Test-Err:0.528 Test-Acc:0.7505
 I:190 Train-Err:0.132 Train-Acc:0.995 Test-Err:0.533 Test-Acc:0.7482
 I:200 Train-Err:0.130 Train-Acc:0.998 Test-Err:0.538 Test-Acc:0.7464
 I:210 Train-Err:0.127 Train-Acc:0.998 Test-Err:0.544 Test-Acc:0.7446
 I:220 Train-Err:0.125 Train-Acc:0.998 Test-Err:0.552 Test-Acc:0.7416
 I:230 Train-Err:0.123 Train-Acc:0.998 Test-Err:0.560 Test-Acc:0.7372
 I:240 Train-Err:0.121 Train-Acc:0.998 Test-Err:0.569 Test-Acc:0.7344
 I:250 Train-Err:0.120 Train-Acc:0.999 Test-Err:0.577 Test-Acc:0.7316
 I:260 Train-Err:0.118 Train-Acc:0.999 Test-Err:0.585 Test-Acc:0.729
 I:270 Train-Err:0.117 Train-Acc:0.999 Test-Err:0.593 Test-Acc:0.7259
 I:280 Train-Err:0.115 Train-Acc:0.999 Test-Err:0.600 Test-Acc:0.723
 I:290 Train-Err:0.114 Train-Acc:0.999 Test-Err:0.607 Test-Acc:0.7196
 I:300 Train-Err:0.113 Train-Acc:0.999 Test-Err:0.614 Test-Acc:0.7183
 I:310 Train-Err:0.112 Train-Acc:0.999 Test-Err:0.622 Test-Acc:0.7165
 I:320 Train-Err:0.111 Train-Acc:0.999 Test-Err:0.629 Test-Acc:0.7133
 I:330 Train-Err:0.110 Train-Acc:0.999 Test-Err:0.637 Test-Acc:0.7125
 I:340 Train-Err:0.109 Train-Acc:1.099 Test-Err:0.645 Test-Acc:0.71
 I:349 Train-Err:0.108 Train-Acc:1.0 Test-Err:0.653 Test-Acc:0.7073
```

4.当完全训练好的神经网络被应用到一张新图像上时，只有当这张新图像与来自训练数据的图像非常相似时，预测的效果才能保证是好的。如果给它一些看起来不熟悉的东西，就会给出随机结果。我们要知道，神经网络只有能够预测你不知道答案的数据时才有用。

5.如果过度训练神经网络，它会变得更糟糕

6.查看神经网络权重的一种方法是将它看成一个高维的形状。当你进行训练时，这个形状会围绕数据的形状塑造，学习如何区分不同的模式。不幸的是，测试数据集中的图像与训练数据集中的数据中隐含的模式略有不同。这导致了神经网络在很多测试样例上失效。

7.神经网络过拟合现象的一个更正式的定义是，这个神经网络学习到了数据集中的噪声，而不是仅基于真实信号做出决策。

8.训练数据中与测试数据不相容的细节信息是什么？在图像中，它通常被称为噪声。

9.如何让神经网络只在信号上进行训练，而忽略噪声（与分类无关的其他信息）？一种方法是提前停止（early stopping）。事实证明，大量的噪声来自于图像在细粒度上的各种细节，并且对物体而言，大多数信号都是在图像的一般形状（可能还有颜色）中发现的。

10.**最简单的正则化：提前停止**。（正则化是用于在机器学习模型中鼓励泛化的方法的一个子集，通常通过提高模型学习训练数据的细粒度细节的难度来实现）

11.如何知道神经网络的学习训练什么时候应该停止？------验证集。（某些情况下，如果你使用测试数据来确定何时停止，模型可能会在测试数据集上过拟合。一般来说，我们不会使用测试集来控制训练的过程，而使用验证集。）

## 二、行业标准正则化：dropout

1.方法：在训练过程中随机关闭神经元（设置为0）。--------------------这种正则化技术通常被认为是对大多数网络来说最重要且最先进的正则化技术。

2.dropout为什么有效？---------------------通过每次随机训练神经网络中的一小部分，dropout能够让一张大网络像小网络一样进行学习—**较小的神经网络不会发生过拟合**（因为较小的神经网络没有太多的表达能力。它们无法抓住那些可能导致过拟合的更细粒度的细节（或噪声）。它们只留下了捕捉那些更大、更明显、更高级特性的空间）。

3.如何在获得小型神经网络的扛过拟合能力的同时，拥有大型网络的表达能力？------------------------答案是随机关闭一个大型神经网络中的节点。当你创建一个大型神经网络，但是只是用它的一小部分训练的时候会发生什么？-------------------------它表现的像一个小型神经网络。但是，当你随机地对其数以百万计的不同子网络进行训练时，这些子网络叠加起来依旧能够保持整个网络的表现力。

4.dropout是一种训练一系列网络并对其进行平均的形式。

5.虽然大型非正则化神经网络更可能对噪声过度拟合，但**它们不太可能对相同的噪声过拟合**（因为**神经网络的初始化是随机的**，一旦其学会的噪声信息足以区分训练数据集中的图像，它就会停止学习）。

6.尽管神经网络是随机生成的，但是它们仍然是从学习最大的、最广泛的特征开始的，之后才会捕捉更过关于噪声的信息。

7.Neural_Network_03.ipynb  （将dropout添加到隐藏层，让50%的节点随机关闭。其中测试准确率峰值达到82.36%。在训练完成时，测试准确率为81.81%）（其中#1，#2，#3是dropout的实现）

```python
import sys, numpy as np
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])

one_hot_labels = np.zeros((len(labels),10))
for i,l in enumerate(labels):
    one_hot_labels[i][l] = 1
labels = one_hot_labels

test_images = x_test.reshape(len(x_test),28*28) / 255
test_labels = np.zeros((len(y_test),10))
for i,l in enumerate(y_test):
    test_labels[i][l] = 1

np.random.seed(1)
def relu(x):
    return (x >= 0) * x # returns x if x > 0
                        # returns 0 otherwise

def relu2deriv(output):
    return output >= 0 #returns 1 for input > 0

alpha, iterations, hidden_size = (0.005, 300, 100)
pixels_per_image, num_labels = (784, 10)

weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1
weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1

for j in range(iterations):
    error, correct_cnt = (0.0,0)
    for i in range(len(images)):
        layer_0 = images[i:i+1]
        layer_1 = relu(np.dot(layer_0,weights_0_1))
        
        #1
        dropout_mask = np.random.randint(2, size=layer_1.shape)
        
        #2
        layer_1 *= dropout_mask * 2
        
        layer_2 = np.dot(layer_1,weights_1_2)

        error += np.sum((labels[i:i+1] - layer_2) ** 2)
        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))
        layer_2_delta = (labels[i:i+1] - layer_2)
        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)
        
        #3
        layer_1_delta *= dropout_mask #dropout_mask使用的是所谓的50%Bernoulli分布，也就是每个值有50%的机会是1，另外         50%的机会是0

        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)

    if(j%10 == 0):
        test_error = 0.0
        test_correct_cnt = 0

        for i in range(len(test_images)):
            layer_0 = test_images[i:i+1]
            layer_1 = relu(np.dot(layer_0,weights_0_1))
            layer_2 = np.dot(layer_1, weights_1_2)

            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)
            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))

        sys.stdout.write("\n" + \
                         "I:" + str(j) + \
                         " Test-Err:" + str(test_error/ float(len(test_images)))[0:5] +\
                         " Test-Acc:" + str(test_correct_cnt/ float(len(test_images)))+\
                         " Train-Err:" + str(error/ float(len(images)))[0:5] +\
                         " Train-Acc:" + str(correct_cnt/ float(len(images))))
        
#结果：
I:0 Test-Err:0.641 Test-Acc:0.6333 Train-Err:0.891 Train-Acc:0.413
I:10 Test-Err:0.458 Test-Acc:0.787 Train-Err:0.472 Train-Acc:0.764
I:20 Test-Err:0.415 Test-Acc:0.8133 Train-Err:0.430 Train-Acc:0.809
I:30 Test-Err:0.421 Test-Acc:0.8114 Train-Err:0.415 Train-Acc:0.811
I:40 Test-Err:0.419 Test-Acc:0.8112 Train-Err:0.413 Train-Acc:0.827
I:50 Test-Err:0.409 Test-Acc:0.8133 Train-Err:0.392 Train-Acc:0.836
I:60 Test-Err:0.412 Test-Acc:0.8236 Train-Err:0.402 Train-Acc:0.836
I:70 Test-Err:0.412 Test-Acc:0.8033 Train-Err:0.383 Train-Acc:0.857
I:80 Test-Err:0.410 Test-Acc:0.8054 Train-Err:0.386 Train-Acc:0.854
I:90 Test-Err:0.411 Test-Acc:0.8144 Train-Err:0.376 Train-Acc:0.868
I:100 Test-Err:0.411 Test-Acc:0.7903 Train-Err:0.369 Train-Acc:0.864
I:110 Test-Err:0.411 Test-Acc:0.8003 Train-Err:0.371 Train-Acc:0.868
I:120 Test-Err:0.402 Test-Acc:0.8046 Train-Err:0.353 Train-Acc:0.857
I:130 Test-Err:0.408 Test-Acc:0.8091 Train-Err:0.352 Train-Acc:0.867
I:140 Test-Err:0.405 Test-Acc:0.8083 Train-Err:0.355 Train-Acc:0.885
I:150 Test-Err:0.404 Test-Acc:0.8107 Train-Err:0.342 Train-Acc:0.883
I:160 Test-Err:0.399 Test-Acc:0.8146 Train-Err:0.361 Train-Acc:0.876
I:170 Test-Err:0.404 Test-Acc:0.8074 Train-Err:0.344 Train-Acc:0.889
I:180 Test-Err:0.399 Test-Acc:0.807 Train-Err:0.333 Train-Acc:0.892
I:190 Test-Err:0.407 Test-Acc:0.8066 Train-Err:0.335 Train-Acc:0.898
I:200 Test-Err:0.405 Test-Acc:0.8036 Train-Err:0.347 Train-Acc:0.893
I:210 Test-Err:0.405 Test-Acc:0.8034 Train-Err:0.336 Train-Acc:0.894
I:220 Test-Err:0.402 Test-Acc:0.8067 Train-Err:0.325 Train-Acc:0.896
I:230 Test-Err:0.404 Test-Acc:0.8091 Train-Err:0.321 Train-Acc:0.894
I:240 Test-Err:0.415 Test-Acc:0.8091 Train-Err:0.332 Train-Acc:0.898
I:250 Test-Err:0.395 Test-Acc:0.8182 Train-Err:0.320 Train-Acc:0.899
I:260 Test-Err:0.390 Test-Acc:0.8204 Train-Err:0.321 Train-Acc:0.899
I:270 Test-Err:0.382 Test-Acc:0.8194 Train-Err:0.312 Train-Acc:0.906
I:280 Test-Err:0.396 Test-Acc:0.8208 Train-Err:0.317 Train-Acc:0.9
I:290 Test-Err:0.399 Test-Acc:0.8181 Train-Err:0.301 Train-Acc:0.908
```



## 三、批量梯度下降

1.小批量随机梯度下降，可以提高训练速度和收敛速度。（在训练过程中不断地进行平均权重的更新。）

2.Neural_Network_04.ipynb  

 (批量梯度下降，与以前相比，训练精度的上升趋势更平稳了。只要是因为在训练过程中不断地进行平均权重的更新。-------------事实证明，对单个样例进行训练，在生成权重更新增量时会有非常大的噪声。因此，对这些权重增量更新进行平均可使学习过程更平滑。)

```python
import sys, numpy as np
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])

one_hot_labels = np.zeros((len(labels),10))
for i,l in enumerate(labels):
    one_hot_labels[i][l] = 1
labels = one_hot_labels

test_images = x_test.reshape(len(x_test),28*28) / 255
test_labels = np.zeros((len(y_test),10))
for i,l in enumerate(y_test):
    test_labels[i][l] = 1
    
np.random.seed(1)

def relu(x):
    return (x >= 0) * x # returns x if x > 0

def relu2deriv(output):
    return output >= 0 # returns 1 for input > 0

batch_size = 100
alpha, iterations = (0.001, 300)
pixels_per_image, num_labels, hidden_size = (784, 10, 100)

weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1
weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1

for j in range(iterations):
    error, correct_cnt = (0.0, 0)
    for i in range(int(len(images) / batch_size)):
        batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))

        layer_0 = images[batch_start:batch_end]
        layer_1 = relu(np.dot(layer_0,weights_0_1))
        dropout_mask = np.random.randint(2,size=layer_1.shape)
        layer_1 *= dropout_mask * 2
        layer_2 = np.dot(layer_1,weights_1_2)

        error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)
        for k in range(batch_size):
            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))

            layer_2_delta = (labels[batch_start:batch_end]-layer_2)/batch_size
            layer_1_delta = layer_2_delta.dot(weights_1_2.T)* relu2deriv(layer_1)
            layer_1_delta *= dropout_mask

            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)
            
    if(j%10 == 0):
        test_error = 0.0
        test_correct_cnt = 0

        for i in range(len(test_images)):
            layer_0 = test_images[i:i+1]
            layer_1 = relu(np.dot(layer_0,weights_0_1))
            layer_2 = np.dot(layer_1, weights_1_2)

            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)
            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))

        sys.stdout.write("\n" + \
                         "I:" + str(j) + \
                         " Test-Err:" + str(test_error/ float(len(test_images)))[0:5] +\
                         " Test-Acc:" + str(test_correct_cnt/ float(len(test_images)))+\
                         " Train-Err:" + str(error/ float(len(images)))[0:5] +\
                         " Train-Acc:" + str(correct_cnt/ float(len(images))))
        
#结果：
I:0 Test-Err:0.815 Test-Acc:0.3832 Train-Err:1.284 Train-Acc:0.165
I:10 Test-Err:0.568 Test-Acc:0.7173 Train-Err:0.591 Train-Acc:0.672
I:20 Test-Err:0.510 Test-Acc:0.7571 Train-Err:0.532 Train-Acc:0.729
I:30 Test-Err:0.485 Test-Acc:0.7793 Train-Err:0.498 Train-Acc:0.754
I:40 Test-Err:0.468 Test-Acc:0.7877 Train-Err:0.489 Train-Acc:0.749
I:50 Test-Err:0.458 Test-Acc:0.793 Train-Err:0.468 Train-Acc:0.775
I:60 Test-Err:0.452 Test-Acc:0.7995 Train-Err:0.452 Train-Acc:0.799
I:70 Test-Err:0.446 Test-Acc:0.803 Train-Err:0.453 Train-Acc:0.792
I:80 Test-Err:0.451 Test-Acc:0.7968 Train-Err:0.457 Train-Acc:0.786
I:90 Test-Err:0.447 Test-Acc:0.795 Train-Err:0.454 Train-Acc:0.799
I:100 Test-Err:0.448 Test-Acc:0.793 Train-Err:0.447 Train-Acc:0.796
I:110 Test-Err:0.441 Test-Acc:0.7943 Train-Err:0.426 Train-Acc:0.816
I:120 Test-Err:0.442 Test-Acc:0.7966 Train-Err:0.431 Train-Acc:0.813
I:130 Test-Err:0.441 Test-Acc:0.7906 Train-Err:0.434 Train-Acc:0.816
I:140 Test-Err:0.447 Test-Acc:0.7874 Train-Err:0.437 Train-Acc:0.822
I:150 Test-Err:0.443 Test-Acc:0.7899 Train-Err:0.414 Train-Acc:0.823
I:160 Test-Err:0.438 Test-Acc:0.797 Train-Err:0.427 Train-Acc:0.811
I:170 Test-Err:0.440 Test-Acc:0.7884 Train-Err:0.418 Train-Acc:0.828
I:180 Test-Err:0.436 Test-Acc:0.7935 Train-Err:0.407 Train-Acc:0.834
I:190 Test-Err:0.434 Test-Acc:0.7935 Train-Err:0.410 Train-Acc:0.831
I:200 Test-Err:0.435 Test-Acc:0.7972 Train-Err:0.416 Train-Acc:0.829
I:210 Test-Err:0.434 Test-Acc:0.7923 Train-Err:0.409 Train-Acc:0.83
I:220 Test-Err:0.433 Test-Acc:0.8032 Train-Err:0.396 Train-Acc:0.832
I:230 Test-Err:0.431 Test-Acc:0.8036 Train-Err:0.393 Train-Acc:0.853
I:240 Test-Err:0.430 Test-Acc:0.8047 Train-Err:0.397 Train-Acc:0.844
I:250 Test-Err:0.429 Test-Acc:0.8028 Train-Err:0.386 Train-Acc:0.843
I:260 Test-Err:0.431 Test-Acc:0.8038 Train-Err:0.394 Train-Acc:0.843
I:270 Test-Err:0.428 Test-Acc:0.8014 Train-Err:0.384 Train-Acc:0.845
I:280 Test-Err:0.430 Test-Acc:0.8067 Train-Err:0.401 Train-Acc:0.846
I:290 Test-Err:0.428 Test-Acc:0.7975 Train-Err:0.383 Train-Acc:0.851
```

##  四、MNIST

- MNIST数据集信息参考：http://yann.lecun.com/exdb/mnist/index.html

-  MNIST是手写数字0~10图片数据集，每一张图片包含28*28个像素。 

-  MNIST训练数据集包含：

  （1）60000张图片的像素信息，因为神经网络的输入层表示特征的维度，所以将图像表示成一个[60000,28,28]的pixel张量；

  （2）60000张图片的标签信息，表示成一个[60000,10]的矩阵，因为图片的标签是介于0-9的数字，把标签分为10类进行one-hot编码(一个one-hot向量除了某一位数字是1以外，其余维度数字都是0)，比如标签0将表示为([1,0,0,0,0,0,0,0,0,0])。

- MNIST测试数据集包含：

  10000张图片的像素信息，同训练数据集一样，将这10000张图片的像素信息，表示成一个[10000,28,28]的张量，将10000张图片的标签表示成[10000,10]的矩阵。

- 此外，还需要把每张图片的像素信息转化成向量形式，即将[28,28]的矩阵按行拉平成[1,784]的向量，即把数据集转成 [60000, 784]才能放到网络中训练。 第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。 一般还需要把图片中的像素数组归一化为0-1之间。

## 五、import...和from...import...的区别

1、语法分析：

- from A import a1 是从A模块导入a1工具（可以是某个 函数，全局变量，类）；

- import  A是导入整个A模块的全部内容（包括全部的函数，全局变量，类）。

2、内存分析：

- from...import... 会在内存中创建并加载该模块工具的副本，当有另外一个程序导入时，会在内存中先复制另一个副本（变量而非函数）进行加载，不会共用一个副本。所以程序进行的修改不会影响到被导入的原模块，且**不同程序之间不会互相影响**。

- import...方法导入模块会在内存中直接加载该模块的全部属性。当出现多个程序导入该模块时,会**共用一个模块**，程序之间会互相影响，包括原模块。

#  第九章：概率和非线性建模：激活函数

## 一、什么是激活函数？

1.激活函数：是在预测时应用于一层神经元的函数。（比如前面用到的relu函数）

2.要使一个函数作为激活函数，需要满足几个限制条件：

- 约束一：函数必须连续且定义域是无穷的。
- 约束二：好的激活函数是单调的，不会改变方向。（严格来说，这个特定的限制条件不是必须的。可以对非单调函数进行优化。）
- 约束三：好的激活函数是非线性的（扭曲或反转）。
- 约束四：合适的激活函数（及其导数）应该可以高效计算。

## 二、标准隐藏层激活函数

**基础激活函数：sigmoid**

- sigmoid是一个伟大的激活函数，因为它能平滑的将输入从无穷大的空间压缩到0到1之间。
- 很多情况下，这可以让你把单个神经元的输出解释为一个概率。因此，在隐藏层和输出层中使用这种非线性函数。
- 图像：<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\9_01.jpg" style="zoom: 33%;" />

**tanh激活函数：**

- sigmoid函数能够给出不同程度的正相关性；tanh它的取值在-1到1之间，可以引入一部分负相关性。
- 负相关性对于隐藏层来说作用很大：在许多问题中，tanh在隐藏层中的表现优于sigmoid。
- 图像：<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\9_02.jpg" style="zoom: 33%;" />

## 三、标准输出层激活函数

1.对隐藏层来说最好的激活函数可能与对输出层来说最好的激活函数与很大的不同。

2.输出层主要有三种类型：

- 类型一：预测原始数据值（没有激活函数）----------（最直观但最不常见的输出层类型）
- 类型二：预测不相关的“是”或“否”的概率（sigmoid）-----------（当神经网络有隐藏层时，同时预测多个结果是有好处的。通常网络在预测一个标签时会学习到对预测其他标签有用的东西。）
- 类型三：预测“哪一个”的概率（softmax）

3.不同数字具有相同特征。让神经网络相信这一点有好处。（MNIST数据集中的数字并非完全不同:它们有重叠的像素。）

4.计算softmax：

- softmax计算每个输入值的指数形式，然后除以该层的和。
- softmax的好处是，神经网络对一个值的预测越高，它对所有其他值的预测就越低。它增加了信号衰减的锐度，鼓励网络以非常高的概率预测某项输出。
- 要调整它的执行力度，可以在进行指数运算时使用略高于或低于e的数做底。数值越小衰减越少，数值越大衰减越大。但大多数人还是坚持使用e。

## 四、激活函数使用说明

1.要记住：某一层的输入是指应用非线性函数之前的值。

2.**relu函数：**

- 图像：<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\9_03.jpg" style="zoom: 25%;" />
- 对于正数来说，relu的斜率正好是1；对于负数来说，relu的斜率恰好是0。（也就是说，如果预测结果为正，则对该函数的输入进行（一点点）修改将会产生1:1的影响。如果预测结果为负，则对输入的微小改变将会产生0:1的影响（也就是没有影响）。relu图像的斜率能够告诉我们，如果relu的输入发生（一定量）的变化，那么它的输出将会发生则怎样的变化。）

3.激活函数的斜率可被看成一个指示器，用来表示对输入的（一定量）更改会对输出带来多大影响。

4.要计算layer_delta，需要将反向传播的delta乘以该层的斜率。

5.非线性函数的引入，有助于使偶尔出现错误的训练实例难破坏已多次得到强化学习的效果。

6.多数优秀的激活函数可以将其输出转换为斜率（效率第一！）

7.大多数优秀的激活函数没有按照通常的习惯在曲线的某一点计算倒数，而是采用另一种方法：用神经元在正向传播时的输出来计算导数。

8.下面的小型表格中包含了到目前位置见过的所有函数以及它们的导数。

- input是NumPy向量（对应于网络层的输入）
- output是该层的预测结果
- 导数（deriv）指的是对应于每个节点上的激活函数的导数的激活导数向量。
- true是真实值的向量（通常用1表示正确的标签位置，0表示其他位置。）
- <img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\9_04.jpg" style="zoom: 33%;" />

9.Upgrading_our_MNIST_Network_01.ipynb   （tanh作为隐藏层的激活函数，softmax作为输出层的激活函数，alpha进行了调优。-----------这个网络达到了更高的测试精度——87%）

```python
import numpy as np, sys
np.random.seed(1)

from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])

one_hot_labels = np.zeros((len(labels),10))
for i,l in enumerate(labels):
    one_hot_labels[i][l] = 1
labels = one_hot_labels

test_images = x_test.reshape(len(x_test),28*28) / 255
test_labels = np.zeros((len(y_test),10))
for i,l in enumerate(y_test):
    test_labels[i][l] = 1

def tanh(x):
    return np.tanh(x)

def tanh2deriv(output):
    return 1 - (output ** 2)

def softmax(x):
    temp = np.exp(x)
    return temp / np.sum(temp, axis=1, keepdims=True)

alpha, iterations, hidden_size = (2, 300, 100)
pixels_per_image, num_labels = (784, 10)
batch_size = 100

weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))-0.01
weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1

for j in range(iterations):
    correct_cnt = 0
    for i in range(int(len(images) / batch_size)):
        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))
        layer_0 = images[batch_start:batch_end]
        layer_1 = tanh(np.dot(layer_0,weights_0_1))
        dropout_mask = np.random.randint(2,size=layer_1.shape)
        layer_1 *= dropout_mask * 2
        layer_2 = softmax(np.dot(layer_1,weights_1_2))

        for k in range(batch_size):
            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))

        layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])
        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)
        layer_1_delta *= dropout_mask

        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)

    test_correct_cnt = 0

    for i in range(len(test_images)):

        layer_0 = test_images[i:i+1]
        layer_1 = tanh(np.dot(layer_0,weights_0_1))
        layer_2 = np.dot(layer_1,weights_1_2)

        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))
    if(j % 10 == 0):
        sys.stdout.write("\n"+ \
         "I:" + str(j) + \
         " Test-Acc:"+str(test_correct_cnt/float(len(test_images)))+\
         " Train-Acc:" + str(correct_cnt/float(len(images))))
        
 #结果：
I:0 Test-Acc:0.394 Train-Acc:0.156
I:10 Test-Acc:0.6867 Train-Acc:0.723
I:20 Test-Acc:0.7025 Train-Acc:0.732
I:30 Test-Acc:0.734 Train-Acc:0.763
I:40 Test-Acc:0.7663 Train-Acc:0.794
I:50 Test-Acc:0.7913 Train-Acc:0.819
I:60 Test-Acc:0.8102 Train-Acc:0.849
I:70 Test-Acc:0.8228 Train-Acc:0.864
I:80 Test-Acc:0.831 Train-Acc:0.867
I:90 Test-Acc:0.8364 Train-Acc:0.885
I:100 Test-Acc:0.8407 Train-Acc:0.883
I:110 Test-Acc:0.845 Train-Acc:0.891
I:120 Test-Acc:0.8481 Train-Acc:0.901
I:130 Test-Acc:0.8505 Train-Acc:0.901
I:140 Test-Acc:0.8526 Train-Acc:0.905
I:150 Test-Acc:0.8555 Train-Acc:0.914
I:160 Test-Acc:0.8577 Train-Acc:0.925
I:170 Test-Acc:0.8596 Train-Acc:0.918
I:180 Test-Acc:0.8619 Train-Acc:0.933
I:190 Test-Acc:0.863 Train-Acc:0.933
I:200 Test-Acc:0.8642 Train-Acc:0.926
I:210 Test-Acc:0.8653 Train-Acc:0.931
I:220 Test-Acc:0.8668 Train-Acc:0.93
I:230 Test-Acc:0.8672 Train-Acc:0.937
I:240 Test-Acc:0.8681 Train-Acc:0.938
I:250 Test-Acc:0.8687 Train-Acc:0.937
I:260 Test-Acc:0.8684 Train-Acc:0.945
I:270 Test-Acc:0.8703 Train-Acc:0.951
I:280 Test-Acc:0.8699 Train-Acc:0.949
I:290 Test-Acc:0.8701 Train-Acc:0.94
```

# 第十章：卷积神经网络神经概论：关于边与角的学习

## 一、在多个位置复用权重

1. 如果需要在多个位置检测相同的特征，请使用相同的权重。
2. 神经网络最大的挑战是过拟合，指的是神经网络试图记忆一个数据集，而不是从中学习可以泛化到还没有见过的数据的有用抽象。（也就是说，神经网络学会的是基于数据集中的噪声进行预测，而不是依赖于基本信号。）
3. 过拟合的产生通常是由于当前网络参数的数量多于学习特定数据集所需要的参数数量。（当神经网络有很多参数，但并没有很多训练样例时，过拟合是很难避免的。）
4. 更好的防止过拟合的方法：**网络结构**。（网络结构指的是，在神经网络中，因为我们相信能够在多个位置检测到相同的模式，所以可以有选择地重用针对多个目标的权重。这可以显著地减少过拟合，并导致模型的精度更高，因为它降低了权重数量与数据量的比例。）
5. 在神经网络中，最著名且最广泛使用的网络结构叫做卷积，当做为一层使用时叫做卷积层。
6. **网络结构的诀窍：**当神经网络需要在多处使用相同的想法时，应试着在这些地方使用相同的权重。这样做会使那些权重有更多的样本可以学习并提高泛化能力，从而让权重更智能。

## 二、卷积层

1. 化整为0，将许多小线性神经元层在各处重用。

2. 卷积层背后的核心思想是：它不是一个大的、稠密的线性神经元层，在其中从每个输入到每个输出都有连接，二是由很多非常小的线性层构成，每个线性层通常拥有少于25个输入和一个输出，我们可以在任意输入位置使用它。每个小神经元层都被称为卷积核，但它实际上知识一个很小的线性层，接收少量的输入并作为单一输出。

3. 一个卷积层通常包含很多卷积核。

4. 概念：

   - 矩阵求和（求和池化）
   - 均值（平均池化）
   - 按元素计算最大值（最大池化）

5. 基于NumPy的简单实现：

   Upgrading_our_MNIST_Network_01.ipynb   (用卷积层替换掉在第九章中提到的网络中的第一层，能够减少几个百分点的误差。-----这里预测精度达到了87.74%)

   ```python
   import numpy as np, sys
   np.random.seed(1)
   
   from keras.datasets import mnist
   
   (x_train, y_train), (x_test, y_test) = mnist.load_data()
   
   images, labels = (x_train[0:1000].reshape(1000,28*28) / 255,
                     y_train[0:1000])
   
   
   one_hot_labels = np.zeros((len(labels),10))
   for i,l in enumerate(labels):
       one_hot_labels[i][l] = 1
   labels = one_hot_labels
   
   test_images = x_test.reshape(len(x_test),28*28) / 255
   test_labels = np.zeros((len(y_test),10))
   for i,l in enumerate(y_test):
       test_labels[i][l] = 1
   
   def tanh(x):
       return np.tanh(x)
   
   def tanh2deriv(output):
       return 1 - (output ** 2)
   
   def softmax(x):
       temp = np.exp(x)
       return temp / np.sum(temp, axis=1, keepdims=True)
   
   alpha, iterations = (2, 300)
   pixels_per_image, num_labels = (784, 10)
   batch_size = 128
   
   input_rows = 28
   input_cols = 28
   
   kernel_rows = 3
   kernel_cols = 3
   num_kernels = 16
   
   hidden_size = ((input_rows - kernel_rows) * 
                  (input_cols - kernel_cols)) * num_kernels
   
   # weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))-0.01
   kernels = 0.02*np.random.random((kernel_rows*kernel_cols,
                                    num_kernels))-0.01
   
   weights_1_2 = 0.2*np.random.random((hidden_size,
                                       num_labels)) - 0.1
   
   
   #这个函数为整批图像选择相同的子区域
   def get_image_section(layer,row_from, row_to, col_from, col_to):
       section = layer[:,row_from:row_to,col_from:col_to]
       return section.reshape(-1,1,row_to-row_from, col_to-col_from)
   
   for j in range(iterations):
       correct_cnt = 0
       for i in range(int(len(images) / batch_size)):
           batch_start, batch_end=((i * batch_size),((i+1)*batch_size))
           layer_0 = images[batch_start:batch_end]
           layer_0 = layer_0.reshape(layer_0.shape[0],28,28)
           layer_0.shape
   
           sects = list()
           
           #for循环遍历了图像中的每个（kernel_rows×kernel_cols）子区域，将它们放入一个名为sects（切片）的列表中，然后sects列表中的部分连接起来并转换为一种特殊形状。
           for row_start in range(layer_0.shape[1]-kernel_rows):
               for col_start in range(layer_0.shape[2] - kernel_cols):
                   sect = get_image_section(layer_0,
                                            row_start,
                                            row_start+kernel_rows,
                                            col_start,
                                            col_start+kernel_cols)
                   sects.append(sect)
   
           expanded_input = np.concatenate(sects,axis=1)
           es = expanded_input.shape
           flattened_input = expanded_input.reshape(es[0]*es[1],-1)
   
           kernel_output = flattened_input.dot(kernels)
           layer_1 = tanh(kernel_output.reshape(es[0],-1))
           dropout_mask = np.random.randint(2,size=layer_1.shape)
           layer_1 *= dropout_mask * 2
           layer_2 = softmax(np.dot(layer_1,weights_1_2))
   
           for k in range(batch_size):
               labelset = labels[batch_start+k:batch_start+k+1]
               _inc = int(np.argmax(layer_2[k:k+1]) == 
                                  np.argmax(labelset))
               correct_cnt += _inc
   
           layer_2_delta = (labels[batch_start:batch_end]-layer_2)\
                           / (batch_size * layer_2.shape[0])
           layer_1_delta = layer_2_delta.dot(weights_1_2.T) * \
                           tanh2deriv(layer_1)
           layer_1_delta *= dropout_mask
           weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
           l1d_reshape = layer_1_delta.reshape(kernel_output.shape)
           k_update = flattened_input.T.dot(l1d_reshape)
           kernels -= alpha * k_update
       
       test_correct_cnt = 0
   
       for i in range(len(test_images)):
   
           layer_0 = test_images[i:i+1]
   #         layer_1 = tanh(np.dot(layer_0,weights_0_1))
           layer_0 = layer_0.reshape(layer_0.shape[0],28,28)
           layer_0.shape
   
           sects = list()
           for row_start in range(layer_0.shape[1]-kernel_rows):
               for col_start in range(layer_0.shape[2] - kernel_cols):
                   sect = get_image_section(layer_0,
                                            row_start,
                                            row_start+kernel_rows,
                                            col_start,
                                            col_start+kernel_cols)
                   sects.append(sect)
   
           expanded_input = np.concatenate(sects,axis=1)
           es = expanded_input.shape
           flattened_input = expanded_input.reshape(es[0]*es[1],-1)
   
           kernel_output = flattened_input.dot(kernels)
           layer_1 = tanh(kernel_output.reshape(es[0],-1))
           layer_2 = np.dot(layer_1,weights_1_2)
   
           test_correct_cnt += int(np.argmax(layer_2) == 
                                   np.argmax(test_labels[i:i+1]))
       if(j % 1 == 0):
           sys.stdout.write("\n"+ \
            "I:" + str(j) + \
            " Test-Acc:"+str(test_correct_cnt/float(len(test_images)))+\
            " Train-Acc:" + str(correct_cnt/float(len(images))))
           
    #结果：
   I:0 Test-Acc:0.0288 Train-Acc:0.055
   I:1 Test-Acc:0.0273 Train-Acc:0.037
   I:2 Test-Acc:0.028 Train-Acc:0.037
   I:3 Test-Acc:0.0292 Train-Acc:0.04
   I:4 Test-Acc:0.0339 Train-Acc:0.046
   I:5 Test-Acc:0.0478 Train-Acc:0.068
   I:6 Test-Acc:0.076 Train-Acc:0.083
   I:7 Test-Acc:0.1316 Train-Acc:0.096
   I:8 Test-Acc:0.2137 Train-Acc:0.127
   I:9 Test-Acc:0.2941 Train-Acc:0.148
   I:10 Test-Acc:0.3563 Train-Acc:0.181
   I:11 Test-Acc:0.4023 Train-Acc:0.209
   I:12 Test-Acc:0.4358 Train-Acc:0.238
   I:13 Test-Acc:0.4473 Train-Acc:0.286
   I:14 Test-Acc:0.4389 Train-Acc:0.274
   I:15 Test-Acc:0.3951 Train-Acc:0.257
   I:16 Test-Acc:0.2222 Train-Acc:0.243
   I:17 Test-Acc:0.0613 Train-Acc:0.112
   I:18 Test-Acc:0.0266 Train-Acc:0.035
   I:19 Test-Acc:0.0127 Train-Acc:0.026
   .....................................
   I:286 Test-Acc:0.8792 Train-Acc:0.822
   I:287 Test-Acc:0.8791 Train-Acc:0.817
   I:288 Test-Acc:0.8769 Train-Acc:0.814
   I:289 Test-Acc:0.8785 Train-Acc:0.807
   I:290 Test-Acc:0.8778 Train-Acc:0.817
   I:291 Test-Acc:0.8794 Train-Acc:0.82
   I:292 Test-Acc:0.8804 Train-Acc:0.824
   I:293 Test-Acc:0.8779 Train-Acc:0.812
   I:294 Test-Acc:0.8784 Train-Acc:0.816
   I:295 Test-Acc:0.877 Train-Acc:0.817
   I:296 Test-Acc:0.8767 Train-Acc:0.826
   I:297 Test-Acc:0.8774 Train-Acc:0.816
   I:298 Test-Acc:0.8774 Train-Acc:0.804
   I:299 Test-Acc:0.8774 Train-Acc:0.814
   
   ```

6. 层叠卷积层是促成非常深的神经网络（以及“深度学习”一次流行）的主要进展之一。

# 第十一章：能够理解自然语言的神经网络：国王-男人+女人=？

## 一、自然语言处理（NLP）

1. NLP可被划分为一系列任务或挑战的集合。

2. 下面列出常见的NLP分类问题:

   - 使用文档中的字符,预测单词的开始和结束位置。
   - 使用文档中的单词,预测句子的开头和结束位置。
   - 使用句子中的单词,预测每个单词的词性。
   - 在句子中,使用单词,预测短语的开头和结尾。
   - 在句子中,使用单词,预测命名实体(例如人、地点、事物)指代的开始结束的位置。
   - 在文档中,使用句子,预测哪个代词指代同一个人/地点/事物。
   - 在句子中,使用单词,预测句子的情感色彩。

   3.一般来说,NLP任务希望能够做到以下三件事之一:

   - 标记文本区域(如词性标注,情感分类或命名实体识别);
   - 链接两个以上的文本区域(例如共指,指的是识别表示同一个实体的名词短语或代词,并将其进行归类——这里的实体一般来自真实世界,通常是一个人,一个地方或其他命名实体),
   - 试着填补基于上下文的信息空缺(例如用完形填空的形式补齐单词缺失的部分)。

   4.深度学习已经在NLP领域得到广泛应用：词向量算法和递归神经网络。

## 二、NLP监督学习

1. 输入句子，输出结果。
2. 由于神经网络只能将输入的数字映射到输出的数字，所以第一步是将文本转换成数字形式。<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\10_01.jpg" style="zoom: 25%;" />
3. 我们将文本转换成数字的目标可以看做：使输入和输出之间的相关性对神经网络来说更明显。这将有助于更快的训练和更好的泛化。

## 三、IMDB电影评论数据集

1. 预测人们发表的评论是正面的还是负面的。
2. 在输入数据中提取单词相关性。-------------词袋模型：给定电影评论中的词汇，预测其情绪。
3. 创建一个能够表示电影评论词汇的输入矩阵。（其中每一行（向量）对应于每个电影评论，每一列表示评论中是否包含词汇表中的特定单词。找到评论中的每个单词在词汇列表中对应的位置，在这个对应的位置插入1，剩余位置插入0.----------------这种存储形式称为one-hot编码，是二进制数据最常见的编码格式。）
4. One_hot_01.ipynb   

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\10_02.jpg" style="zoom:25%;" />

```python
#基于输入数据中可能出现的词汇列表，表示某个输入数据点中对应的词汇是否存在。
import numpy as np

onehots={}
onehots['cat']=np.array([1,0,0,0])
onehots['the']=np.array([0,1,0,0])
onehots['dog']=np.array([0,0,1,0])
onehots['sat']=np.array([0,0,0,1])

sentence =['the','cat','sat']

x = onehots[sentence[0]]+ \
    onehots[sentence[1]]+ \
    onehots[sentence[2]]

print("Sent Encoding:"+str(x))

#结果：
Sent Encoding:[1 1 0 1]
```

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\10_03.jpg" style="zoom:25%;" />

5.如果某个单词出现了多次，有多重方法来处理。此处用例子（“cat cat cat”）来说明。（对NLP来说，方式二更合适）

- 方式一：将cat向量求和三次（结果是[3,0,0,0]）
- 方式二：只取cat一次（结果是[1,0,0,0]）

6.基于编码策略和已掌握的神经网络，可以预测电影评论的情绪色彩。

7.数据预处理部分：（  将电影评论词汇的输入转换为目标数据集target_dataset为0-1矩阵）

```python
import sys

f = open('F:\DL\IMDB\\reviews.txt')
raw_reviews = f.readlines()
f.close()

f = open('F:\DL\IMDB\\labels.txt')
raw_labels = f.readlines()
f.close()

tokens = list(map(lambda x:set(x.split(" ")),raw_reviews))

vocab = set()
for sent in tokens:
    for word in sent:
        if(len(word)>0):
            vocab.add(word)
vocab = list(vocab)

word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i

input_dataset = list()
for sent in tokens:
    sent_indices = list()
    for word in sent:
        try:
            sent_indices.append(word2index[word])
        except:
            ""
    input_dataset.append(list(set(sent_indices)))

target_dataset = list()
for label in raw_labels:
    if label == 'positive\n':
        target_dataset.append(1)
    else:
        target_dataset.append(0)

```

8.对影评预测的完整代码

IMDB_Prediction_02.ipynb

```python
import sys

f = open('F:\DL\IMDB\\reviews.txt')
raw_reviews = f.readlines()
f.close()

f = open('F:\DL\IMDB\\labels.txt')
raw_labels = f.readlines()
f.close()

tokens = list(map(lambda x:set(x.split(" ")),raw_reviews))

vocab = set()
for sent in tokens:
    for word in sent:
        if(len(word)>0):
            vocab.add(word)
vocab = list(vocab)

word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i

input_dataset = list()
for sent in tokens:
    sent_indices = list()
    for word in sent:
        try:
            sent_indices.append(word2index[word])
        except:
            ""
    input_dataset.append(list(set(sent_indices)))

target_dataset = list()
for label in raw_labels:
    if label == 'positive\n':
        target_dataset.append(1)
    else:
        target_dataset.append(0)
        

        
        
import numpy as np
np.random.seed(1)

def sigmoid(x):
    return 1/(1 + np.exp(-x))

alpha, iterations = (0.01, 2)
hidden_size = 100

weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1
weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1

correct,total = (0,0)
for iter in range(iterations):
    
    # train on first 24,000（在前24000条电影评论上进行训练）
    for i in range(len(input_dataset)-1000):

        x,y = (input_dataset[i],target_dataset[i])
        layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) #embed + sigmoid   嵌入层+sigmoid激活函数
        layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) # linear + softmax   线性层+softmax激活函数

        layer_2_delta = layer_2 - y # compare pred with truth（将预测结果和真值进行比较）
        layer_1_delta = layer_2_delta.dot(weights_1_2.T) #backprop（反向传播）

        weights_0_1[x] -= layer_1_delta * alpha
        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha

        if(np.abs(layer_2_delta) < 0.5):
            correct += 1
        total += 1
        if(i % 10 == 9):
            progress = str(i/float(len(input_dataset)))
            sys.stdout.write('\rIter:'+str(iter)\
                             +' Progress:'+progress[2:4]\
                             +'.'+progress[4:6]\
                             +'% Training Accuracy:'\
                             + str(correct/float(total)) + '%')
    print()
correct,total = (0,0)
for i in range(len(input_dataset)-1000,len(input_dataset)):

    x = input_dataset[i]
    y = target_dataset[i]

    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))
    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))
    
    if(np.abs(layer_2 - y) < 0.5):
        correct += 1
    total += 1
print("Test Accuracy:" + str(correct / float(total)))

#结果：
Iter:0 Progress:95.99% Training Accuracy:0.8314166666666667%%
Iter:1 Progress:95.99% Training Accuracy:0.8659791666666666%
Test Accuracy:0.848
```

## 四、神经网络结构

1.隐藏层从根本上意味着对前一层的数据进行分组。

2.如果输入数据点的分组满足下面两个条件，那么它被认为是有用的：

- 分组必须对输出标签的预测有用
- 如果分组结果是你所关心的数据中某种实际出现的现象，那么分组是有用的。（糟糕的分组只能记住数据，而好的分组能够发现其中在语言学上有用的现象。）

3.具有类似预测能力的单词应该属于类似的组（由隐藏神经元设置）。------------与类似标签（正面评价或负面评价）相关的单词将具有类似的权重，这些权重将它们与各种隐藏神经元连接起来。

4.就神经网络而言，当且仅当一个神经元连接到下一层和/或上一层的权值与同一层的其他神经元相似时，它与这个神经元才具有相似的意义。

## 五、词向量的比较

1.要找出与目标条目最想似的单词，只需要将每个单词的向量（矩阵的行）与目标条目的向量进行比较。这里选择的比较方法被称为欧氏距离。

2.在    **对影评预测的完整代码IMDB_Prediction_02.ipynb**      代码下面继续补充代码：（根据神经网络查询最相似的单词（神经元））

```python
from collections import Counter
import math 

def similar(target='beautiful'):
    target_index = word2index[target]
    scores = Counter()
    for word,index in word2index.items():
        raw_difference = weights_0_1[index] - (weights_0_1[target_index])
        squared_difference = raw_difference * raw_difference
        scores[word] = -math.sqrt(sum(squared_difference))

    return scores.most_common(10)


print(similar('beautiful'))
#结果：
[('beautiful', -0.0), ('joan', -0.7030876428228362), ('will', -0.7053280210871152), ('human', -0.7128186638989764), ('unique', -0.7171660852077054), ('finest', -0.7206387123560463), ('criticism', -0.7253927098599565), ('knowing', -0.7275462007774962), ('different', -0.7283468103201057), ('powerful', -0.7285418904548122)]
#分析：与每个单词最相似的词是它本身，然后是和目标条目使用方法相似的词汇。


print(similar('terrible'))
#结果：
[('terrible', -0.0), ('worse', -0.748495808802653), ('disappointment', -0.7714183266927317), ('dull', -0.7794529003229554), ('poor', -0.7903572921957338), ('avoid', -0.8011599795429407), ('annoying', -0.8027402339579557), ('boring', -0.8148297834280082), ('lame', -0.8291661777146029), ('fails', -0.8302352578296499)]
```

3.神经元在网络中的含义来自于预测目标的标签。（神经网络中的每一件事的推导都基于关联抽象，都致力于试图正确的做出预测。）

## 六、完形填空

1.本例使用的神经网络与前一个示例几乎完全相同，只是做了一些修改。此例，将文本划分成5个单词一组的短语，删除一个单词（一个重点词汇），尝试训练一个网络，利用去掉单词之后的剩余部分来预测去掉的那个词汇。我们此处使用负抽样的技巧来让网络训练更快一些。

2.Filling_in_the_Blank_03.ipynb   （基于在同一个短语中出现的可能性（与情绪无关）聚在一起。）（完形填空案例和词向量案例的架构都是三层，交叉熵，sigmoid非线性。数据集也相似。但是误差函数有根本的不同（--------因为选择了两个不同的学习目标值，一个是情感的正负极性，另一个是句子的完形填空），导致每个网络中单词聚集的方式不同。）

```python
import sys,random,math
from collections import Counter
import numpy as np

np.random.seed(1)
random.seed(1)
f = open('F:\DL\IMDB\\reviews.txt')
raw_reviews = f.readlines()
f.close()

tokens = list(map(lambda x:(x.split(" ")),raw_reviews))
wordcnt = Counter()
for sent in tokens:
    for word in sent:
        wordcnt[word] -= 1
vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))

word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i

concatenated = list()
input_dataset = list()
for sent in tokens:
    sent_indices = list()
    for word in sent:
        try:
            sent_indices.append(word2index[word])
            concatenated.append(word2index[word])
        except:
            ""
    input_dataset.append(sent_indices)
concatenated = np.array(concatenated)
random.shuffle(input_dataset)


alpha, iterations = (0.05, 2)
hidden_size,window,negative = (50,2,5)

weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2
weights_1_2 = np.random.rand(len(vocab),hidden_size)*0

layer_2_target = np.zeros(negative+1)
layer_2_target[0] = 1

def similar(target='beautiful'):
  target_index = word2index[target]

  scores = Counter()
  for word,index in word2index.items():
    raw_difference = weights_0_1[index] - (weights_0_1[target_index])
    squared_difference = raw_difference * raw_difference
    scores[word] = -math.sqrt(sum(squared_difference))
  return scores.most_common(10)

def sigmoid(x):
    return 1/(1 + np.exp(-x))

for rev_i,review in enumerate(input_dataset * iterations):
  for target_i in range(len(review)):
        
    # since it's really expensive to predict every vocabulary
    # we're only going to predict a random subset（每次只预测一个随机子集，因为对每个词汇都进行预测的代价过于高昂）
    target_samples = [review[target_i]]+list(concatenated\
    [(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])

    left_context = review[max(0,target_i-window):target_i]
    right_context = review[target_i+1:min(len(review),target_i+window)]

    layer_1 = np.mean(weights_0_1[left_context+right_context],axis=0)
    layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))
    layer_2_delta = layer_2 - layer_2_target
    layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])

    weights_0_1[left_context+right_context] -= layer_1_delta * alpha
    weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1)*alpha

  if(rev_i % 250 == 0):
    sys.stdout.write('\rProgress:'+str(rev_i/float(len(input_dataset)
        *iterations)) + "   " + str(similar('terrible')))#这里预测不同单词要进行对应的修改（如果预测‘terrible’，需要将‘beautiful’--->'terrible'）
  sys.stdout.write('\rProgress:'+str(rev_i/float(len(input_dataset)
        *iterations)))
print(similar('terrible'))
#结果：
Progress:0.99998 [('terrible', -0.0), ('horrible', -2.813080113647467), ('brilliant', -2.9938592877512216), ('superb', -3.6431531304485807), ('pathetic', -3.7671686344966715), ('masterful', -3.8121253883797235), ('phenomenal', -3.8311823974324213), ('mediocre', -3.882874210242489), ('marvelous', -3.8954688664247095), ('miserable', -4.0666134871390165)])]5)]][('terrible', -0.0), ('horrible', -2.793877548649756), ('brilliant', -3.2617421953410575), ('pathetic', -3.7769433267039156), ('superb', -3.8847397745984042), ('mediocre', -3.925206573623755), ('phenomenal', -3.9434025044366896), ('masterful', -3.9464500407757876), ('bad', -3.9890595240154383), ('marvelous', -3.999728063101329)]
    
    
print(similar('beautiful'))
#结果：
Progress:0.99998 [('beautiful', -0.0), ('lovely', -3.098570899399311), ('creepy', -3.274427787369829), ('fantastic', -3.3789566307967798), ('glamorous', -3.499868816839923), ('nightmarish', -3.5711653076793763), ('drab', -3.6076766088884367), ('cute', -3.6327932371018337), ('heartwarming', -3.6383442717329326), ('spooky', -3.689035489399073)]521285)]]]]4)]][('beautiful', -0.0), ('creepy', -3.1582892686522883), ('lovely', -3.18366112900597), ('fantastic', -3.4744260467688646), ('glamorous', -3.5033589032448678), ('spooky', -3.5333994048000403), ('nightmarish', -3.5535868170570026), ('heartwarming', -3.564444398069048), ('drab', -3.619371953548581), ('pedestrian', -3.6319897172206828)]
```

3.词向量与完形填空结果对比：---------------**分析见2中代码上面。**

```python
#一、print(similar('beautiful'))

词向量：
print(similar('beautiful'))
#结果：
[('beautiful', -0.0), ('joan', -0.7030876428228362), ('will', -0.7053280210871152), ('human', -0.7128186638989764), ('unique', -0.7171660852077054), ('finest', -0.7206387123560463), ('criticism', -0.7253927098599565), ('knowing', -0.7275462007774962), ('different', -0.7283468103201057), ('powerful', -0.7285418904548122)]

完形填空：
print(similar('beautiful'))
#结果：
Progress:0.99998 [('beautiful', -0.0), ('lovely', -3.098570899399311), ('creepy', -3.274427787369829), ('fantastic', -3.3789566307967798), ('glamorous', -3.499868816839923), ('nightmarish', -3.5711653076793763), ('drab', -3.6076766088884367), ('cute', -3.6327932371018337), ('heartwarming', -3.6383442717329326), ('spooky', -3.689035489399073)]521285)]]]]4)]][('beautiful', -0.0), ('creepy', -3.1582892686522883), ('lovely', -3.18366112900597), ('fantastic', -3.4744260467688646), ('glamorous', -3.5033589032448678), ('spooky', -3.5333994048000403), ('nightmarish', -3.5535868170570026), ('heartwarming', -3.564444398069048), ('drab', -3.619371953548581), ('pedestrian', -3.6319897172206828)]
    
    
#二、print(similar('terrible'))

词向量：
print(similar('terrible'))
#结果：
[('terrible', -0.0), ('worse', -0.748495808802653), ('disappointment', -0.7714183266927317), ('dull', -0.7794529003229554), ('poor', -0.7903572921957338), ('avoid', -0.8011599795429407), ('annoying', -0.8027402339579557), ('boring', -0.8148297834280082), ('lame', -0.8291661777146029), ('fails', -0.8302352578296499)]

完形填空：
print(similar('terrible'))
#结果：
Progress:0.99998 [('terrible', -0.0), ('horrible', -2.813080113647467), ('brilliant', -2.9938592877512216), ('superb', -3.6431531304485807), ('pathetic', -3.7671686344966715), ('masterful', -3.8121253883797235), ('phenomenal', -3.8311823974324213), ('mediocre', -3.882874210242489), ('marvelous', -3.8954688664247095), ('miserable', -4.0666134871390165)])]5)]][('terrible', -0.0), ('horrible', -2.793877548649756), ('brilliant', -3.2617421953410575), ('pathetic', -3.7769433267039156), ('superb', -3.8847397745984042), ('mediocre', -3.925206573623755), ('phenomenal', -3.9434025044366896), ('masterful', -3.9464500407757876), ('bad', -3.9890595240154383), ('marvelous', -3.999728063101329)]
   
```

## 七、损失函数

1.选择想让网络学习的内容的过程称为“智能定位”。

2.“智能定位”的方法：

- 控制输入和预测目标的值
- 调整网络测量误差的方式
- 网络层的参数数量和类型
- 应用的正则化类型

3.神经网络并不能真正从数据中学习，它们只是使损失函数最小化。

4.损失函数的选择决定了神经网络学到的知识。

5.**误差函数**更正式术语是**损失函数**或**目标函数**。

6.网络过拟合：---------------可以通过选择更简单的非线性关系、更少的网络层参数、更浅的网络架构、更大的数据集或者更激进的正则化技术，来增强损失函数。

7.学习过程就是建立一个损失函数，然后将它最小化。

## 八、单词类比

1.单词类比是先前构建的网络的一个有趣结果。

2.完形填空的任务能够为词汇创建有趣的嵌入表达，这种现象称为单词类比。意味着你可以基于不同单词的嵌入向量，对它们进行基本的代数运算。

3.King - Man + Woman ~= Queen_04.ipynb      （利用前面的网络，只是修改了similar函数为analogy）

```python
import sys,random,math
from collections import Counter
import numpy as np

np.random.seed(1)
random.seed(1)
f = open('F:\DL\IMDB\\reviews.txt')
raw_reviews = f.readlines()
f.close()

tokens = list(map(lambda x:(x.split(" ")),raw_reviews))
wordcnt = Counter()
for sent in tokens:
    for word in sent:
        wordcnt[word] -= 1
vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))

word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i

concatenated = list()
input_dataset = list()
for sent in tokens:
    sent_indices = list()
    for word in sent:
        try:
            sent_indices.append(word2index[word])
            concatenated.append(word2index[word])
        except:
            ""
    input_dataset.append(sent_indices)
concatenated = np.array(concatenated)
random.shuffle(input_dataset)


alpha, iterations = (0.05, 2)
hidden_size,window,negative = (50,2,5)

weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2
weights_1_2 = np.random.rand(len(vocab),hidden_size)*0

layer_2_target = np.zeros(negative+1)
layer_2_target[0] = 1

def analogy(positive=['terrible','good'],negative=['bad']):
    
    norms = np.sum(weights_0_1 * weights_0_1,axis=1)
    norms.resize(norms.shape[0],1)
    
    normed_weights = weights_0_1 * norms
    
    query_vect = np.zeros(len(weights_0_1[0]))
    for word in positive:
        query_vect += normed_weights[word2index[word]]
    for word in negative:
        query_vect -= normed_weights[word2index[word]]
    
    scores = Counter()
    for word,index in word2index.items():
        raw_difference = weights_0_1[index] - query_vect
        squared_difference = raw_difference * raw_difference
        scores[word] = -math.sqrt(sum(squared_difference))
        
    return scores.most_common(10)[1:]


def sigmoid(x):
    return 1/(1 + np.exp(-x))

for rev_i,review in enumerate(input_dataset * iterations):
  for target_i in range(len(review)):
        
    # since it's really expensive to predict every vocabulary
    # we're only going to predict a random subset（每次只预测一个随机子集，因为对每个词汇都进行预测的代价过于高昂）
    target_samples = [review[target_i]]+list(concatenated\
    [(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])

    left_context = review[max(0,target_i-window):target_i]
    right_context = review[target_i+1:min(len(review),target_i+window)]

    layer_1 = np.mean(weights_0_1[left_context+right_context],axis=0)
    layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))
    layer_2_delta = layer_2 - layer_2_target
    layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])

    weights_0_1[left_context+right_context] -= layer_1_delta * alpha
    weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1)*alpha

  if(rev_i % 250 == 0):
    sys.stdout.write('\rProgress:'+str(rev_i/float(len(input_dataset)
        *iterations)) + "   " + str(analogy(['terrible','good'],['bad'])))#这里预测不同输入要进行对应的修改（如果预测analogy(['elizabeth','he'],['she'])，需要将print(analogy(['terrible','good'],['bad']))--->analogy(['elizabeth','he'],['she'])）
  sys.stdout.write('\rProgress:'+str(rev_i/float(len(input_dataset)
        *iterations)))


print(analogy(['terrible','good'],['bad']))
#结果：
[('terrific', -210.46593317724228),
 ('perfect', -210.52652806032205),
 ('worth', -210.53162266358495),
 ('good', -210.55072184482773),
 ('terrible', -210.58429046605724),
 ('decent', -210.87945442008805),
 ('superb', -211.01143515971094),
 ('great', -211.1327058081335),
 ('worthy', -211.13577238103477)]


#print(analogy(['elizabeth','he'],['she']))
#结果：
[('simon', -193.82490698964878),
 ('obsessed', -193.91805919583555),
 ('stanwyck', -194.22311983847902),
 ('sandler', -194.22846640800597),
 ('branagh', -194.24551334589853),
 ('daniel', -194.24631020485714),
 ('peter', -194.29908544092078),
 ('tony', -194.31388897167716),
 ('aged', -194.35115773165094)]

```

4.单词类比------>数据中现有属性的线性压缩表达。

5.国王-男人+女人~=女王

# 第十二章：像莎士比亚一样写作的神经网络：变长数据的递归层

## 一、长度任意性的挑战

1.如果生成句子向量的方法没有反应你在两个句子之间观察到的相似度，那么网络也难以识别两个句子是相似的。

2.创建句子向量的方法：

- 一是通过连接
- 二是取一个句子中的每个单词的向量，然后求平均。

3.求平均的词嵌入是创建词嵌入有效的方法；它并不完美，但在刻画可能看起来非常复杂的单词之间关系方面做得很好。

## 二、平均词向量的神奇力量

1.平均词向量是做神经预测的特别强大的工具。

2.The_Surprising_Power_of_Averaged_Word_Vectors_01.ipynb

```python
import sys

f = open('F:\\DL\\IMDB\\reviews.txt')
raw_reviews = f.readlines()
f.close()

f = open('F:\\DL\\IMDB\\labels.txt')
raw_labels = f.readlines()
f.close()

tokens = list(map(lambda x:set(x.split(" ")),raw_reviews))

vocab = set()
for sent in tokens:
    for word in sent:
        if(len(word)>0):
            vocab.add(word)
vocab = list(vocab)

word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i

input_dataset = list()
for sent in tokens:
    sent_indices = list()
    for word in sent:
        try:
            sent_indices.append(word2index[word])
        except:
            ""
    input_dataset.append(list(set(sent_indices)))

target_dataset = list()
for label in raw_labels:
    if label == 'positive\n':
        target_dataset.append(1)
    else:
        target_dataset.append(0)
        
import sys,random,math
from collections import Counter
import numpy as np

alpha, iterations = (0.05, 2)
hidden_size,window,negative = (50,2,5)

weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2

norms = np.sum(weights_0_1 * weights_0_1,axis=1)
norms.resize(norms.shape[0],1)
normed_weights = weights_0_1 * norms

def make_sent_vect(words):
    indices = list(map(lambda x:word2index[x],filter(lambda x:x in word2index,words)))
    return np.mean(normed_weights[indices],axis=0)

reviews2vectors = list()
for review in tokens: # tokenized reviews
    reviews2vectors.append(make_sent_vect(review))
reviews2vectors = np.array(reviews2vectors)

def most_similar_reviews(review):
    v = make_sent_vect(review)
    scores = Counter()
    for i,val in enumerate(reviews2vectors.dot(v)):
        scores[i] = val
    most_similar = list()
    
    for idx,score in scores.most_common(3):
        most_similar.append(raw_reviews[idx][0:40])
    return most_similar

most_similar_reviews(['boring','awful'])

#结果
['nine minutes of psychedelic  pulsating  ',
 'you  d better choose paul verhoeven  s e',
 'long  boring  blasphemous . never have i']
```

3.信息是如何存储在这些向量嵌入中的？-------------对词嵌入进行平均时，其平均形状能够保持。（利用抽象思维，把词向量当做可观察到的弯曲的线条）

4.每个词向量对应的形状是唯一的，但是不同单词之间是有一定相似度的。

5.神经网络是如何使用嵌入的？--------------神经网络检测出与目标标签具有相关性的曲线。（具有相似意义的单词经常以某种方式在曲线中具有同一种弯曲特征：由权重之中的高低值存在的模式所形成的组合。）

6.关联抽象是如何处理这些输入曲线的？----------------本章是把这些单词嵌入求和称为一个句子嵌入。（此方法有一个缺点：将任意长度序列的信息存储为一个固定长度的向量时，如果你想存储的信息太多，最终句子向量（大量词汇向量的平均）会平均称为一条直线（元素都接近0的向量）。）

7.一个句子常常不等于一堆单词的聚合；不过，如果句子中有重复模式，那么这个句子向量可能有用，因为它保持了被求和的词向量中存在的最主要模式。

## 三、词袋向量的局限

1.如果对词嵌入进行平均，那么结果与单词顺序无关。（这种对词嵌入求和或求平均以形成一个短语或句子的嵌入方法，传统上叫做词袋法。）

2.本章重点是寻找一种与单词顺序相关的方式生成句子向量。

3.为序列生成向量的最著名和成功的方法之一就是递归神经网络（RNN）。

## 四、使用单位向量求词嵌入之和

1.在每步求和之间加了一部步：使用单位矩阵的向量矩阵乘法。

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\12_01.jpg" style="zoom:25%;" />

2.Matrices_that_Change_Absolutely_Nothing_02.ipynb   （这里我们使用的是单位矩阵，所以输出与输入是一样的向量。试想，如果我们不适用单位矩阵会发生什么？如果我们使用一种不同的矩阵会怎样？）

```python
import numpy as np

a=np.array([1,2,3])
b=np.array([0.1,0.2,0.3])
c=np.array([-1,-0.5,0])
d=np.array([0,0,0])

identity=np.eye(3)
print(identity)
print("----------------")

print(a.dot(identity))
print(b.dot(identity))
print(c.dot(identity))
print(d.dot(identity))
print("---------------")

this = np.array([2,4,6])
movie = np.array([10,10,10])
rocks = np.array([1,1,1])

print(this + movie + rocks)
print((this.dot(identity) + movie).dot(identity) + rocks)

#结果
[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]
----------------
[1. 2. 3.]
[0.1 0.2 0.3]
[-1.  -0.5  0. ]
[0. 0. 0.]
---------------
[13 15 17]
[13. 15. 17.]
```

3.单位矩阵是唯一能够在向量-矩阵乘法中保证返回相同向量的矩阵。其他矩阵都不能保证这个性质。

## 五、学习转移矩阵

1.如果我们使用与单位矩阵类似的方式生成句子向量，不过将单位矩阵换成一个不同的矩阵，使得生成的句子向量将因单词的不同顺序而不同。------------>那么有一个自然的问题：用哪种非单位矩阵（称转移矩阵）呢？

## 六、学习创建有用的句子向量

1.创建句子向量，做出预测，并通过它的各部分修改句子向量。

<img src="D:\360MoveData\Users\Lenovo\Desktop\1.《深度学习图解》\笔记\img\12_02.png" style="zoom:25%;" />

2.通过允许（初始单位矩阵的）矩阵变化（使其编程非单位矩阵），可以让神经网络学到如何创建这个矩阵，使得单词出现的顺序能够改变句子向量。

## 七、Python下的前向传播

1.Forward_Propagation_in_Python_03.ipynb   (在给定长度为3的句子向量时，用于预测下一个单词。)

```python
import numpy as np

def softmax(x_):
    x = np.atleast_2d(x_)  #将输入的数组转化为至少两维
    temp = np.exp(x)
    return temp / np.sum(temp, axis=1, keepdims=True)

#词向量
word_vects = {}
word_vects['yankees'] = np.array([[0.,0.,0.]])
word_vects['bears'] = np.array([[0.,0.,0.]])
word_vects['braves'] = np.array([[0.,0.,0.]])
word_vects['red'] = np.array([[0.,0.,0.]])
word_vects['socks'] = np.array([[0.,0.,0.]])
word_vects['lose'] = np.array([[0.,0.,0.]])
word_vects['defeat'] = np.array([[0.,0.,0.]])
word_vects['beat'] = np.array([[0.,0.,0.]])
word_vects['tie'] = np.array([[0.,0.,0.]])

#输入分类权重的句子嵌入
sent2output = np.random.rand(3,len(word_vects))

#权重转移矩阵
identity = np.eye(3)

#创建一个句子嵌入
layer_0 = word_vects['red']
layer_1 = layer_0.dot(identity) + word_vects['socks']
layer_2 = layer_1.dot(identity) + word_vects['defeat']

#在所有词汇之上做出预测
pred = softmax(layer_2.dot(sent2output))
print(pred)

#结果
[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
  0.11111111 0.11111111 0.11111111]]
```

2.反向传播！！！

- 使用bash命令下载Babi数据集（tasksv11）

- Babi_Data_04_04.ipynb

  ```python
  import sys,random,math
  from collections import Counter
  import numpy as np
  
f = open('F:\\DL\\Tasksv11\\tasksv11\\en\\qa1_single-supporting-fact_train.txt','r')
  raw = f.readlines()
  f.close()
  
  tokens = list()
  for line in raw[0:1000]:
      tokens.append(line.lower().replace("\n","").split(" ")[1:])
  
  print(tokens[0:3])
  
  #结果
  [['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\tbathroom\t1']]
  ```
  

## 八、任意长度的前向传播与反向传播

1.使用之前描述的同样逻辑做前向传播和反向传播。

## 九、任意长度的权重更新

1.困惑度（perplexity）:是预测标签与正确标签（单词）匹配的概率再使用对数函数、取反数、求指数得到的结果。

2.递归神经网络能够在任意长度的序列上做出预测。

3.Together_05.ipynb---------------（七八九的部分代码整合）

```python
import sys,random,math
from collections import Counter
import numpy as np

f = open('F:\\DL\\Tasksv11\\tasksv11\\en\\qa1_single-supporting-fact_train.txt','r')
raw = f.readlines()
f.close()

tokens = list()
for line in raw[0:1000]:
    tokens.append(line.lower().replace("\n","").split(" ")[1:])

#进行设置
vocab = set()
for sent in tokens:
    for word in sent:
        vocab.add(word)

vocab = list(vocab)

word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i
    
def words2indices(sentence):
    idx = list()
    for word in sentence:
        idx.append(word2index[word])
    return idx

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

np.random.seed(1)
embed_size = 10

# word embeddings
embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1

# embedding -> embedding (initially the identity matrix)
recurrent = np.eye(embed_size)

# sentence embedding for empty sentence
start = np.zeros(embed_size)

# embedding -> output weights
decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1

# one hot lookups (for loss function)
one_hot = np.eye(len(vocab))





# Forward Propagation with Arbitrary Length
def predict(sent):
    
    layers = list()
    layer = {}
    layer['hidden'] = start
    layers.append(layer)

    loss = 0

    # forward propagate
    preds = list()
    for target_i in range(len(sent)):

        layer = {}

        # try to predict the next term
        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))

        loss += -np.log(layer['pred'][sent[target_i]])

        # generate the next hidden state
        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]
        layers.append(layer)

    return layers, loss



# Backpropagation with Arbitrary Length
# forward
for iter in range(30000):
    alpha = 0.001
    sent = words2indices(tokens[iter%len(tokens)][1:])
    layers,loss = predict(sent) 

    # back propagate
    for layer_idx in reversed(range(len(layers))):
        layer = layers[layer_idx]
        target = sent[layer_idx-1]

        if(layer_idx > 0):  # if not the first layer
            layer['output_delta'] = layer['pred'] - one_hot[target]
            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())

            # if the last layer - don't pull from a later one becasue it doesn't exist
            if(layer_idx == len(layers)-1):
                layer['hidden_delta'] = new_hidden_delta
            else:
                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())
        else: # if the first layer
            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())


            
# Weight Update with Arbitrary Length
# forward
for iter in range(30000):
    alpha = 0.001
    sent = words2indices(tokens[iter%len(tokens)][1:])

    layers,loss = predict(sent) 

    # back propagate
    for layer_idx in reversed(range(len(layers))):
        layer = layers[layer_idx]
        target = sent[layer_idx-1]

        if(layer_idx > 0):
            layer['output_delta'] = layer['pred'] - one_hot[target]
            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())

            # if the last layer - don't pull from a 
            # later one becasue it doesn't exist
            if(layer_idx == len(layers)-1):
                layer['hidden_delta'] = new_hidden_delta
            else:
                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())
        else:
            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())

    # update weights
    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))
    for layer_idx,layer in enumerate(layers[1:]):
        
        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sent))
        
        embed_idx = sent[layer_idx]
        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sent))
        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))
        
    if(iter % 1000 == 0):
        print("Perplexity:" + str(np.exp(loss/len(sent))))
        
        
#结果
Perplexity:82.0397357244083
Perplexity:82.01641036857438
Perplexity:81.94814173071725
Perplexity:81.75715793948906
Perplexity:81.28379179889295
Perplexity:80.13646249700416
Perplexity:77.09396009459111
Perplexity:65.9703951162951
Perplexity:35.544881187597724
Perplexity:21.744306299850663
Perplexity:19.506393031162737
Perplexity:18.28678960611287
Perplexity:16.789130475628564
Perplexity:14.50009431588117
Perplexity:11.339138255217321
Perplexity:8.486902317506518
Perplexity:6.978177962334441
Perplexity:6.116097016447519
Perplexity:5.532477013723799
Perplexity:5.13177910983888
Perplexity:4.894887339599657
Perplexity:4.738861731124725
Perplexity:4.629805363860346
Perplexity:4.563287138658489
Perplexity:4.521726280872137
Perplexity:4.477564548573361
Perplexity:4.414015049184302
Perplexity:4.326978333068886
Perplexity:4.212492200541782
Perplexity:4.079266887134636
    
    
    
    
# Execution and Output Analysis

sent_index = 4

l,_ = predict(words2indices(tokens[sent_index]))

print(tokens[sent_index])

for i,each_layer in enumerate(l[1:-1]):
    input = tokens[sent_index][i]
    true = tokens[sent_index][i+1]
    pred = vocab[each_layer['pred'].argmax()]
    print("Prev Input:" + input + (' ' * (12 - len(input))) +\
          "True:" + true + (" " * (15 - len(true))) + "Pred:" + pred)
#结果
['sandra', 'moved', 'to', 'the', 'garden.']
Prev Input:sandra      True:moved          Pred:is
Prev Input:moved       True:to             Pred:to
Prev Input:to          True:the            Pred:the
Prev Input:the         True:garden.        Pred:bedroom.
```

# 第十三章：介绍自动优化：搭建深度学习框架

##  一、深度学习框架是什么？

1. 优秀的工具能够减少错误、加速开发并提高运行性能。
2. 此处是将根据最新的框架开发的趋势搭建一个轻量级的深度学习框架；自己搭建一个小框架会让自己顺利过渡到使用真正的框架上来。

##  二、张量（tensor）是什么

1. 张量是矩阵与向量的抽象形式。
2. 矩阵是向量的列表，向量是标量（单独的数字）的列表；张量是这种嵌套数字列表形式的抽象版本。
3. 向量是一维张量；矩阵式二维张量，更高维的被称为n维张量。
4. Part_1_Introduction_to_Tensors_01.ipynb    （它把所有数值信息都存储在一个NumPy数组（self.data）中，而且它支持一种张量操作（加法 add）。增加更多的操作是容易的：只需要在张量类中创建具有适当功能的其他函数即可。）

```python
import numpy as np

class Tensor (object):
    def __init__(self,data):
        self.data=np.array(data)
        
    def __add__(self,other):
        return Tensor(self.data+other.data)
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())
    
x=Tensor([1,2,3,4,5])

print(x)

y=x + x
print(y)

#结果
[1 2 3 4 5]
[ 2  4  6  8 10]
```

##  三、自动梯度计算（autograd）介绍

1. 计算梯度需要反向经过网络：首先要计算输出层的梯度，然后用它来计算倒数第二层的梯度，以此类推，知道为所有的权重都求出正确的梯度。
2. Part_2_Introduction_to_Autograd_02.iypnb

```python
import numpy as np

class Tensor (object):
    
    #creators是一个列表，包含创建当前张量（默认为None）用到的所有张量。（因此当两个张量x和y加到一起时，z包含两个creators，即x和y）.
    #creation_op是一个相关特性，存储了creators在创建过程中用到的指令。
    def __init__(self, data, creators=None, creation_op = None):
        self.data = np.array(data)
        self.creation_op = creation_op
        self.creators = creators
        self.grad = None
    
    def backward(self, grad):
        self.grad = grad
        
        if(self.creation_op == "add"):
            self.creators[0].backward(grad)
            self.creators[1].backward(grad)

    def __add__(self, other):
        return Tensor(self.data + other.data,  creators=[self,other], creation_op="add")
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())
    
x = Tensor([1,2,3,4,5])
y = Tensor([2,2,2,2,2])

z=x+y
z.backward(Tensor(np.array([1,1,1,1,1])))

print(x.grad)
print(y.grad)
print(z.creators)

print(z.creation_op)


print("----------------")

a = Tensor([1,2,3,4,5])
b = Tensor([2,2,2,2,2])
c = Tensor([5,4,3,2,1])
d = Tensor([-1,-2,-3,-4,-5])

e = a + b
f = c + d
g = e + f

g.backward(Tensor(np.array([1,1,1,1,1])))
print(a.grad)

#结果
[1 1 1 1 1]
[1 1 1 1 1]
[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]
add
----------------
[1 1 1 1 1]
```

3.在前向传播中创建的图称为动态计算图（动态计算图更容易写、更容易实验）。这是在新的深度学习框架中出现的（DyNet和PyTorch），在较老的框架（TensorFlow）中使用的是静态计算图（静态计算图有更快的运行速度）。

4.Part_3_Tensors_That_Are_Used_Multiple_Times_03.ipynb   （在这个例子中，变量b在创建f的过程中用了两次，因此，它的梯度应该是两个倒数之和：[2,2,2,2,2]）

```python
import numpy as np

class Tensor (object):
    
    #creators是一个列表，包含创建当前张量（默认为None）用到的所有张量。（因此当两个张量x和y加到一起时，z包含两个creators，即x和y）.
    #creation_op是一个相关特性，存储了creators在创建过程中用到的指令。
    def __init__(self, data, creators=None, creation_op = None):
        self.data = np.array(data)
        self.creation_op = creation_op
        self.creators = creators
        self.grad = None
    
    def backward(self, grad):
        self.grad = grad
        
        if(self.creation_op == "add"):
            self.creators[0].backward(grad)
            self.creators[1].backward(grad)

    def __add__(self, other):
        return Tensor(self.data + other.data,  creators=[self,other], creation_op="add")
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())
    
a = Tensor([1,2,3,4,5])
b = Tensor([2,2,2,2,2])
c = Tensor([5,4,3,2,1])

d = a + b
e = b + c
f = d + e
f.backward(Tensor(np.array([1,1,1,1,1])))

print(b.grad.data == np.array([2,2,2,2,2]))

#结果
[False, False, False, False, False]
```

5.Part_4_Upgrading_Autograd_to_Support_Multiple_Tensors_04.ipynb   （升级autograd以支持多次使用的张量）

  对Tensor对象增加了两个新特性：

- 一是，梯度可以累加，使得一个变量被使用超过一次时，它能够接收来自所有子节点的梯度。
- 二是，增加一个新函数    all_children_grads_accounted_for() 。这个函数的目的是计算一个张量是否已经从它在计算图中的所有孩子哪里接收了梯度。

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 
                 autograd=False,
                 
                 creators=None,
                 creation_op=None,
                 
                 id=None
                        ):
        
        self.data = np.array(data)
        self.creators = creators
        self.creation_op = creation_op
        
        
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.children = {}   #这是一个计数器，用来计算在反向传播过程中从每个子节点中接收到的梯度个数
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):#追踪一个张量有多少个子节点
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):#检查一个张量是否从每个子节点接受了正确数量的梯度
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True        
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
            if(grad is None):
                grad = FloatTensor(np.ones_like(self.data))
            
            if(grad_origin is not None):#检查确保你可以向后传播还是在等待一个梯度；在后一种情况下，减少计数器
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):#累加来自于若干个子节点的梯度
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
a = Tensor([1,2,3,4,5], autograd=True)
b = Tensor([2,2,2,2,2], autograd=True)
c = Tensor([5,4,3,2,1], autograd=True)

d = a + b
e = b + c
f = d + e

f.backward(Tensor(np.array([1,1,1,1,1])))

print(b.grad.data == np.array([2,2,2,2,2]))

#结果
[ True  True  True  True  True]
```

##  四、加法反向传播是如何工作的

1.Part_5_Add_Support_for_Negation_05.ipynb   -------------------------------- 增加取负值操作的支持

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True        
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
            if(grad is None):
                grad = FloatTensor(np.ones_like(self.data))
            
            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                #取负值操作反向传播逻辑
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    #增加取负值操作的支持
    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1) 
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
a = Tensor([1,2,3,4,5], autograd=True)
b = Tensor([2,2,2,2,2], autograd=True)
c = Tensor([5,4,3,2,1], autograd=True)

d = a + (-b)
e = (-b) + c
f = d + e

f.backward(Tensor(np.array([1,1,1,1,1])))

print(b.grad.data == np.array([-2,-2,-2,-2,-2]))

#结果
[ True  True  True  True  True]
```

2.Part_6_Add_Support_for_Additional_Functions_06.ipynb--------------------添加更多函数的支持

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    #反向传播逻辑
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
    
    #加法
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    #取负值
    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    #减法
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    #乘法
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    #求和
    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    #扩充
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    #转置
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    #矩阵乘法
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
a = Tensor([1,2,3,4,5], autograd=True)
b = Tensor([2,2,2,2,2], autograd=True)
c = Tensor([5,4,3,2,1], autograd=True)

d = a + b
e = b + c
f = d + e

f.backward(Tensor(np.array([1,1,1,1,1])))

print(b.grad.data == np.array([2,2,2,2,2]))

#结果：
[ True  True  True  True  True]



#A few Notes on Sum and Expand
x = Tensor(np.array([[1,2,3],
                     [4,5,6]]))


x.sum(0)
#结果  array([5, 7, 9])

x.sum(1)
#结果   array([ 6, 15])

x.expand(dim=2, copies=4)
#结果
 array([[[1, 1, 1, 1],
        [2, 2, 2, 2],
        [3, 3, 3, 3]],

       [[4, 4, 4, 4],
        [5, 5, 5, 5],
        [6, 6, 6, 6]]])
```

3.Part_7_Use_Autograd_to_Train_a_Neural_Network_07.ipynb-----------使用autograd训练神经网络

```python
#部分一：
print("Previously we would train a model like this:") 
print()
import numpy as np
np.random.seed(0)

data = np.array([[0,0],[0,1],[1,0],[1,1]])
target = np.array([[0],[1],[0],[1]])

weights_0_1 = np.random.rand(2,3)
weights_1_2 = np.random.rand(3,1)

for i in range(10):
    
    # Predict
    layer_1 = data.dot(weights_0_1)
    layer_2 = layer_1.dot(weights_1_2)
    
    # Compare
    diff = (layer_2 - target)
    sqdiff = (diff * diff)
    loss = sqdiff.sum(0) # mean squared error loss

    # Learn: this is the backpropagation piece
    layer_1_grad = diff.dot(weights_1_2.transpose())
    weight_1_2_update = layer_1.transpose().dot(diff)
    weight_0_1_update = data.transpose().dot(layer_1_grad)
    
    weights_1_2 -= weight_1_2_update * 0.1
    weights_0_1 -= weight_0_1_update * 0.1
    print(loss[0])
    
    
#结果：
Previously we would train a model like this:

5.066439994622395
0.4959907791902342
0.4180671892167177
0.35298133007809646
0.2972549636567377
0.2492326038163328
0.20785392075862477
0.17231260916265176
0.14193744536652986
0.11613979792168384

#部分二：使用autograd训练神经网络
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    #反向传播逻辑
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
    
    #加法
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    #取负值
    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    #减法
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    #乘法
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    #求和
    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    #扩充
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    #转置
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    #矩阵乘法
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
import numpy as np
np.random.seed(0)

data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)
target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)

w = list()
w.append(Tensor(np.random.rand(2,3), autograd=True))
w.append(Tensor(np.random.rand(3,1), autograd=True))

for i in range(10):

    # Predict
    pred = data.mm(w[0]).mm(w[1])
    
    # Compare
    loss = ((pred - target)*(pred - target)).sum(0)
    
    # Learn
    loss.backward(Tensor(np.ones_like(loss.data)))

    for w_ in w:
        w_.data -= w_.grad.data * 0.1
        w_.grad.data *= 0

    print(loss)
    
#结果：
[0.58128304]
[0.48988149]
[0.41375111]
[0.34489412]
[0.28210124]
[0.2254484]
[0.17538853]
[0.1324231]
[0.09682769]
[0.06849361]
```

4.Part_8_Adding_Automatic_Optimization_08.ipynb   (增加自动优化器，对前一个神经网络进一步简化，但运行结果一样)

```python
#创建一个随机梯度下降（SGD）优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0


                
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    #反向传播逻辑
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
    
    #加法
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    #取负值
    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    #减法
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    #乘法
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    #求和
    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    #扩充
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    #转置
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    #矩阵乘法
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
                
import numpy as np
np.random.seed(0)

data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)
target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)

w = list()
w.append(Tensor(np.random.rand(2,3), autograd=True))
w.append(Tensor(np.random.rand(3,1), autograd=True))

optim = SGD(parameters=w, alpha=0.1)

for i in range(10):

    # Predict
    pred = data.mm(w[0]).mm(w[1])
    
    # Compare
    loss = ((pred - target)*(pred - target)).sum(0)
    
    # Learn
    loss.backward(Tensor(np.ones_like(loss.data)))
    optim.step()

    print(loss)

#结果
[0.58128304]
[0.48988149]
[0.41375111]
[0.34489412]
[0.28210124]
[0.2254484]
[0.17538853]
[0.1324231]
[0.09682769]
[0.06849361]
```

5.Part_9_Adding_Support_for_Layer_Types_09.ipynb-------添加神经元层类型的支持

```python
class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters

    
#添加线性层
class Linear(Layer):

    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        self.parameters.append(self.bias)

    def forward(self, input):
        return input.mm(self.weight)+self.bias.expand(0,len(input.data))


#神经元层包含其它层
class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params
    
#优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0


                
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    #反向传播逻辑
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
    
    #加法
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    #取负值
    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    #减法
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    #乘法
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    #求和
    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    #扩充
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    #转置
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    #矩阵乘法
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
import numpy
np.random.seed(0)

data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)
target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)

model = Sequential([Linear(2,3), Linear(3,1)])

optim = SGD(parameters=model.get_parameters(), alpha=0.05)

for i in range(10):
    
    # Predict
    pred = model.forward(data)
    
    # Compare
    loss = ((pred - target)*(pred - target)).sum(0)
    
    # Learn
    loss.backward(Tensor(np.ones_like(loss.data)))
    optim.step()
    print(loss)
    
#结果
[2.33428272]
[0.06743796]
[0.0521849]
[0.04079507]
[0.03184365]
[0.02479336]
[0.01925443]
[0.01491699]
[0.01153118]
[0.00889602]
```

6.Part_11_Loss_Function_Layers_10.ipynb   -----------添加损失函数层

```python
class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters
    
#损失函数层
class MSELoss(Layer):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, pred, target):
        return ((pred - target)*(pred - target)).sum(0)
    

    
#添加线性层
class Linear(Layer):

    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        self.parameters.append(self.bias)

    def forward(self, input):
        return input.mm(self.weight)+self.bias.expand(0,len(input.data))


#神经元层包含其它层
class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params
    
#优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0


                
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    #反向传播逻辑
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
    
    #加法
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    #取负值
    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    #减法
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    #乘法
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    #求和
    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    #扩充
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    #转置
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    #矩阵乘法
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
import numpy
np.random.seed(0)

data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)
target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)

model = Sequential([Linear(2,3), Linear(3,1)])
criterion = MSELoss()

optim = SGD(parameters=model.get_parameters(), alpha=0.05)

for i in range(10):
    
    # Predict
    pred = model.forward(data)
    
    # Compare
    loss = criterion.forward(pred, target)
    
    # Learn
    loss.backward(Tensor(np.ones_like(loss.data)))
    optim.step()
    print(loss)
    
#结果
[2.33428272]
[0.06743796]
[0.0521849]
[0.04079507]
[0.03184365]
[0.02479336]
[0.01925443]
[0.01491699]
[0.01153118]
[0.00889602]
```

##  五、如何学习框架

1.框架就是autograd加上一组预先定义好的神经元层和优化器

2.大型框架的神经元层和优化器列表：

- https://pytorch.org/docs/stable/nn.html
- https://keras.io/layers/about-keras-layers
- https://www.tensorflow.org/api_docs/python/tf/layers

3.学习新框架的一般流程是先找到尽可能简单的样例代码，微调代码并了解其中autograd系统的API，然后一段一段地修改样例代码直到写出你关心的实验。

##  六、非线性层

1.Part_12_Non-linearity_Layers_11.ipynb------------添加非线性层Tanh 和 Sigmoid

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
        
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters
    
#损失函数层
class MSELoss(Layer):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, pred, target):
        return ((pred - target)*(pred - target)).sum(0)
    

    
#添加线性层
class Linear(Layer):

    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        self.parameters.append(self.bias)

    def forward(self, input):
        return input.mm(self.weight)+self.bias.expand(0,len(input.data))


#神经元层包含其它层
class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params
    
#优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0

                
#添加非线性层Tanh 和 Sigmoid
class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()
    
class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()



import numpy
np.random.seed(0)

data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)
target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)

model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()])
criterion = MSELoss()

optim = SGD(parameters=model.get_parameters(), alpha=1)

for i in range(10):
    
    # Predict
    pred = model.forward(data)
    
    # Compare
    loss = criterion.forward(pred, target)
    
    # Learn
    loss.backward(Tensor(np.ones_like(loss.data)))
    optim.step()
    print(loss)
    
    
#结果
[1.06372865]
[0.75148144]
[0.57384259]
[0.39574294]
[0.2482279]
[0.15515294]
[0.10423398]
[0.07571169]
[0.05837623]
[0.04700013]
```

##  七、嵌入层

1.Part_14_Add_Indexing_to_Autograd_12.ipynb------------------------让autograd支持下标操作

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                    
                if(self.creation_op == "index_select"):
                    new_grad = np.zeros_like(self.creators[0].data)
                    indices_ = self.index_select_indices.data.flatten()
                    grad_ = grad.data.reshape(len(indices_), -1)
                    for i in range(len(indices_)):
                        new_grad[indices_[i]] += grad_[i]
                    self.creators[0].backward(Tensor(new_grad))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
        
    #让autograd支持下标操作
    def index_select(self, indices):

        if(self.autograd):
            new = Tensor(self.data[indices.data],
                         autograd=True,
                         creators=[self],
                         creation_op="index_select")
            new.index_select_indices = indices
            return new
        return Tensor(self.data[indices.data])
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters
    
#损失函数层
class MSELoss(Layer):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, pred, target):
        return ((pred - target)*(pred - target)).sum(0)
    

    
#添加线性层
class Linear(Layer):

    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        self.parameters.append(self.bias)

    def forward(self, input):
        return input.mm(self.weight)+self.bias.expand(0,len(input.data))


#神经元层包含其它层
class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params
    
#优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0

                
#添加非线性层Tanh 和 Sigmoid
class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()
    
class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()

x = Tensor(np.eye(5), autograd=True)
x.index_select(Tensor([[1,2,3],[2,3,4]])).backward()
print(x.grad)

#结果
[[0. 0. 0. 0. 0.]
 [1. 1. 1. 1. 1.]
 [2. 2. 2. 2. 2.]
 [2. 2. 2. 2. 2.]
 [1. 1. 1. 1. 1.]]
```

2.Part_15_The_Embedding_Layer_(revisited)_13.ipynb   -----增加嵌入层，然后使用  .index_select()方法完成向前传播。

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                    
                if(self.creation_op == "index_select"):
                    new_grad = np.zeros_like(self.creators[0].data)
                    indices_ = self.index_select_indices.data.flatten()
                    grad_ = grad.data.reshape(len(indices_), -1)
                    for i in range(len(indices_)):
                        new_grad[indices_[i]] += grad_[i]
                    self.creators[0].backward(Tensor(new_grad))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
        
    #让autograd支持下标操作
    def index_select(self, indices):

        if(self.autograd):
            new = Tensor(self.data[indices.data],
                         autograd=True,
                         creators=[self],
                         creation_op="index_select")
            new.index_select_indices = indices
            return new
        return Tensor(self.data[indices.data])
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters
    
#损失函数层
class MSELoss(Layer):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, pred, target):
        return ((pred - target)*(pred - target)).sum(0)
    

    
#添加线性层
class Linear(Layer):

    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        self.parameters.append(self.bias)

    def forward(self, input):
        return input.mm(self.weight)+self.bias.expand(0,len(input.data))


#神经元层包含其它层
class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params
    
#优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0

                
#添加非线性层Tanh 和 Sigmoid
class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()
    
class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()
    
#嵌入层
class Embedding(Layer):
    
    def __init__(self, vocab_size, dim):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        
        # this random initialiation style is just a convention from word2vec
        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)
        
        self.parameters.append(self.weight)
    
    def forward(self, input):
        return self.weight.index_select(input)
    
    

    
import numpy
np.random.seed(0)

data = Tensor(np.array([1,2,1,2]), autograd=True)
target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)

embed = Embedding(5,3)
model = Sequential([embed, Tanh(), Linear(3,1), Sigmoid()])
criterion = MSELoss()

optim = SGD(parameters=model.get_parameters(), alpha=0.5)

for i in range(10):
    
    # Predict
    pred = model.forward(data)
    
    # Compare
    loss = criterion.forward(pred, target)
    
    # Learn
    loss.backward(Tensor(np.ones_like(loss.data)))
    optim.step()
    print(loss)
    
#结果：
[0.98874126]
[0.6658868]
[0.45639889]
[0.31608168]
[0.2260925]
[0.16877423]
[0.13120515]
[0.10555487]
[0.08731868]
[0.07387834]
```

##  八、交叉熵层

1.Part_16_The_Cross_Entropy_Layer_14.ipynb--------------------添加交叉熵层

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                    
                if(self.creation_op == "index_select"):
                    new_grad = np.zeros_like(self.creators[0].data)
                    indices_ = self.index_select_indices.data.flatten()
                    grad_ = grad.data.reshape(len(indices_), -1)
                    for i in range(len(indices_)):
                        new_grad[indices_[i]] += grad_[i]
                    self.creators[0].backward(Tensor(new_grad))
                    
                if(self.creation_op == "cross_entropy"):
                    dx = self.softmax_output - self.target_dist
                    self.creators[0].backward(Tensor(dx))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
        
    #让autograd支持下标操作
    def index_select(self, indices):

        if(self.autograd):
            new = Tensor(self.data[indices.data],
                         autograd=True,
                         creators=[self],
                         creation_op="index_select")
            new.index_select_indices = indices
            return new
        return Tensor(self.data[indices.data])
    
    #交叉熵
    def cross_entropy(self, target_indices):

        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        
        t = target_indices.data.flatten()
        p = softmax_output.reshape(len(t),-1)
        target_dist = np.eye(p.shape[1])[t]
        loss = -(np.log(p) * (target_dist)).sum(1).mean()
    
        if(self.autograd):
            out = Tensor(loss,
                         autograd=True,
                         creators=[self],
                         creation_op="cross_entropy")
            out.softmax_output = softmax_output
            out.target_dist = target_dist
            return out

        return Tensor(loss)
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters
    
#损失函数层
class MSELoss(Layer):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, pred, target):
        return ((pred - target)*(pred - target)).sum(0)
    

    
#添加线性层
class Linear(Layer):

    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        self.parameters.append(self.bias)

    def forward(self, input):
        return input.mm(self.weight)+self.bias.expand(0,len(input.data))


#神经元层包含其它层
class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params
    
#优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0

                
#添加非线性层Tanh 和 Sigmoid
class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()
    
class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()
    
#嵌入层
class Embedding(Layer):
    
    def __init__(self, vocab_size, dim):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        
        # this random initialiation style is just a convention from word2vec
        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)
        
        self.parameters.append(self.weight)
    
    def forward(self, input):
        return self.weight.index_select(input)
    
#交叉熵损失函数
class CrossEntropyLoss(object):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, input, target):
        return input.cross_entropy(target)
    
    
import numpy
np.random.seed(0)

# data indices
data = Tensor(np.array([1,2,1,2]), autograd=True)

# target indices
target = Tensor(np.array([0,1,0,1]), autograd=True)

model = Sequential([Embedding(3,3), Tanh(), Linear(3,4)])
criterion = CrossEntropyLoss()

optim = SGD(parameters=model.get_parameters(), alpha=0.1)

for i in range(10):
    
    # Predict
    pred = model.forward(data)
    
    # Compare
    loss = criterion.forward(pred, target)
    
    # Learn
    loss.backward(Tensor(np.ones_like(loss.data)))
    optim.step()
    print(loss)
   
#结果：
1.3885032434928422
0.9558181509266036
0.6823083585795604
0.509525996749312
0.39574491472895856
0.31752527285348264
0.2617222861964216
0.22061283923954225
0.18946427334830068
0.16527389263866668
```

##  九、递归层

1.Part_17_The_Recurrent_Neural_Network_Layer_15.ipynb --------------添加递归层  RNNCell

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None
        if(id is None):
            self.id = np.random.randint(0,100000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                    
                if(self.creation_op == "index_select"):
                    new_grad = np.zeros_like(self.creators[0].data)
                    indices_ = self.index_select_indices.data.flatten()
                    grad_ = grad.data.reshape(len(indices_), -1)
                    for i in range(len(indices_)):
                        new_grad[indices_[i]] += grad_[i]
                    self.creators[0].backward(Tensor(new_grad))
                    
                if(self.creation_op == "cross_entropy"):
                    dx = self.softmax_output - self.target_dist
                    self.creators[0].backward(Tensor(dx))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
        
    #让autograd支持下标操作
    def index_select(self, indices):

        if(self.autograd):
            new = Tensor(self.data[indices.data],
                         autograd=True,
                         creators=[self],
                         creation_op="index_select")
            new.index_select_indices = indices
            return new
        return Tensor(self.data[indices.data])
    
    #交叉熵
    def cross_entropy(self, target_indices):

        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        
        t = target_indices.data.flatten()
        p = softmax_output.reshape(len(t),-1)
        target_dist = np.eye(p.shape[1])[t]
        loss = -(np.log(p) * (target_dist)).sum(1).mean()
    
        if(self.autograd):
            out = Tensor(loss,
                         autograd=True,
                         creators=[self],
                         creation_op="cross_entropy")
            out.softmax_output = softmax_output
            out.target_dist = target_dist
            return out

        return Tensor(loss)
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  
    
class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters
    
#损失函数层
class MSELoss(Layer):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, pred, target):
        return ((pred - target)*(pred - target)).sum(0)
    

    
#添加线性层
class Linear(Layer):

    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        self.parameters.append(self.bias)

    def forward(self, input):
        return input.mm(self.weight)+self.bias.expand(0,len(input.data))


#神经元层包含其它层
class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params
    
#优化器
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0

                
#添加非线性层Tanh 和 Sigmoid
class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()
    
class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()
    
#嵌入层
class Embedding(Layer):
    
    def __init__(self, vocab_size, dim):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        
        # this random initialiation style is just a convention from word2vec
        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)
        
        self.parameters.append(self.weight)
    
    def forward(self, input):
        return self.weight.index_select(input)
    
#交叉熵损失函数
class CrossEntropyLoss(object):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, input, target):
        return input.cross_entropy(target)
    
#添加递归层
class RNNCell(Layer):
    
    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):
        super().__init__()

        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.n_output = n_output
        
        if(activation == 'sigmoid'):
            self.activation = Sigmoid()
        elif(activation == 'tanh'):
            self.activation == Tanh()
        else:
            raise Exception("Non-linearity not found")

        self.w_ih = Linear(n_inputs, n_hidden)#输入-隐藏层
        self.w_hh = Linear(n_hidden, n_hidden)#隐藏-隐藏层
        self.w_ho = Linear(n_hidden, n_output)#隐藏-输出层
        
        self.parameters += self.w_ih.get_parameters()
        self.parameters += self.w_hh.get_parameters()
        self.parameters += self.w_ho.get_parameters()        
    
    def forward(self, input, hidden):
        from_prev_hidden = self.w_hh.forward(hidden)
        combined = self.w_ih.forward(input) + from_prev_hidden
        new_hidden = self.activation.forward(combined)
        output = self.w_ho.forward(new_hidden)
        return output, new_hidden
    
    def init_hidden(self, batch_size=1):
        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
  

import sys,random,math
from collections import Counter
import numpy as np

f = open('F:\\DL\\Tasksv11\\tasksv11\\en\\qa1_single-supporting-fact_train.txt','r')
raw = f.readlines()
f.close()

tokens = list()
for line in raw[0:1000]:
    tokens.append(line.lower().replace("\n","").split(" ")[1:])

new_tokens = list()
for line in tokens:
    new_tokens.append(['-'] * (6 - len(line)) + line)

tokens = new_tokens

vocab = set()
for sent in tokens:
    for word in sent:
        vocab.add(word)

vocab = list(vocab)

word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i
    
def words2indices(sentence):
    idx = list()
    for word in sentence:
        idx.append(word2index[word])
    return idx

indices = list()
for line in tokens:
    idx = list()
    for w in line:
        idx.append(word2index[w])
    indices.append(idx)

data = np.array(indices)

embed = Embedding(vocab_size=len(vocab),dim=16)
model = RNNCell(n_inputs=16, n_hidden=16, n_output=len(vocab))

criterion = CrossEntropyLoss()
optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)

for iter in range(1000):
    batch_size = 100
    total_loss = 0
    
    hidden = model.init_hidden(batch_size=batch_size)

    for t in range(5):
        input = Tensor(data[0:batch_size,t], autograd=True)
        rnn_input = embed.forward(input=input)
        output, hidden = model.forward(input=rnn_input, hidden=hidden)

    target = Tensor(data[0:batch_size,t+1], autograd=True)    
    loss = criterion.forward(output, target)
    loss.backward()
    optim.step()
    total_loss += loss.data
    if(iter % 200 == 0):
        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()
        print("Loss:",total_loss / (len(data)/batch_size),"% Correct:",p_correct)
        
#结果（神经网络学习了大约以37&的准确率预测训练了数据的前100个例子）
Loss: 0.4639582726559787 % Correct: 0.0
Loss: 0.178332205917977 % Correct: 0.19
Loss: 0.15447594842656479 % Correct: 0.34
Loss: 0.14365522303768166 % Correct: 0.36
Loss: 0.13507286391521264 % Correct: 0.37
        
        
batch_size = 1
hidden = model.init_hidden(batch_size=batch_size)
for t in range(5):
    input = Tensor(data[0:batch_size,t], autograd=True)
    rnn_input = embed.forward(input=input)
    output, hidden = model.forward(input=rnn_input, hidden=hidden)

target = Tensor(data[0:batch_size,t+1], autograd=True)    
loss = criterion.forward(output, target)

ctx = ""
for idx in data[0:batch_size][0][0:-1]:
    ctx += vocab[idx] + " "
print("Context:",ctx)
print("True:",vocab[target.data[0]])
print("Pred:", vocab[output.data.argmax()])


#结果（预测marry可能要去的位置）
Context: - mary moved to the 
True: bathroom.
Pred: garden.
```

2.RNN有若干不同的权重矩阵：一个把输入向量映射到隐藏向量（处理输入数据）；一个把隐藏向量映射到隐藏向量（即根据前一个隐藏向量更新当前的）；以及包含可能会有一个隐藏-输出层基于隐藏向量做出预测。

3.元胞是实现单层递归的递归层的传统名称。如果创建了另外一种可以配置任意数量元胞的神经元层，它应该叫做RNN，而n_layers会作为参数。

4.框架是前向与反向传播逻辑的高效方便的抽象。

#  第十四章：像莎士比亚一样写作：长短期记忆网络

##  一、字符语言模型

1.普通的RNN模型实现字符语言模型

```python
import sys,random,math
from collections import Counter
import numpy as np
import sys

np.random.seed(0)

f=open('F:\DL\Shakespear\shakespear.txt','r')
raw=f.read()
f.close()

vocab=list(set(raw))
word2index={}
for i,word in enumerate(vocab):
    word2index[word]=i
indices=np.array(list(map(lambda x:word2index[x],raw)))

embed=Embedding(vocab_size=len(vocab),dim=512)
model=RNNCell(n_inputs=512,n_hidden=512,n_output=len(vocab))

criterion=CrossEntropyLoss()
optin=SGD(parameters=model.get_parameters()+embed.get_parameters(),alpha=0.05)
```

##  二、截断式反向传播

1.截断式反向传播（truncated backpropagation）,它减弱了神经网络的理论能力上限。

2.截断变量称为bptt,通常设置在16~64之间。

3.Part_2_Learn_to_Writ_Like_Shakespeare_02.ipynb

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None

        if(id is None):
            self.id = np.random.randint(0,1000000000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    return
                    print(self.id)
                    print(self.creation_op)
                    print(len(self.creators))
                    for c in self.creators:
                        print(c.creation_op)
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                
                if(self.creation_op == "index_select"):
                    new_grad = np.zeros_like(self.creators[0].data)
                    indices_ = self.index_select_indices.data.flatten()
                    grad_ = grad.data.reshape(len(indices_), -1)
                    for i in range(len(indices_)):
                        new_grad[indices_[i]] += grad_[i]
                    self.creators[0].backward(Tensor(new_grad))
                    
                if(self.creation_op == "cross_entropy"):
                    dx = self.softmax_output - self.target_dist
                    self.creators[0].backward(Tensor(dx))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
    
    def index_select(self, indices):

        if(self.autograd):
            new = Tensor(self.data[indices.data],
                         autograd=True,
                         creators=[self],
                         creation_op="index_select")
            new.index_select_indices = indices
            return new
        return Tensor(self.data[indices.data])
    
    def softmax(self):
        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        return softmax_output
    
    def cross_entropy(self, target_indices):

        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        
        t = target_indices.data.flatten()
        p = softmax_output.reshape(len(t),-1)
        target_dist = np.eye(p.shape[1])[t]
        loss = -(np.log(p) * (target_dist)).sum(1).mean()
    
        if(self.autograd):
            out = Tensor(loss,
                         autograd=True,
                         creators=[self],
                         creation_op="cross_entropy")
            out.softmax_output = softmax_output
            out.target_dist = target_dist
            return out

        return Tensor(loss)
        
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  

class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters

    
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0


class Linear(Layer):

    def __init__(self, n_inputs, n_outputs, bias=True):
        super().__init__()
        
        self.use_bias = bias
        
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        if(self.use_bias):
            self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        
        if(self.use_bias):        
            self.parameters.append(self.bias)

    def forward(self, input):
        if(self.use_bias):
            return input.mm(self.weight)+self.bias.expand(0,len(input.data))
        return input.mm(self.weight)


class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params


class Embedding(Layer):
    
    def __init__(self, vocab_size, dim):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        
        # this random initialiation style is just a convention from word2vec
        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)
        
        self.parameters.append(self.weight)
    
    def forward(self, input):
        return self.weight.index_select(input)


class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()


class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()
    

class CrossEntropyLoss(object):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, input, target):
        return input.cross_entropy(target)

    
class RNNCell(Layer):
    
    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):
        super().__init__()

        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.n_output = n_output
        
        if(activation == 'sigmoid'):
            self.activation = Sigmoid()
        elif(activation == 'tanh'):
            self.activation == Tanh()
        else:
            raise Exception("Non-linearity not found")

        self.w_ih = Linear(n_inputs, n_hidden)
        self.w_hh = Linear(n_hidden, n_hidden)
        self.w_ho = Linear(n_hidden, n_output)
        
        self.parameters += self.w_ih.get_parameters()
        self.parameters += self.w_hh.get_parameters()
        self.parameters += self.w_ho.get_parameters()        
    
    def forward(self, input, hidden):
        from_prev_hidden = self.w_hh.forward(hidden)
        combined = self.w_ih.forward(input) + from_prev_hidden
        new_hidden = self.activation.forward(combined)
        output = self.w_ho.forward(new_hidden)
        return output, new_hidden
    
    def init_hidden(self, batch_size=1):
        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
    
class LSTMCell(Layer):
    
    def __init__(self, n_inputs, n_hidden, n_output):
        super().__init__()

        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.n_output = n_output

        self.xf = Linear(n_inputs, n_hidden)
        self.xi = Linear(n_inputs, n_hidden)
        self.xo = Linear(n_inputs, n_hidden)        
        self.xc = Linear(n_inputs, n_hidden)        
        
        self.hf = Linear(n_hidden, n_hidden, bias=False)
        self.hi = Linear(n_hidden, n_hidden, bias=False)
        self.ho = Linear(n_hidden, n_hidden, bias=False)
        self.hc = Linear(n_hidden, n_hidden, bias=False)        
        
        self.w_ho = Linear(n_hidden, n_output, bias=False)
        
        self.parameters += self.xf.get_parameters()
        self.parameters += self.xi.get_parameters()
        self.parameters += self.xo.get_parameters()
        self.parameters += self.xc.get_parameters()

        self.parameters += self.hf.get_parameters()
        self.parameters += self.hi.get_parameters()        
        self.parameters += self.ho.get_parameters()        
        self.parameters += self.hc.get_parameters()                
        
        self.parameters += self.w_ho.get_parameters()        
    
    def forward(self, input, hidden):
        
        prev_hidden = hidden[0]        
        prev_cell = hidden[1]
        
        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()
        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()
        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        
        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        
        c = (f * prev_cell) + (i * g)

        h = o * c.tanh()
        
        output = self.w_ho.forward(h)
        return output, (h, c)
    
    def init_hidden(self, batch_size=1):
        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
        init_hidden.data[:,0] += 1
        init_cell.data[:,0] += 1
        return (init_hidden, init_cell)

    def generate_sample(n=30, init_char=' '):
        s = ""
        hidden = model.init_hidden(batch_size=1)
        input = Tensor(np.array([word2index[init_char]]))
        for i in range(n):
            rnn_input = embed.forward(input)
            output, hidden = model.forward(input=rnn_input, hidden=hidden)
            output.data *= 10
            temp_dist = output.softmax()
            temp_dist /= temp_dist.sum()

            m = (temp_dist > np.random.rand()).argmax()
    #         m = output.data.argmax()
            c = vocab[m]
            input = Tensor(np.array([m]))
            s += c
        return s
    print(generate_sample(n=2000, init_char='\n'))
 
 
import sys,random,math
from collections import Counter
import numpy as np
import sys

np.random.seed(0)
# dataset from http://karpathy.github.io/2015/05/21/rnn-effectiveness/
f = open('F:\DL\Shakespear\shakespear.txt','r')
raw = f.read()
f.close()

vocab = list(set(raw))
word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i
indices = np.array(list(map(lambda x:word2index[x], raw)))

embed = Embedding(vocab_size=len(vocab),dim=512)
model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))

criterion = CrossEntropyLoss()
optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)

batch_size = 32
bptt = 16
n_batches = int((indices.shape[0] / (batch_size)))

trimmed_indices = indices[:n_batches*batch_size]
batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()

input_batched_indices = batched_indices[0:-1]
target_batched_indices = batched_indices[1:]

n_bptt = int(((n_batches-1) / bptt))
input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)
target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)


raw[0:5]
#结果（莎士比亚数据集中的前五个字符）
'That,'


indices[0:5]
#结果（前五个字符的索引）
array([56, 61, 49, 58, 16])

batched_indices[0:5]
#结果
array([[56, 13,  8, 46, 24, 61, 56, 20, 20, 49, 49, 28, 24, 20, 16, 31,
        20,  9, 61,  8, 28, 49, 20, 20, 20, 33, 20, 33, 33, 20, 20, 42],
       [61, 31, 31, 46, 19, 28, 49, 55,  9, 42, 53, 28, 53, 58, 20, 31,
         3, 24, 24, 31, 53,  2,  3, 48, 61, 42, 57, 45, 42, 45, 26, 20],
       [49, 31, 18, 28, 28, 49, 53, 30, 24, 20, 53,  8, 48, 61, 51,  4,
        28, 55, 53, 31, 48, 28, 11, 28, 28, 50, 24, 16, 20, 24, 24,  3],
       [58, 59, 56, 11, 20, 11,  3, 20, 55, 42, 20, 20, 13, 28, 28, 14,
        28, 16,  9, 60, 30, 20, 24, 28, 11, 53, 55, 31, 25, 11, 45, 24],
       [16, 14, 33, 18, 51, 20, 24, 11, 20, 24, 51, 41, 31, 20, 20, 44,
        42, 20, 20, 24, 13,  9, 51, 19, 16, 49, 53, 14, 55, 28, 28,  9]])

input_batches[0][0:5]
#结果
array([[56, 13,  8, 46, 24, 61, 56, 20, 20, 49, 49, 28, 24, 20, 16, 31,
        20,  9, 61,  8, 28, 49, 20, 20, 20, 33, 20, 33, 33, 20, 20, 42],
       [61, 31, 31, 46, 19, 28, 49, 55,  9, 42, 53, 28, 53, 58, 20, 31,
         3, 24, 24, 31, 53,  2,  3, 48, 61, 42, 57, 45, 42, 45, 26, 20],
       [49, 31, 18, 28, 28, 49, 53, 30, 24, 20, 53,  8, 48, 61, 51,  4,
        28, 55, 53, 31, 48, 28, 11, 28, 28, 50, 24, 16, 20, 24, 24,  3],
       [58, 59, 56, 11, 20, 11,  3, 20, 55, 42, 20, 20, 13, 28, 28, 14,
        28, 16,  9, 60, 30, 20, 24, 28, 11, 53, 55, 31, 25, 11, 45, 24],
       [16, 14, 33, 18, 51, 20, 24, 11, 20, 24, 51, 41, 31, 20, 20, 44,
        42, 20, 20, 24, 13,  9, 51, 19, 16, 49, 53, 14, 55, 28, 28,  9]])


target_batches[0][0:5]
#结果
array([[61, 31, 31, 46, 19, 28, 49, 55,  9, 42, 53, 28, 53, 58, 20, 31,
         3, 24, 24, 31, 53,  2,  3, 48, 61, 42, 57, 45, 42, 45, 26, 20],
       [49, 31, 18, 28, 28, 49, 53, 30, 24, 20, 53,  8, 48, 61, 51,  4,
        28, 55, 53, 31, 48, 28, 11, 28, 28, 50, 24, 16, 20, 24, 24,  3],
       [58, 59, 56, 11, 20, 11,  3, 20, 55, 42, 20, 20, 13, 28, 28, 14,
        28, 16,  9, 60, 30, 20, 24, 28, 11, 53, 55, 31, 25, 11, 45, 24],
       [16, 14, 33, 18, 51, 20, 24, 11, 20, 24, 51, 41, 31, 20, 20, 44,
        42, 20, 20, 24, 13,  9, 51, 19, 16, 49, 53, 14, 55, 28, 28,  9],
       [20, 23, 30, 48, 33, 28, 58, 28, 51, 20, 28, 61, 31, 48, 51, 29,
        20, 45,  3, 30, 31, 24, 20, 20, 20, 51, 48, 42, 46, 20, 16, 30]])





#截断式反向传播做迭代（每一步都生成一个batch_loss；然后每bptt步，都执行反向传播和更新权重）
def train(iterations=400):
    for iter in range(iterations):
        total_loss = 0
        n_loss = 0

        hidden = model.init_hidden(batch_size=batch_size)
        for batch_i in range(len(input_batches)):

            hidden = Tensor(hidden.data, autograd=True)
            loss = None
            losses = list()
            for t in range(bptt):
                input = Tensor(input_batches[batch_i][t], autograd=True)
                rnn_input = embed.forward(input=input)
                output, hidden = model.forward(input=rnn_input, hidden=hidden)

                target = Tensor(target_batches[batch_i][t], autograd=True)    
                batch_loss = criterion.forward(output, target)
                losses.append(batch_loss)
                if(t == 0):
                    loss = batch_loss
                else:
                    loss = loss + batch_loss
            for loss in losses:
                ""
            loss.backward()
            optim.step()
            total_loss += loss.data
            log = "\r Iter:" + str(iter)
            log += " - Batch "+str(batch_i+1)+"/"+str(len(input_batches))
            log += " - Loss:" + str(np.exp(total_loss / (batch_i+1)))
            if(batch_i == 0):
                log += " - " + generate_sample(n=70, init_char='\n').replace("\n"," ")
            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):
                sys.stdout.write(log)
        optim.alpha *= 0.99
        print()
        
train(100)
#结果
 Iter:0 - Batch 191/195 - Loss:129.36583391134325tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt
 Iter:1 - Batch 191/195 - Loss:20.242771121077917 o  me th tht thet thxt thet thxt thet thxt thet thet thet thet thet th
 Iter:2 - Batch 191/195 - Loss:15.210465839751164 d  ar the the the the the the the the the the the the the the the the 
 Iter:3 - Batch 191/195 - Loss:13.103900099685596d  thert the the the the the the the the the the the the thext the the
 Iter:4 - Batch 191/195 - Loss:11.916391047248327dh thxt the the the the the the the the the the the the the the the th
 Iter:5 - Batch 191/195 - Loss:11.089942651313692dh thext the the the the the the the the the the the the the the the t
 Iter:6 - Batch 191/195 - Loss:10.463383227959811dh the the the the the the the the the the the the the the the the the
 Iter:7 - Batch 191/195 - Loss:9.9597294461510188dh the the the the the the the the the the the the the the the the the
 Iter:8 - Batch 191/195 - Loss:9.541578069969015th the the the the the the the the the the the the the the the the the
 Iter:9 - Batch 191/195 - Loss:9.178434390136504- th the the the the the the the thxt the the the the the the the the th
 Iter:10 - Batch 191/195 - Loss:8.853748084166597 th the the the the the the the the the the thxt the the the the the th
 Iter:11 - Batch 191/195 - Loss:8.559047758977764 th the the the the the the the the the the the the the the the the the
 Iter:12 - Batch 191/195 - Loss:8.286113393991068 th the the the the the the the the the thxxext the the the the the the
 Iter:13 - Batch 191/195 - Loss:8.031386849022624- th the the the the the the the the the the the the the the the the the
 Iter:14 - Batch 191/195 - Loss:7.7933446547040375th the the the the the the the the the the the the the the the the the
 Iter:15 - Batch 191/195 - Loss:7.569154126589835 HE the the the the the the the the the the the the the the the the the
 Iter:16 - Batch 191/195 - Loss:7.3569490889345995HE the the the the the the the the the the the the the the the the the
 Iter:17 - Batch 191/195 - Loss:7.1550670391544955E the the the the the the the the to the the the the the the the the 
 Iter:18 - Batch 191/195 - Loss:6.9604750413227525HE the the the the the the the the the the the the the the the the the
 Iter:19 - Batch 191/195 - Loss:6.7723870489560435HE Haxst the sto the the the the the the the the the the the xaxk the 
 Iter:20 - Batch 191/195 - Loss:6.5896830508896175HE Haxst the sto the xaxxxare the the the sto the the xand the the the
 Iter:21 - Batch 191/195 - Loss:6.4106000802013185HE HxESS HEES: Hay a xay the the the sto the the the sto the the the t
 Iter:22 - Batch 191/195 - Loss:6.2360575122729655HE HEES: Hay wxent the xand the xand the the sto the the stxest the th
 Iter:23 - Batch 191/195 - Loss:6.066320810857062 HE HEES: xood the xand the the xand the sto the the xaxxxare the the x
 Iter:24 - Batch 191/195 - Loss:5.9011858484743865HE HEES: Hath the xand the the the xand the xand the wast the xand the
 Iter:25 - Batch 191/195 - Loss:5.7403923294434615HE HEES: Hath the xand the xand the wast the wast the sing the xaxk wo
 Iter:26 - Batch 191/195 - Loss:5.5836907556460496 HEES: Hath the xaxpet the xairt the xand the xand the xairt the was
 Iter:27 - Batch 191/195 - Loss:5.4303540721088274HE HExHEESS HEESS: HaxChack the wast the wast the wast the xairt the w
 Iter:28 - Batch 191/195 - Loss:5.279886484461537 HE HEOPHE HExSecommy wxecommy would the xaxpet the sxxairt the wast th
 Iter:29 - Batch 191/195 - Loss:5.1315712635989925 HE HExHEESS HEESSxOPHESSIUS: Hathe xairts a pet the stod the wast the 
 Iter:30 - Batch 191/195 - Loss:4.9847006629441055HE HExxEOPHE HENOR STI HENOR: Hathe stxest the wast the xairthing the 
 Iter:31 - Batch 191/195 - Loss:4.839187149441926- Hxand the wast the wast the wast the wast the wast the wast the wast t
 Iter:32 - Batch 191/195 - Loss:4.6954854666392614Hy Hamy were the was the sirther the wastxSecond the wast the wast the
 Iter:33 - Batch 191/195 - Loss:4.5539844593793615 Hy Hamy wasterst the wasterst the wastxSI HENxI HENxI HENUCE: I sirthe
 Iter:34 - Batch 191/195 - Loss:4.4154257550342075Hy Hamy wxecommy wasterst the was the sxeath, a peaxthx and the waster
 Iter:35 - Batch 191/195 - Loss:4.2801388734312375 Hy Hay, I wo the pard the wasters, axpext so the was the sirthere the 
 Iter:36 - Batch 191/195 - Loss:4.1480819617630775Hy HaxChomm the wasxed the wasterst xacrost the wastersxing the waster
 Iter:37 - Batch 191/195 - Loss:4.0189942629974675Hy Hay, I wxecond woth the was the sirthere the was the stod the waste
 Iter:38 - Batch 191/195 - Loss:3.8918028620535696Hy at the stod the wasterst the was the stod the wasterst the was the 
 Iter:39 - Batch 191/195 - Loss:3.7674224792704765 Hy have the was the stod the xacrine the wastersxing the wasters, and 
 Iter:40 - Batch 191/195 - Loss:3.6456126471482913to the xaxxxpery prace the xaxthx': xommont the xace the now the was t
 Iter:41 - Batch 191/195 - Loss:3.5269685518780767 to thy allaing the was the stollains, these the nave the nave the nave
 Iter:42 - Batch 191/195 - Loss:3.4124449163886283 to the nave the nave the nave the nave the nave the nave the nave the 
 Iter:43 - Batch 191/195 - Loss:3.2979762353794784 to the nave the nave the nave the nave the nave strack woth the nave t
 Iter:44 - Batch 191/195 - Loss:3.1866560013619596 to the nave the nave strack where in the nave strack where in the nave
 Iter:45 - Batch 191/195 - Loss:3.0763035941915642 to the nave xacrine the was nave the nave xacrine the was nave the nav
 Iter:46 - Batch 191/195 - Loss:2.9686940306103202 to the nave strack where in the nave strack where in the nave xacrine 
 Iter:47 - Batch 191/195 - Loss:2.8641981497133935to the nave straxno fxom the name thex and be strathere the xace the n
 Iter:48 - Batch 191/195 - Loss:2.7572231097796545 to the naxbxpeak.  CHERecont won prepersx and be strathere the namm th
 Iter:49 - Batch 191/195 - Loss:2.6554282274294922 toxhim the xacring ow?  MALVAxIUS: So my he was naxthere in the name t
 Iter:50 - Batch 191/195 - Loss:2.5566557611526145toes, the namxerxxxOR HENRES: Gx and be straxno xexpear the name thex 
 Iter:51 - Batch 191/195 - Loss:2.4641648939632597toxhim the xacrak.  CHEReak: Yessers, axpear of the name, my xacring o
 Iter:52 - Batch 191/195 - Loss:2.3820140161671493toes, they a plemere the mand the a xing own are, and be string the na
 Iter:53 - Batch 191/195 - Loss:2.2968907299702826txeemn a speak: Yessers, and be string of the name, my worsxerd him, a
 Iter:54 - Batch 191/195 - Loss:2.2227710341816374the dish the name, my was namm the name, my wors of the name, my wxeet
 Iter:55 - Batch 191/195 - Loss:2.1763893383408446 toes, they a speak: Yessere him, a sparet the a xingm the name, my wor
 Iter:56 - Batch 191/195 - Loss:2.0809659214234864the xacrahe himserverxthe namxere name, my worse, my hears, and beggar
 Iter:57 - Batch 191/195 - Loss:1.9947685827491135 the dish the xacrost thou and be string ox they a speak: Yesser, himse
 Iter:58 - Batch 191/195 - Loss:1.9309457603995492 the by that hears, and beggarbser, here in a speak: Yessere him the a 
 Iter:59 - Batch 191/195 - Loss:1.8692858658410503 the by that here in a speak. xom to your pardxeemn with a show didamem
 Iter:60 - Batch 191/195 - Loss:1.8333279137071524the xacrost thou the nammer the sing or xuch three the name, my was na
 Iter:61 - Batch 191/195 - Loss:1.8098792580565746 the by that hears'd the xaidto't speak: Yesx speak: Yessere the xacros
 Iter:62 - Batch 191/195 - Loss:1.7442347486952934 the xack xack what me his xand an toundammer, sirts are, and my staxn 
 Iter:63 - Batch 191/195 - Loss:1.7006271645893503 ty xomand him the and beggarbs, thex and xoto prom this, shat hears, a
 Iter:64 - Batch 191/195 - Loss:1.6776327794630281I sing of thy his xand bones, and my soxthinke, my sirgett doth neaven
 Iter:65 - Batch 191/195 - Loss:1.7244395740181273 I sing own and my somet, himser, sind a sxacrine any axsell my sirged 
 Iter:66 - Batch 191/195 - Loss:1.8416187380626625 th this this staxne a spaixt a spaint a spairthing own a severy stake 
 Iter:67 - Batch 191/195 - Loss:1.6517962739099992 ty to samy something the tamm thising off a sparet my stakes name, my 
 Iter:68 - Batch 191/195 - Loss:1.5762257352495973I sing of thy himse, my ses, and sames, that me the sing of SexxOPHEST
 Iter:69 - Batch 191/195 - Loss:1.5290698522652375 I sixhains by hear and my so xxeat, sind have tas namin tollonge, and 
 Iter:70 - Batch 191/195 - Loss:1.4959614794283362 I sid, Sxack what this stake Were his sequmer they a mine an I shall h
 Iter:71 - Batch 191/195 - Loss:1.4341313405668366 I sim's seemn with a sever that mein tolk whicking of me xay is tith e
 Iter:72 - Batch 191/195 - Loss:1.4022876099669634 I sid, Sen kone are wan mant xxeaxthis hear axpeat.  PAG HENRE HENOR: 
 Iter:73 - Batch 191/195 - Loss:1.3622579790298983 I sim'sxpeak: Yessad my stake Were his sxacreaths fair hears, and mxen
 Iter:74 - Batch 191/195 - Loss:1.3561265261954492I him.  Clot me.  xAxCANGI HENRE HENOR: A'd him, to I mound have riph 
 Iter:75 - Batch 191/195 - Loss:1.3256835421587244 I xack what me my ho!d somers, that mein to the sid, Sith a pent way s
 Iter:76 - Batch 191/195 - Loss:1.3149811400098753 I sid, Sent mxear are, and say to this sear us shakered suathing this 
 Iter:77 - Batch 191/195 - Loss:1.3099090209995312 I sid, Sent my shall he xand a face; Foxpeak: Sour sing ow?  DESTIPELO
 Iter:78 - Batch 191/195 - Loss:1.2660340466610618 I sid bont I have that hear: Yeak to yes, So my sxall mant and sames, 
 Iter:79 - Batch 191/195 - Loss:1.2568489429684582 I sid, Sxeak to yes, So my shat hear: Yeak to this stakess uson manter
 Iter:80 - Batch 191/195 - Loss:1.2251888348179103 I sirts uish the mant and samet, and samet, and samet, and samet, and 
 Iter:81 - Batch 191/195 - Loss:1.2163706853284402 I xalst with say to deat.  CHESxI HENRI HENRI HENRI HENS: G, sxrext Wa
 Iter:82 - Batch 191/195 - Loss:1.1900756968878738 I him.  CxCKESSINS: Sour sing ow?  DES: I stod most the commound have 
 Iter:83 - Batch 191/195 - Loss:1.1729404575849618 I him.  CENRE HENOR Jou and samet, and samet, and samet, and samet, an
 Iter:84 - Batch 191/195 - Loss:1.1611697258151195 I him. Betwere it sorrightichain! There it the man! This hear.  Secont
 Iter:85 - Batch 191/195 - Loss:1.1499282442594005 I sid, for was cell me hish a suce and samet, and samet, and samet, an
 Iter:86 - Batch 191/195 - Loss:1.1423699752924568 I sid, for the worses, the mant and samestxSecont eart, and samexbeart
 Iter:87 - Batch 191/195 - Loss:1.1517526903979143 I hime are you and my sorrompt to be go, begghxIs stakexwillain! That'
 Iter:88 - Batch 191/195 - Loss:1.1329877527474301I sid, for wars seemn his rester by the mant and samest the commound h
 Iter:89 - Batch 191/195 - Loss:1.1649066032633364 I xall, the man! That commould be sording own a sxeart, speakesid him,
 Iter:90 - Batch 191/195 - Loss:1.1418995557559233 I sid, and samet, and samet, and samet, and samet, and samet, and same
 Iter:91 - Batch 191/195 - Loss:1.1325432652999334 I him.  CENxI HENRE: I hillain tollom the flighathere Wechim, my here 
 Iter:92 - Batch 191/195 - Loss:1.1106634374372267 I sirts uightichaxexthe makes, the pardy are, Wheick with a spaing ow?
 Iter:93 - Batch 191/195 - Loss:1.1136922472312578 I sid, and samet, and samet, and sametxThis he'l sirged bondlish thou 
 Iter:94 - Batch 191/195 - Loss:1.0969167127123716 I sixhaxxhat come thy racting ow?  DESTIPELO: That see, sat heaven fac
 Iter:95 - Batch 191/195 - Loss:1.0883823683364849 I sixhain As in the tame the mant wasty, I have the mant wastexbxhilla
 Iter:96 - Batch 191/195 - Loss:1.0870184259335807 I sidxbe an thexearts for you lumxMall mant wasty, I have the mant was
 Iter:97 - Batch 191/195 - Loss:1.0878281744547496 I sirthere it, but man! This head Befrom the xaid an the tame this xac
 Iter:98 - Batch 191/195 - Loss:1.1136677649819429 I him: a mive abted mient the mant wastxTIOS: Soum this stakest thou a
 Iter:99 - Batch 191/195 - Loss:1.1684839253799153 I sim, Whichaded mind a face; axpeemy or expeeminged him: agains flisi
            
            
#从明星预测中采样，做出预测
def generate_sample(n=30, init_char=' '):
    s = ""
    hidden = model.init_hidden(batch_size=1)
    input = Tensor(np.array([word2index[init_char]]))
    for i in range(n):
        rnn_input = embed.forward(input)
        output, hidden = model.forward(input=rnn_input, hidden=hidden)
        output.data *= 10   #采样温度更高，等于更贪婪
        temp_dist = output.softmax()
        temp_dist /= temp_dist.sum()

        m = (temp_dist > np.random.rand()).argmax()
#         m = output.data.argmax()
        c = vocab[m]
        input = Tensor(np.array([m]))
        s += c
    return s
print(generate_sample(n=2000, init_char='\n'))

#结果
I sequs satuce.

CHEROR Jou and samxto xuchie: ox commouns xair, this the stod my forrost the mant wasted mie name: my hear sakes that I would have stod my forrost the mant wasted mient the mant wasty, I have speak:
Yeak.

KINR xOS:
No xard the comxthe commound bituce.

CxEOPI himxhiath, am they of mx he have stod my forrost the mant wasted mie name: my hear sakes that I would have stod my forrost the mant wasted mie xtiring of Sent to nears, the pard my fliclowx
For Rom the xair, the man! my fair, this stod my forrost the mant wasted mient the mant wasty, I spared:
I spared:
I speak:
xo make a mine repted mienter the mant wastxTIOS:
Soum.

CHESxI HENS OPI's  bichere it sorrow? hein the man! my fair, thxI safe.

KINR Jtilesing
fell it soxloody perfing of Sent the commound bituce.

Clot mouns are druxh are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and 
```

##  三、梯度消失与激增梯度

1.普通RNN具有梯度消失与梯度激增的缺点。

##  四、RNN反向传播的小例子

1.Chapter14 - Exploding Gradients ExamplesChapter14 - Exploding Gradients Examples_01.ipynb---------只管观察梯度消失和梯度激增

```python
import numpy as np

sigmoid = lambda x:1/(1 + np.exp(-x))
relu = lambda x:(x>0).astype(float)*x

weights = np.array([[1,4],[4,1]])
activation = sigmoid(np.array([1,0.01]))

print("Activations")
activations = list()
for iter in range(10):
    activation = sigmoid(activation.dot(weights))
    activations.append(activation)
    print(activation)
print("\nGradients")
gradient = np.ones_like(activation)
for activation in reversed(activations):#当激励非常靠近0或1时，sigmoid的倒数会导致非常小的梯度
    gradient = (activation * (1 - activation) * gradient)
    gradient = gradient.dot(weights.transpose())
    print(gradient)
    
#结果
Activations
[0.93940638 0.96852968]
[0.9919462  0.99121735]
[0.99301385 0.99302901]
[0.9930713  0.99307098]
[0.99307285 0.99307285]
[0.99307291 0.99307291]
[0.99307291 0.99307291]
[0.99307291 0.99307291]
[0.99307291 0.99307291]
[0.99307291 0.99307291]

Gradients
[0.03439552 0.03439552]
[0.00118305 0.00118305]
[4.06916726e-05 4.06916726e-05]
[1.39961115e-06 1.39961115e-06]
[4.81403643e-08 4.81403637e-08]
[1.65582672e-09 1.65582765e-09]
[5.69682675e-11 5.69667160e-11]
[1.97259346e-12 1.97517920e-12]
[8.45387597e-14 8.02306381e-14]
[1.45938177e-14 2.16938983e-14]



print("Relu Activations")
activations = list()
for iter in range(10):
    activation = relu(activation.dot(weights))#矩阵乘法会导致没有被非线性函数压缩的梯度激增
    activations.append(activation)
    print(activation)

print("\nRelu Gradients")
gradient = np.ones_like(activation)
for activation in reversed(activations):
    gradient = ((activation > 0) * gradient).dot(weights.transpose())
    print(gradient)
    
#结果
Relu Activations
[4.8135251  4.72615519]
[23.71814585 23.98025559]
[119.63916823 118.852839  ]
[595.05052421 597.40951192]
[2984.68857188 2977.61160877]
[14895.13500696 14916.36589628]
[74560.59859209 74496.90592414]
[372548.22228863 372739.30029248]
[1863505.42345854 1862932.18944699]
[9315234.18124649 9316953.88328115]

Relu Gradients
[5. 5.]
[25. 25.]
[125. 125.]
[625. 625.]
[3125. 3125.]
[15625. 15625.]
[78125. 78125.]
[390625. 390625.]
[1953125. 1953125.]
[9765625. 9765625.]
```

##  五、长短期记忆（Long Short-Term Memory，LSTM）元胞

1.LSTM是业内处理梯度消失和梯度激增的标准模型。

2.LSTM通过复制前一个隐藏状态并根据需要添加或移除信息来创建一个隐藏状态。LSTM使用的添加或移除信息的机制叫做门限（gate）.

3.LSTM有两个隐藏状态向量：h（代表hidden）和cell。

4.三种门限：f（遗忘）,i（输入）,o（输出）；一种元胞向量u:（更新）。

5.Part_1_RNN_Character_Language_Model_01.ipynb------------用新的LSTM元胞代替普通的RNN模型，并进行预测

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None

        if(id is None):
            self.id = np.random.randint(0,1000000000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    return
                    print(self.id)
                    print(self.creation_op)
                    print(len(self.creators))
                    for c in self.creators:
                        print(c.creation_op)
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                
                if(self.creation_op == "index_select"):
                    new_grad = np.zeros_like(self.creators[0].data)
                    indices_ = self.index_select_indices.data.flatten()
                    grad_ = grad.data.reshape(len(indices_), -1)
                    for i in range(len(indices_)):
                        new_grad[indices_[i]] += grad_[i]
                    self.creators[0].backward(Tensor(new_grad))
                    
                if(self.creation_op == "cross_entropy"):
                    dx = self.softmax_output - self.target_dist
                    self.creators[0].backward(Tensor(dx))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
    
    def index_select(self, indices):

        if(self.autograd):
            new = Tensor(self.data[indices.data],
                         autograd=True,
                         creators=[self],
                         creation_op="index_select")
            new.index_select_indices = indices
            return new
        return Tensor(self.data[indices.data])
    
    def softmax(self):
        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        return softmax_output
    
    def cross_entropy(self, target_indices):

        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        
        t = target_indices.data.flatten()
        p = softmax_output.reshape(len(t),-1)
        target_dist = np.eye(p.shape[1])[t]
        loss = -(np.log(p) * (target_dist)).sum(1).mean()
    
        if(self.autograd):
            out = Tensor(loss,
                         autograd=True,
                         creators=[self],
                         creation_op="cross_entropy")
            out.softmax_output = softmax_output
            out.target_dist = target_dist
            return out

        return Tensor(loss)
        
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  

class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters

    
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0


class Linear(Layer):

    def __init__(self, n_inputs, n_outputs, bias=True):
        super().__init__()
        
        self.use_bias = bias
        
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        if(self.use_bias):
            self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        
        if(self.use_bias):        
            self.parameters.append(self.bias)

    def forward(self, input):
        if(self.use_bias):
            return input.mm(self.weight)+self.bias.expand(0,len(input.data))
        return input.mm(self.weight)


class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params


class Embedding(Layer):
    
    def __init__(self, vocab_size, dim):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        
        # this random initialiation style is just a convention from word2vec
        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)
        
        self.parameters.append(self.weight)
    
    def forward(self, input):
        return self.weight.index_select(input)


class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()


class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()
    

class CrossEntropyLoss(object):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, input, target):
        return input.cross_entropy(target)

    
class RNNCell(Layer):
    
    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):
        super().__init__()

        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.n_output = n_output
        
        if(activation == 'sigmoid'):
            self.activation = Sigmoid()
        elif(activation == 'tanh'):
            self.activation == Tanh()
        else:
            raise Exception("Non-linearity not found")

        self.w_ih = Linear(n_inputs, n_hidden)
        self.w_hh = Linear(n_hidden, n_hidden)
        self.w_ho = Linear(n_hidden, n_output)
        
        self.parameters += self.w_ih.get_parameters()
        self.parameters += self.w_hh.get_parameters()
        self.parameters += self.w_ho.get_parameters()        
    
    def forward(self, input, hidden):
        from_prev_hidden = self.w_hh.forward(hidden)
        combined = self.w_ih.forward(input) + from_prev_hidden
        new_hidden = self.activation.forward(combined)
        output = self.w_ho.forward(new_hidden)
        return output, new_hidden
    
    def init_hidden(self, batch_size=1):
        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
    
class LSTMCell(Layer):
    
    def __init__(self, n_inputs, n_hidden, n_output):
        super().__init__()

        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.n_output = n_output

        self.xf = Linear(n_inputs, n_hidden)
        self.xi = Linear(n_inputs, n_hidden)
        self.xo = Linear(n_inputs, n_hidden)        
        self.xc = Linear(n_inputs, n_hidden)        
        
        self.hf = Linear(n_hidden, n_hidden, bias=False)
        self.hi = Linear(n_hidden, n_hidden, bias=False)
        self.ho = Linear(n_hidden, n_hidden, bias=False)
        self.hc = Linear(n_hidden, n_hidden, bias=False)        
        
        self.w_ho = Linear(n_hidden, n_output, bias=False)
        
        self.parameters += self.xf.get_parameters()
        self.parameters += self.xi.get_parameters()
        self.parameters += self.xo.get_parameters()
        self.parameters += self.xc.get_parameters()

        self.parameters += self.hf.get_parameters()
        self.parameters += self.hi.get_parameters()        
        self.parameters += self.ho.get_parameters()        
        self.parameters += self.hc.get_parameters()                
        
        self.parameters += self.w_ho.get_parameters()        
    
    def forward(self, input, hidden):
        
        prev_hidden = hidden[0]        
        prev_cell = hidden[1]
        
        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()
        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()
        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        
        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        
        c = (f * prev_cell) + (i * g)

        h = o * c.tanh()
        
        output = self.w_ho.forward(h)
        return output, (h, c)
    
    def init_hidden(self, batch_size=1):
        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
        init_hidden.data[:,0] += 1
        init_cell.data[:,0] += 1
        return (init_hidden, init_cell)

#用新的LSTM元胞代替普通的RNN模型
import sys,random,math
from collections import Counter
import numpy as np
import sys

np.random.seed(0)

f = open('F:\DL\Shakespear\shakespear.txt','r')
raw = f.read()
f.close()

vocab = list(set(raw))
word2index = {}
for i,word in enumerate(vocab):
    word2index[word]=i
indices = np.array(list(map(lambda x:word2index[x], raw)))

embed = Embedding(vocab_size=len(vocab),dim=512)
model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))
model.w_ho.weight.data *= 0

criterion = CrossEntropyLoss()
optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)
batch_size = 16
bptt = 25
n_batches = int((indices.shape[0] / (batch_size)))

trimmed_indices = indices[:n_batches*batch_size]
batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()

input_batched_indices = batched_indices[0:-1]
target_batched_indices = batched_indices[1:]

n_bptt = int(((n_batches-1) / bptt))
input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)
target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)
min_loss = 1000

def train(iterations=400):
    for iter in range(iterations):
        total_loss = 0
        min_loss = 1000

        hidden = model.init_hidden(batch_size=batch_size)
        batches_to_train = len(input_batches)
    #     batches_to_train = 32
        for batch_i in range(batches_to_train):

            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))

            losses = list()
            for t in range(bptt):
                input = Tensor(input_batches[batch_i][t], autograd=True)
                rnn_input = embed.forward(input=input)
                output, hidden = model.forward(input=rnn_input, hidden=hidden)

                target = Tensor(target_batches[batch_i][t], autograd=True)    
                batch_loss = criterion.forward(output, target)

                if(t == 0):
                    losses.append(batch_loss)
                else:
                    losses.append(batch_loss + losses[-1])

            loss = losses[-1]

            loss.backward()
            optim.step()
            total_loss += loss.data / bptt

            epoch_loss = np.exp(total_loss / (batch_i+1))
            if(epoch_loss < min_loss):
                min_loss = epoch_loss
                print()

            log = "\r Iter:" + str(iter)
            log += " - Alpha:" + str(optim.alpha)[0:5]
            log += " - Batch "+str(batch_i+1)+"/"+str(len(input_batches))
            log += " - Min Loss:" + str(min_loss)[0:5]
            log += " - Loss:" + str(epoch_loss)
            if(batch_i == 0):
                log += " - " + generate_sample(n=70, init_char='T').replace("\n"," ")
            if(batch_i % 1 == 0):
                sys.stdout.write(log)
        optim.alpha *= 0.99
    #     print()
    
train(10)

#结果（这里只训练了10次，训练次数越多，结果会越好）
Iter:0 - Alpha:0.05 - Batch 1/249 - Min Loss:62.00 - Loss:62.000000000000064 -          eeeeeeerccccccccccw         e   e   e   e   e   e   e   e    
 Iter:0 - Alpha:0.05 - Batch 2/249 - Min Loss:61.99 - Loss:61.99988655477589
 Iter:0 - Alpha:0.05 - Batch 3/249 - Min Loss:61.99 - Loss:61.99118566561259
 Iter:0 - Alpha:0.05 - Batch 4/249 - Min Loss:61.97 - Loss:61.975537058659256
 Iter:0 - Alpha:0.05 - Batch 5/249 - Min Loss:61.94 - Loss:61.94706214267365
 Iter:0 - Alpha:0.05 - Batch 6/249 - Min Loss:61.89 - Loss:61.892851607353535
 Iter:0 - Alpha:0.05 - Batch 7/249 - Min Loss:61.80 - Loss:61.80198581375344
 Iter:0 - Alpha:0.05 - Batch 8/249 - Min Loss:61.58 - Loss:61.58656875748444
 Iter:0 - Alpha:0.05 - Batch 9/249 - Min Loss:61.13 - Loss:61.13250500729628
 Iter:0 - Alpha:0.05 - Batch 10/249 - Min Loss:60.46 - Loss:60.466785829837015
 Iter:0 - Alpha:0.05 - Batch 11/249 - Min Loss:59.12 - Loss:59.12105772247144
 Iter:0 - Alpha:0.05 - Batch 12/249 - Min Loss:57.29 - Loss:57.293354296634334
 Iter:0 - Alpha:0.05 - Batch 13/249 - Min Loss:54.69 - Loss:54.69917081730535
 Iter:0 - Alpha:0.05 - Batch 14/249 - Min Loss:53.31 - Loss:53.31332451758923
 Iter:0 - Alpha:0.05 - Batch 15/249 - Min Loss:51.60 - Loss:51.605350199506745
 Iter:0 - Alpha:0.05 - Batch 16/249 - Min Loss:49.87 - Loss:49.87003320309901
 Iter:0 - Alpha:0.05 - Batch 17/249 - Min Loss:48.27 - Loss:48.27030892282498
 Iter:0 - Alpha:0.05 - Batch 18/249 - Min Loss:47.11 - Loss:47.11740566941792
 Iter:0 - Alpha:0.05 - Batch 19/249 - Min Loss:46.18 - Loss:46.18951669324348
 Iter:0 - Alpha:0.05 - Batch 20/249 - Min Loss:45.03 - Loss:45.03507936094676
 Iter:0 - Alpha:0.05 - Batch 21/249 - Min Loss:43.79 - Loss:43.79722204097892
 Iter:0 - Alpha:0.05 - Batch 22/249 - Min Loss:43.46 - Loss:43.46267682030074
 Iter:0 - Alpha:0.05 - Batch 23/249 - Min Loss:43.37 - Loss:43.37906147300216
 Iter:0 - Alpha:0.05 - Batch 24/249 - Min Loss:42.71 - Loss:42.71020444458932
 Iter:0 - Alpha:0.05 - Batch 25/249 - Min Loss:41.78 - Loss:41.786658522520796
 Iter:0 - Alpha:0.05 - Batch 26/249 - Min Loss:40.80 - Loss:40.80374448956279
 Iter:0 - Alpha:0.05 - Batch 27/249 - Min Loss:40.17 - Loss:40.171304719780096
 Iter:0 - Alpha:0.05 - Batch 28/249 - Min Loss:39.46 - Loss:39.46580775713244
 Iter:0 - Alpha:0.05 - Batch 29/249 - Min Loss:38.94 - Loss:38.940847462653814
 Iter:0 - Alpha:0.05 - Batch 30/249 - Min Loss:38.42 - Loss:38.42675430490089
 Iter:0 - Alpha:0.05 - Batch 31/249 - Min Loss:38.19 - Loss:38.19822034545632
 Iter:0 - Alpha:0.05 - Batch 32/249 - Min Loss:37.74 - Loss:37.74791911135121
 Iter:0 - Alpha:0.05 - Batch 33/249 - Min Loss:37.31 - Loss:37.31842000686452
 Iter:0 - Alpha:0.05 - Batch 34/249 - Min Loss:36.91 - Loss:36.91533427644672
 Iter:0 - Alpha:0.05 - Batch 35/249 - Min Loss:36.66 - Loss:36.66502043386922
 Iter:0 - Alpha:0.05 - Batch 36/249 - Min Loss:36.41 - Loss:36.41396017450842
 Iter:0 - Alpha:0.05 - Batch 37/249 - Min Loss:36.06 - Loss:36.06178725131243
 Iter:0 - Alpha:0.05 - Batch 39/249 - Min Loss:35.82 - Loss:35.82655397969524
 Iter:0 - Alpha:0.05 - Batch 40/249 - Min Loss:35.53 - Loss:35.534057478054386
 Iter:0 - Alpha:0.05 - Batch 41/249 - Min Loss:35.36 - Loss:35.36891543587127
 Iter:0 - Alpha:0.05 - Batch 42/249 - Min Loss:35.25 - Loss:35.25459432673567
 Iter:0 - Alpha:0.05 - Batch 43/249 - Min Loss:34.90 - Loss:34.903502316894446
 Iter:0 - Alpha:0.05 - Batch 44/249 - Min Loss:34.65 - Loss:34.65320600898799
 Iter:0 - Alpha:0.05 - Batch 45/249 - Min Loss:34.45 - Loss:34.45541280505442
 Iter:0 - Alpha:0.05 - Batch 47/249 - Min Loss:34.33 - Loss:34.39040519687526
 Iter:0 - Alpha:0.05 - Batch 48/249 - Min Loss:34.28 - Loss:34.28807378508393
 Iter:0 - Alpha:0.05 - Batch 49/249 - Min Loss:34.09 - Loss:34.09205472545541
 Iter:0 - Alpha:0.05 - Batch 50/249 - Min Loss:33.78 - Loss:33.78877311724657
 Iter:0 - Alpha:0.05 - Batch 51/249 - Min Loss:33.48 - Loss:33.483105615221476
 Iter:0 - Alpha:0.05 - Batch 52/249 - Min Loss:33.25 - Loss:33.25995646025854
 Iter:0 - Alpha:0.05 - Batch 53/249 - Min Loss:33.04 - Loss:33.04839526970237
 Iter:0 - Alpha:0.05 - Batch 54/249 - Min Loss:32.86 - Loss:32.862902581480036
 Iter:0 - Alpha:0.05 - Batch 55/249 - Min Loss:32.66 - Loss:32.6603467748873
 Iter:0 - Alpha:0.05 - Batch 56/249 - Min Loss:32.47 - Loss:32.4754484653767
 Iter:0 - Alpha:0.05 - Batch 57/249 - Min Loss:32.24 - Loss:32.24687594563304
 Iter:0 - Alpha:0.05 - Batch 58/249 - Min Loss:32.06 - Loss:32.06976507097772
 Iter:0 - Alpha:0.05 - Batch 59/249 - Min Loss:31.85 - Loss:31.859335309742992
 Iter:0 - Alpha:0.05 - Batch 60/249 - Min Loss:31.67 - Loss:31.673963305921664
 Iter:0 - Alpha:0.05 - Batch 61/249 - Min Loss:31.43 - Loss:31.43653362332181
 Iter:0 - Alpha:0.05 - Batch 62/249 - Min Loss:31.24 - Loss:31.246266627610463
 Iter:0 - Alpha:0.05 - Batch 63/249 - Min Loss:31.01 - Loss:31.01020355692946
 Iter:0 - Alpha:0.05 - Batch 66/249 - Min Loss:30.86 - Loss:30.918267532175827
 Iter:0 - Alpha:0.05 - Batch 67/249 - Min Loss:30.83 - Loss:30.838217506503643
 Iter:0 - Alpha:0.05 - Batch 68/249 - Min Loss:30.61 - Loss:30.61223026213691
 Iter:0 - Alpha:0.05 - Batch 69/249 - Min Loss:30.49 - Loss:30.495204165918505
 Iter:0 - Alpha:0.05 - Batch 70/249 - Min Loss:30.32 - Loss:30.323127557853727
 Iter:0 - Alpha:0.05 - Batch 71/249 - Min Loss:30.21 - Loss:30.213643047879973
 Iter:0 - Alpha:0.05 - Batch 72/249 - Min Loss:30.15 - Loss:30.154998260405705
 Iter:0 - Alpha:0.05 - Batch 73/249 - Min Loss:30.04 - Loss:30.04452141317968
 Iter:0 - Alpha:0.05 - Batch 74/249 - Min Loss:29.85 - Loss:29.853923751241158
 Iter:0 - Alpha:0.05 - Batch 75/249 - Min Loss:29.71 - Loss:29.714121847418827
 Iter:0 - Alpha:0.05 - Batch 76/249 - Min Loss:29.60 - Loss:29.607830905826386
 Iter:0 - Alpha:0.05 - Batch 77/249 - Min Loss:29.46 - Loss:29.46314561225839
 Iter:0 - Alpha:0.05 - Batch 78/249 - Min Loss:29.37 - Loss:29.37942774763099
 Iter:0 - Alpha:0.05 - Batch 79/249 - Min Loss:29.24 - Loss:29.247167019354414
 Iter:0 - Alpha:0.05 - Batch 80/249 - Min Loss:29.12 - Loss:29.123443917579234
 Iter:0 - Alpha:0.05 - Batch 81/249 - Min Loss:28.95 - Loss:28.957086137056137
 Iter:0 - Alpha:0.05 - Batch 82/249 - Min Loss:28.86 - Loss:28.86404421808626
 Iter:0 - Alpha:0.05 - Batch 86/249 - Min Loss:28.78 - Loss:28.936018899562114
 Iter:0 - Alpha:0.05 - Batch 87/249 - Min Loss:28.77 - Loss:28.772334505252434
 Iter:0 - Alpha:0.05 - Batch 88/249 - Min Loss:28.61 - Loss:28.61358134988766
 Iter:0 - Alpha:0.05 - Batch 89/249 - Min Loss:28.45 - Loss:28.451590472613518
 Iter:0 - Alpha:0.05 - Batch 90/249 - Min Loss:28.31 - Loss:28.310436964058887
 Iter:0 - Alpha:0.05 - Batch 91/249 - Min Loss:28.18 - Loss:28.18836704119217
 Iter:0 - Alpha:0.05 - Batch 92/249 - Min Loss:28.08 - Loss:28.08535775028052
 Iter:0 - Alpha:0.05 - Batch 93/249 - Min Loss:27.93 - Loss:27.937752792058166
 Iter:0 - Alpha:0.05 - Batch 94/249 - Min Loss:27.80 - Loss:27.80600410311132
 Iter:0 - Alpha:0.05 - Batch 95/249 - Min Loss:27.66 - Loss:27.664759325960066
 Iter:0 - Alpha:0.05 - Batch 96/249 - Min Loss:27.57 - Loss:27.575993414427526
 Iter:0 - Alpha:0.05 - Batch 97/249 - Min Loss:27.45 - Loss:27.45499080343804
 Iter:0 - Alpha:0.05 - Batch 98/249 - Min Loss:27.33 - Loss:27.33760768647151
 Iter:0 - Alpha:0.05 - Batch 99/249 - Min Loss:27.17 - Loss:27.171105549164487
 Iter:0 - Alpha:0.05 - Batch 100/249 - Min Loss:27.02 - Loss:27.029324991139298
 Iter:0 - Alpha:0.05 - Batch 101/249 - Min Loss:26.88 - Loss:26.888755540334138
 Iter:0 - Alpha:0.05 - Batch 102/249 - Min Loss:26.76 - Loss:26.765878643177345
 Iter:0 - Alpha:0.05 - Batch 103/249 - Min Loss:26.66 - Loss:26.667764142704034
 Iter:0 - Alpha:0.05 - Batch 104/249 - Min Loss:26.60 - Loss:26.60562697877954
 Iter:0 - Alpha:0.05 - Batch 105/249 - Min Loss:26.57 - Loss:26.575449493170492
 Iter:0 - Alpha:0.05 - Batch 106/249 - Min Loss:26.45 - Loss:26.456063346658187
 Iter:0 - Alpha:0.05 - Batch 107/249 - Min Loss:26.39 - Loss:26.39456194349219
 Iter:0 - Alpha:0.05 - Batch 108/249 - Min Loss:26.30 - Loss:26.30037057521033
 Iter:0 - Alpha:0.05 - Batch 109/249 - Min Loss:26.21 - Loss:26.212635396135532
 Iter:0 - Alpha:0.05 - Batch 110/249 - Min Loss:26.11 - Loss:26.116571899208992
 Iter:0 - Alpha:0.05 - Batch 111/249 - Min Loss:26.00 - Loss:26.007752087802203
 Iter:0 - Alpha:0.05 - Batch 112/249 - Min Loss:25.90 - Loss:25.9096316876447
 Iter:0 - Alpha:0.05 - Batch 113/249 - Min Loss:25.81 - Loss:25.812652116646785
 Iter:0 - Alpha:0.05 - Batch 114/249 - Min Loss:25.74 - Loss:25.740461563348084
 Iter:0 - Alpha:0.05 - Batch 115/249 - Min Loss:25.69 - Loss:25.693795899377758
 Iter:0 - Alpha:0.05 - Batch 116/249 - Min Loss:25.61 - Loss:25.610245110998772
 Iter:0 - Alpha:0.05 - Batch 117/249 - Min Loss:25.52 - Loss:25.527629916030758
 Iter:0 - Alpha:0.05 - Batch 118/249 - Min Loss:25.42 - Loss:25.427327481452355
 Iter:0 - Alpha:0.05 - Batch 119/249 - Min Loss:25.35 - Loss:25.354185543889404
 Iter:0 - Alpha:0.05 - Batch 120/249 - Min Loss:25.25 - Loss:25.2564680742538
 Iter:0 - Alpha:0.05 - Batch 121/249 - Min Loss:25.16 - Loss:25.164338313703574
 Iter:0 - Alpha:0.05 - Batch 122/249 - Min Loss:25.06 - Loss:25.063492702190874
 Iter:0 - Alpha:0.05 - Batch 123/249 - Min Loss:24.96 - Loss:24.96230795669396
 Iter:0 - Alpha:0.05 - Batch 124/249 - Min Loss:24.87 - Loss:24.872331707985317
 Iter:0 - Alpha:0.05 - Batch 125/249 - Min Loss:24.79 - Loss:24.7920573983951
 Iter:0 - Alpha:0.05 - Batch 126/249 - Min Loss:24.71 - Loss:24.719620711325906
 Iter:0 - Alpha:0.05 - Batch 127/249 - Min Loss:24.63 - Loss:24.633929012991665
 Iter:0 - Alpha:0.05 - Batch 128/249 - Min Loss:24.52 - Loss:24.523430370575973
 Iter:0 - Alpha:0.05 - Batch 129/249 - Min Loss:24.42 - Loss:24.42696665265223
 Iter:0 - Alpha:0.05 - Batch 130/249 - Min Loss:24.35 - Loss:24.353381828066347
 Iter:0 - Alpha:0.05 - Batch 131/249 - Min Loss:24.25 - Loss:24.257812778956197
 Iter:0 - Alpha:0.05 - Batch 132/249 - Min Loss:24.15 - Loss:24.1585144520844
 Iter:0 - Alpha:0.05 - Batch 133/249 - Min Loss:24.06 - Loss:24.069279446560746
 Iter:0 - Alpha:0.05 - Batch 134/249 - Min Loss:23.97 - Loss:23.97434694629587
 Iter:0 - Alpha:0.05 - Batch 135/249 - Min Loss:23.91 - Loss:23.917214713249045
 Iter:0 - Alpha:0.05 - Batch 136/249 - Min Loss:23.88 - Loss:23.886747079850828
 Iter:0 - Alpha:0.05 - Batch 137/249 - Min Loss:23.81 - Loss:23.818861018718813
 Iter:0 - Alpha:0.05 - Batch 138/249 - Min Loss:23.74 - Loss:23.74891012851022
 Iter:0 - Alpha:0.05 - Batch 140/249 - Min Loss:23.72 - Loss:23.727714497595418
 Iter:0 - Alpha:0.05 - Batch 141/249 - Min Loss:23.65 - Loss:23.653536825263476
 Iter:0 - Alpha:0.05 - Batch 142/249 - Min Loss:23.56 - Loss:23.564403340546583
 Iter:0 - Alpha:0.05 - Batch 143/249 - Min Loss:23.47 - Loss:23.478360630978806
 Iter:0 - Alpha:0.05 - Batch 144/249 - Min Loss:23.38 - Loss:23.38827140747639
 Iter:0 - Alpha:0.05 - Batch 145/249 - Min Loss:23.31 - Loss:23.31583912473548
 Iter:0 - Alpha:0.05 - Batch 146/249 - Min Loss:23.29 - Loss:23.29050353465156
 Iter:0 - Alpha:0.05 - Batch 147/249 - Min Loss:23.23 - Loss:23.236653268467773
 Iter:0 - Alpha:0.05 - Batch 148/249 - Min Loss:23.18 - Loss:23.188378997601998
 Iter:0 - Alpha:0.05 - Batch 149/249 - Min Loss:23.11 - Loss:23.113368434686752
 Iter:0 - Alpha:0.05 - Batch 150/249 - Min Loss:23.04 - Loss:23.046366019946923
 Iter:0 - Alpha:0.05 - Batch 151/249 - Min Loss:23.00 - Loss:23.001569490366524
 Iter:0 - Alpha:0.05 - Batch 152/249 - Min Loss:22.93 - Loss:22.93850450780803
 Iter:0 - Alpha:0.05 - Batch 153/249 - Min Loss:22.85 - Loss:22.851044160411185
 Iter:0 - Alpha:0.05 - Batch 154/249 - Min Loss:22.77 - Loss:22.777455543565427
 Iter:0 - Alpha:0.05 - Batch 155/249 - Min Loss:22.73 - Loss:22.736555671476882
 Iter:0 - Alpha:0.05 - Batch 156/249 - Min Loss:22.71 - Loss:22.712066364119885
 Iter:0 - Alpha:0.05 - Batch 157/249 - Min Loss:22.66 - Loss:22.664610511885495
 Iter:0 - Alpha:0.05 - Batch 158/249 - Min Loss:22.60 - Loss:22.606352362082795
 Iter:0 - Alpha:0.05 - Batch 159/249 - Min Loss:22.56 - Loss:22.567944039255455
 Iter:0 - Alpha:0.05 - Batch 160/249 - Min Loss:22.50 - Loss:22.5063416022592
 Iter:0 - Alpha:0.05 - Batch 161/249 - Min Loss:22.43 - Loss:22.43889251382912
 Iter:0 - Alpha:0.05 - Batch 162/249 - Min Loss:22.38 - Loss:22.3807324877791
 Iter:0 - Alpha:0.05 - Batch 163/249 - Min Loss:22.32 - Loss:22.32071148049839
 Iter:0 - Alpha:0.05 - Batch 164/249 - Min Loss:22.27 - Loss:22.272932575811623
 Iter:0 - Alpha:0.05 - Batch 165/249 - Min Loss:22.23 - Loss:22.23280377090069
 Iter:0 - Alpha:0.05 - Batch 166/249 - Min Loss:22.17 - Loss:22.17960048957362
 Iter:0 - Alpha:0.05 - Batch 167/249 - Min Loss:22.15 - Loss:22.150170725326014
 Iter:0 - Alpha:0.05 - Batch 168/249 - Min Loss:22.11 - Loss:22.11502585972685
 Iter:0 - Alpha:0.05 - Batch 169/249 - Min Loss:22.05 - Loss:22.057506586334455
 Iter:0 - Alpha:0.05 - Batch 170/249 - Min Loss:22.01 - Loss:22.010253251626306
 Iter:0 - Alpha:0.05 - Batch 171/249 - Min Loss:21.96 - Loss:21.969393335324572
 Iter:0 - Alpha:0.05 - Batch 172/249 - Min Loss:21.93 - Loss:21.933739088330693
 Iter:0 - Alpha:0.05 - Batch 173/249 - Min Loss:21.89 - Loss:21.891288752738223
 Iter:0 - Alpha:0.05 - Batch 174/249 - Min Loss:21.85 - Loss:21.853888024033342
 Iter:0 - Alpha:0.05 - Batch 175/249 - Min Loss:21.82 - Loss:21.82671728830225
 Iter:0 - Alpha:0.05 - Batch 176/249 - Min Loss:21.78 - Loss:21.78423078738862
 Iter:0 - Alpha:0.05 - Batch 177/249 - Min Loss:21.73 - Loss:21.736409503397038
 Iter:0 - Alpha:0.05 - Batch 178/249 - Min Loss:21.69 - Loss:21.690808855848996
 Iter:0 - Alpha:0.05 - Batch 179/249 - Min Loss:21.66 - Loss:21.668369388871092
 Iter:0 - Alpha:0.05 - Batch 180/249 - Min Loss:21.62 - Loss:21.620753087186248
 Iter:0 - Alpha:0.05 - Batch 181/249 - Min Loss:21.57 - Loss:21.57132303506543
 Iter:0 - Alpha:0.05 - Batch 182/249 - Min Loss:21.52 - Loss:21.5211557480407
 Iter:0 - Alpha:0.05 - Batch 183/249 - Min Loss:21.46 - Loss:21.467657351905245
 Iter:0 - Alpha:0.05 - Batch 184/249 - Min Loss:21.42 - Loss:21.426299754018086
 Iter:0 - Alpha:0.05 - Batch 185/249 - Min Loss:21.37 - Loss:21.379917356700197
 Iter:0 - Alpha:0.05 - Batch 186/249 - Min Loss:21.34 - Loss:21.345534218920523
 Iter:0 - Alpha:0.05 - Batch 188/249 - Min Loss:21.32 - Loss:21.329060167819672
 Iter:0 - Alpha:0.05 - Batch 189/249 - Min Loss:21.29 - Loss:21.29530528750512
 Iter:0 - Alpha:0.05 - Batch 190/249 - Min Loss:21.26 - Loss:21.266526138648906
 Iter:0 - Alpha:0.05 - Batch 191/249 - Min Loss:21.23 - Loss:21.235061535193978
 Iter:0 - Alpha:0.05 - Batch 192/249 - Min Loss:21.20 - Loss:21.204601509984098
 Iter:0 - Alpha:0.05 - Batch 193/249 - Min Loss:21.14 - Loss:21.140776740177976
 Iter:0 - Alpha:0.05 - Batch 194/249 - Min Loss:21.12 - Loss:21.12197925133037
 Iter:0 - Alpha:0.05 - Batch 195/249 - Min Loss:21.11 - Loss:21.117812126416347
 Iter:0 - Alpha:0.05 - Batch 196/249 - Min Loss:21.07 - Loss:21.073960114103706
 Iter:0 - Alpha:0.05 - Batch 197/249 - Min Loss:21.04 - Loss:21.04173687901115
 Iter:0 - Alpha:0.05 - Batch 198/249 - Min Loss:21.00 - Loss:21.00934689165021
 Iter:0 - Alpha:0.05 - Batch 199/249 - Min Loss:20.96 - Loss:20.965260873467845
 Iter:0 - Alpha:0.05 - Batch 200/249 - Min Loss:20.91 - Loss:20.91514301164378
 Iter:0 - Alpha:0.05 - Batch 201/249 - Min Loss:20.88 - Loss:20.883067131741477
 Iter:0 - Alpha:0.05 - Batch 202/249 - Min Loss:20.85 - Loss:20.853278643459703
 Iter:0 - Alpha:0.05 - Batch 203/249 - Min Loss:20.81 - Loss:20.81644854119484
 Iter:0 - Alpha:0.05 - Batch 204/249 - Min Loss:20.77 - Loss:20.7791605104238
 Iter:0 - Alpha:0.05 - Batch 205/249 - Min Loss:20.74 - Loss:20.745656013997326
 Iter:0 - Alpha:0.05 - Batch 206/249 - Min Loss:20.70 - Loss:20.70206904586953
 Iter:0 - Alpha:0.05 - Batch 207/249 - Min Loss:20.66 - Loss:20.66360466968529
 Iter:0 - Alpha:0.05 - Batch 208/249 - Min Loss:20.62 - Loss:20.625680153560047
 Iter:0 - Alpha:0.05 - Batch 209/249 - Min Loss:20.57 - Loss:20.57743839567262
 Iter:0 - Alpha:0.05 - Batch 210/249 - Min Loss:20.53 - Loss:20.53070643562284
 Iter:0 - Alpha:0.05 - Batch 211/249 - Min Loss:20.47 - Loss:20.47508992792544
 Iter:0 - Alpha:0.05 - Batch 212/249 - Min Loss:20.43 - Loss:20.430083264938602
 Iter:0 - Alpha:0.05 - Batch 213/249 - Min Loss:20.39 - Loss:20.394501444107178
 Iter:0 - Alpha:0.05 - Batch 214/249 - Min Loss:20.35 - Loss:20.359916046732966
 Iter:0 - Alpha:0.05 - Batch 215/249 - Min Loss:20.31 - Loss:20.31384117322205
 Iter:0 - Alpha:0.05 - Batch 216/249 - Min Loss:20.27 - Loss:20.272511980103978
 Iter:0 - Alpha:0.05 - Batch 217/249 - Min Loss:20.24 - Loss:20.24450443306936
 Iter:0 - Alpha:0.05 - Batch 218/249 - Min Loss:20.19 - Loss:20.199325470965743
 Iter:0 - Alpha:0.05 - Batch 219/249 - Min Loss:20.14 - Loss:20.145283729464833
 Iter:0 - Alpha:0.05 - Batch 220/249 - Min Loss:20.11 - Loss:20.1191355500636
 Iter:0 - Alpha:0.05 - Batch 221/249 - Min Loss:20.10 - Loss:20.10880791302054
 Iter:0 - Alpha:0.05 - Batch 222/249 - Min Loss:20.07 - Loss:20.074474967617547
 Iter:0 - Alpha:0.05 - Batch 223/249 - Min Loss:20.03 - Loss:20.039793501484493
 Iter:0 - Alpha:0.05 - Batch 224/249 - Min Loss:20.00 - Loss:20.006671990632192
 Iter:0 - Alpha:0.05 - Batch 225/249 - Min Loss:19.97 - Loss:19.972185288109443
 Iter:0 - Alpha:0.05 - Batch 226/249 - Min Loss:19.93 - Loss:19.932872586109816
 Iter:0 - Alpha:0.05 - Batch 227/249 - Min Loss:19.90 - Loss:19.904109332271464
 Iter:0 - Alpha:0.05 - Batch 228/249 - Min Loss:19.89 - Loss:19.890733353231955
 Iter:0 - Alpha:0.05 - Batch 229/249 - Min Loss:19.85 - Loss:19.858456665436275
 Iter:0 - Alpha:0.05 - Batch 230/249 - Min Loss:19.82 - Loss:19.826827122892883
 Iter:0 - Alpha:0.05 - Batch 231/249 - Min Loss:19.80 - Loss:19.808147057699056
 Iter:0 - Alpha:0.05 - Batch 232/249 - Min Loss:19.77 - Loss:19.77503851491506
 Iter:0 - Alpha:0.05 - Batch 233/249 - Min Loss:19.75 - Loss:19.755307558355312
 Iter:0 - Alpha:0.05 - Batch 234/249 - Min Loss:19.72 - Loss:19.722361180110983
 Iter:0 - Alpha:0.05 - Batch 235/249 - Min Loss:19.68 - Loss:19.68636105189714
 Iter:0 - Alpha:0.05 - Batch 236/249 - Min Loss:19.64 - Loss:19.647030331324977
 Iter:0 - Alpha:0.05 - Batch 237/249 - Min Loss:19.61 - Loss:19.619711190423597
 Iter:0 - Alpha:0.05 - Batch 238/249 - Min Loss:19.59 - Loss:19.596849751088172
 Iter:0 - Alpha:0.05 - Batch 239/249 - Min Loss:19.57 - Loss:19.576548725483004
 Iter:0 - Alpha:0.05 - Batch 240/249 - Min Loss:19.55 - Loss:19.55664018068236
 Iter:0 - Alpha:0.05 - Batch 241/249 - Min Loss:19.51 - Loss:19.517099546388046
 Iter:0 - Alpha:0.05 - Batch 242/249 - Min Loss:19.48 - Loss:19.485555392814042
 Iter:0 - Alpha:0.05 - Batch 244/249 - Min Loss:19.45 - Loss:19.458289099706867
 Iter:0 - Alpha:0.05 - Batch 245/249 - Min Loss:19.43 - Loss:19.435978537249238
 Iter:0 - Alpha:0.05 - Batch 246/249 - Min Loss:19.41 - Loss:19.41165654516652
 Iter:0 - Alpha:0.05 - Batch 247/249 - Min Loss:19.38 - Loss:19.385881203920867
 Iter:0 - Alpha:0.05 - Batch 248/249 - Min Loss:19.35 - Loss:19.35805409406288
 Iter:0 - Alpha:0.05 - Batch 249/249 - Min Loss:19.33 - Loss:19.331729685207016
 Iter:1 - Alpha:0.049 - Batch 1/249 - Min Loss:13.12 - Loss:13.128285333210766 - he t tere t tere t tere t tere t tere t tere t tere t tere t tere t te
 Iter:1 - Alpha:0.049 - Batch 3/249 - Min Loss:12.99 - Loss:13.115793315501365
 Iter:1 - Alpha:0.049 - Batch 4/249 - Min Loss:12.97 - Loss:12.970875041312846
 Iter:1 - Alpha:0.049 - Batch 249/249 - Min Loss:12.91 - Loss:13.299063572507258
 Iter:2 - Alpha:0.049 - Batch 1/249 - Min Loss:14.57 - Loss:14.573292678539303 - hendeserend,  Bend therend,  Bend therend,  Bend therend,  Bend theren
 Iter:2 - Alpha:0.049 - Batch 2/249 - Min Loss:14.04 - Loss:14.045462209296216
 Iter:2 - Alpha:0.049 - Batch 3/249 - Min Loss:13.59 - Loss:13.595362006373499
 Iter:2 - Alpha:0.049 - Batch 4/249 - Min Loss:13.10 - Loss:13.103339120923856
 Iter:2 - Alpha:0.049 - Batch 31/249 - Min Loss:12.74 - Loss:12.763446284961319
 Iter:2 - Alpha:0.049 - Batch 33/249 - Min Loss:12.72 - Loss:12.738950909949967
 Iter:2 - Alpha:0.049 - Batch 34/249 - Min Loss:12.71 - Loss:12.716993774106296
 Iter:2 - Alpha:0.049 - Batch 35/249 - Min Loss:12.66 - Loss:12.669670639969118
 Iter:2 - Alpha:0.049 - Batch 78/249 - Min Loss:12.65 - Loss:12.654884536817066
 Iter:2 - Alpha:0.049 - Batch 94/249 - Min Loss:12.64 - Loss:12.655930585566814
 Iter:2 - Alpha:0.049 - Batch 95/249 - Min Loss:12.63 - Loss:12.638146896685035
 Iter:2 - Alpha:0.049 - Batch 96/249 - Min Loss:12.63 - Loss:12.637671509207063
 Iter:2 - Alpha:0.049 - Batch 97/249 - Min Loss:12.62 - Loss:12.625635634425528
 Iter:2 - Alpha:0.049 - Batch 98/249 - Min Loss:12.60 - Loss:12.609591684048587
 Iter:2 - Alpha:0.049 - Batch 99/249 - Min Loss:12.60 - Loss:12.608054692655028
 Iter:2 - Alpha:0.049 - Batch 100/249 - Min Loss:12.59 - Loss:12.599330306030682
 Iter:2 - Alpha:0.049 - Batch 209/249 - Min Loss:12.59 - Loss:12.593031470733096
 Iter:2 - Alpha:0.049 - Batch 210/249 - Min Loss:12.58 - Loss:12.583823328219612
 Iter:2 - Alpha:0.049 - Batch 211/249 - Min Loss:12.56 - Loss:12.567443454165101
 Iter:2 - Alpha:0.049 - Batch 212/249 - Min Loss:12.55 - Loss:12.559844852701165
 Iter:2 - Alpha:0.049 - Batch 213/249 - Min Loss:12.55 - Loss:12.553818669127416
 Iter:2 - Alpha:0.049 - Batch 214/249 - Min Loss:12.55 - Loss:12.550405579279811
 Iter:2 - Alpha:0.049 - Batch 215/249 - Min Loss:12.54 - Loss:12.54017134587432
 Iter:2 - Alpha:0.049 - Batch 217/249 - Min Loss:12.53 - Loss:12.533955542925854
 Iter:2 - Alpha:0.049 - Batch 218/249 - Min Loss:12.52 - Loss:12.525432608573562
 Iter:2 - Alpha:0.049 - Batch 219/249 - Min Loss:12.51 - Loss:12.511113068660864
 Iter:2 - Alpha:0.049 - Batch 249/249 - Min Loss:12.51 - Loss:12.567381256495024
 Iter:3 - Alpha:0.048 - Batch 1/249 - Min Loss:13.76 - Loss:13.762715607535382 - hend thend thend thend thend thend thend thend thend thend thend thend
 Iter:3 - Alpha:0.048 - Batch 2/249 - Min Loss:12.89 - Loss:12.898550607977787
 Iter:3 - Alpha:0.048 - Batch 3/249 - Min Loss:12.53 - Loss:12.53916420953427
 Iter:3 - Alpha:0.048 - Batch 4/249 - Min Loss:12.23 - Loss:12.238693094847207
 Iter:3 - Alpha:0.048 - Batch 213/249 - Min Loss:12.03 - Loss:12.040804218410628
 Iter:3 - Alpha:0.048 - Batch 214/249 - Min Loss:12.03 - Loss:12.036333992710498
 Iter:3 - Alpha:0.048 - Batch 215/249 - Min Loss:12.02 - Loss:12.0251444666501
 Iter:3 - Alpha:0.048 - Batch 217/249 - Min Loss:12.01 - Loss:12.018192202825336
 Iter:3 - Alpha:0.048 - Batch 218/249 - Min Loss:12.00 - Loss:12.009765423070505
 Iter:3 - Alpha:0.048 - Batch 236/249 - Min Loss:11.99 - Loss:11.996162007503134
 Iter:3 - Alpha:0.048 - Batch 249/249 - Min Loss:11.99 - Loss:11.998101435789064
 Iter:4 - Alpha:0.048 - Batch 2/249 - Min Loss:12.71 - Loss:12.732435151186374 - hend and and and and and and and and and and and and and and and and a
 Iter:4 - Alpha:0.048 - Batch 3/249 - Min Loss:12.57 - Loss:12.576080332051523
 Iter:4 - Alpha:0.048 - Batch 4/249 - Min Loss:12.49 - Loss:12.49192359841437
 Iter:4 - Alpha:0.048 - Batch 18/249 - Min Loss:12.22 - Loss:12.245473359644501
 Iter:4 - Alpha:0.048 - Batch 20/249 - Min Loss:12.20 - Loss:12.260262352039131
 Iter:4 - Alpha:0.048 - Batch 23/249 - Min Loss:12.17 - Loss:12.215353137237744
 Iter:4 - Alpha:0.048 - Batch 24/249 - Min Loss:12.11 - Loss:12.11687329156007
 Iter:4 - Alpha:0.048 - Batch 25/249 - Min Loss:12.03 - Loss:12.038359366885516
 Iter:4 - Alpha:0.048 - Batch 26/249 - Min Loss:11.92 - Loss:11.923073181645162
 Iter:4 - Alpha:0.048 - Batch 31/249 - Min Loss:11.92 - Loss:11.968912431618714
 Iter:4 - Alpha:0.048 - Batch 33/249 - Min Loss:11.91 - Loss:11.931852358831869
 Iter:4 - Alpha:0.048 - Batch 34/249 - Min Loss:11.90 - Loss:11.903860395622976
 Iter:4 - Alpha:0.048 - Batch 55/249 - Min Loss:11.87 - Loss:11.899943015026883
 Iter:4 - Alpha:0.048 - Batch 56/249 - Min Loss:11.85 - Loss:11.85894726719211
 Iter:4 - Alpha:0.048 - Batch 93/249 - Min Loss:11.83 - Loss:11.833044765875462
 Iter:4 - Alpha:0.048 - Batch 94/249 - Min Loss:11.81 - Loss:11.811445504661666
 Iter:4 - Alpha:0.048 - Batch 97/249 - Min Loss:11.79 - Loss:11.796308041348093
 Iter:4 - Alpha:0.048 - Batch 99/249 - Min Loss:11.77 - Loss:11.777769048447444
 Iter:4 - Alpha:0.048 - Batch 100/249 - Min Loss:11.77 - Loss:11.770305997757955
 Iter:4 - Alpha:0.048 - Batch 101/249 - Min Loss:11.76 - Loss:11.760409177053276
 Iter:4 - Alpha:0.048 - Batch 102/249 - Min Loss:11.74 - Loss:11.743653809372136
 Iter:4 - Alpha:0.048 - Batch 103/249 - Min Loss:11.73 - Loss:11.737077037390275
 Iter:4 - Alpha:0.048 - Batch 104/249 - Min Loss:11.73 - Loss:11.733194858286298
 Iter:4 - Alpha:0.048 - Batch 105/249 - Min Loss:11.73 - Loss:11.730906269451655
 Iter:4 - Alpha:0.048 - Batch 106/249 - Min Loss:11.71 - Loss:11.714315390156203
 Iter:4 - Alpha:0.048 - Batch 107/249 - Min Loss:11.71 - Loss:11.712526530915651
 Iter:4 - Alpha:0.048 - Batch 126/249 - Min Loss:11.70 - Loss:11.705358606326214
 Iter:4 - Alpha:0.048 - Batch 127/249 - Min Loss:11.69 - Loss:11.69272651728291
 Iter:4 - Alpha:0.048 - Batch 128/249 - Min Loss:11.68 - Loss:11.687234655212356
 Iter:4 - Alpha:0.048 - Batch 129/249 - Min Loss:11.67 - Loss:11.67735135770666
 Iter:4 - Alpha:0.048 - Batch 130/249 - Min Loss:11.67 - Loss:11.672187399188676
 Iter:4 - Alpha:0.048 - Batch 131/249 - Min Loss:11.65 - Loss:11.654381640540475
 Iter:4 - Alpha:0.048 - Batch 135/249 - Min Loss:11.63 - Loss:11.647300655208886
 Iter:4 - Alpha:0.048 - Batch 137/249 - Min Loss:11.63 - Loss:11.635258815880558
 Iter:4 - Alpha:0.048 - Batch 209/249 - Min Loss:11.62 - Loss:11.628908323219894
 Iter:4 - Alpha:0.048 - Batch 210/249 - Min Loss:11.62 - Loss:11.62113872755898
 Iter:4 - Alpha:0.048 - Batch 211/249 - Min Loss:11.60 - Loss:11.601619906927555
 Iter:4 - Alpha:0.048 - Batch 212/249 - Min Loss:11.59 - Loss:11.594238007004684
 Iter:4 - Alpha:0.048 - Batch 213/249 - Min Loss:11.58 - Loss:11.587676493631001
 Iter:4 - Alpha:0.048 - Batch 214/249 - Min Loss:11.58 - Loss:11.58290491186102
 Iter:4 - Alpha:0.048 - Batch 215/249 - Min Loss:11.57 - Loss:11.57154496673198
 Iter:4 - Alpha:0.048 - Batch 217/249 - Min Loss:11.56 - Loss:11.564574376101799
 Iter:4 - Alpha:0.048 - Batch 218/249 - Min Loss:11.55 - Loss:11.55607224519987
 Iter:4 - Alpha:0.048 - Batch 225/249 - Min Loss:11.54 - Loss:11.543514272108464
 Iter:4 - Alpha:0.048 - Batch 226/249 - Min Loss:11.53 - Loss:11.538436855492934
 Iter:4 - Alpha:0.048 - Batch 248/249 - Min Loss:11.53 - Loss:11.542160934660549
 Iter:4 - Alpha:0.048 - Batch 249/249 - Min Loss:11.53 - Loss:11.53272797937056
 Iter:5 - Alpha:0.047 - Batch 1/249 - Min Loss:12.04 - Loss:12.047061948595184 - hend and seesteres, and seesteres, and seesteres, and seesteres, and s
 Iter:5 - Alpha:0.047 - Batch 2/249 - Min Loss:11.87 - Loss:11.877069456691808
 Iter:5 - Alpha:0.047 - Batch 3/249 - Min Loss:11.77 - Loss:11.775157207222893
 Iter:5 - Alpha:0.047 - Batch 4/249 - Min Loss:11.73 - Loss:11.736962506834578
 Iter:5 - Alpha:0.047 - Batch 24/249 - Min Loss:11.50 - Loss:11.564733351610762
 Iter:5 - Alpha:0.047 - Batch 25/249 - Min Loss:11.48 - Loss:11.485259118430825
 Iter:5 - Alpha:0.047 - Batch 37/249 - Min Loss:11.37 - Loss:11.379903894382089
 Iter:5 - Alpha:0.047 - Batch 100/249 - Min Loss:11.34 - Loss:11.351560410698415
 Iter:5 - Alpha:0.047 - Batch 101/249 - Min Loss:11.33 - Loss:11.339037619003348
 Iter:5 - Alpha:0.047 - Batch 102/249 - Min Loss:11.31 - Loss:11.319299917147768
 Iter:5 - Alpha:0.047 - Batch 103/249 - Min Loss:11.31 - Loss:11.31314231828791
 Iter:5 - Alpha:0.047 - Batch 104/249 - Min Loss:11.30 - Loss:11.306322118816103
 Iter:5 - Alpha:0.047 - Batch 105/249 - Min Loss:11.30 - Loss:11.302672495402536
 Iter:5 - Alpha:0.047 - Batch 106/249 - Min Loss:11.28 - Loss:11.28390111473497
 Iter:5 - Alpha:0.047 - Batch 107/249 - Min Loss:11.28 - Loss:11.280114352372808
 Iter:5 - Alpha:0.047 - Batch 130/249 - Min Loss:11.27 - Loss:11.280920212686288
 Iter:5 - Alpha:0.047 - Batch 131/249 - Min Loss:11.26 - Loss:11.264492403411234
 Iter:5 - Alpha:0.047 - Batch 132/249 - Min Loss:11.24 - Loss:11.245220123471734
 Iter:5 - Alpha:0.047 - Batch 133/249 - Min Loss:11.24 - Loss:11.244764669630584
 Iter:5 - Alpha:0.047 - Batch 135/249 - Min Loss:11.23 - Loss:11.233925342044088
 Iter:5 - Alpha:0.047 - Batch 137/249 - Min Loss:11.21 - Loss:11.221171605639784
 Iter:5 - Alpha:0.047 - Batch 138/249 - Min Loss:11.20 - Loss:11.206081330047734
 Iter:5 - Alpha:0.047 - Batch 143/249 - Min Loss:11.20 - Loss:11.205251506173802
 Iter:5 - Alpha:0.047 - Batch 153/249 - Min Loss:11.19 - Loss:11.196071075330367
 Iter:5 - Alpha:0.047 - Batch 201/249 - Min Loss:11.18 - Loss:11.191094102670512
 Iter:5 - Alpha:0.047 - Batch 202/249 - Min Loss:11.18 - Loss:11.186215609601083
 Iter:5 - Alpha:0.047 - Batch 203/249 - Min Loss:11.18 - Loss:11.18585694662056
 Iter:5 - Alpha:0.047 - Batch 205/249 - Min Loss:11.18 - Loss:11.183786895050535
 Iter:5 - Alpha:0.047 - Batch 206/249 - Min Loss:11.17 - Loss:11.179301309190508
 Iter:5 - Alpha:0.047 - Batch 207/249 - Min Loss:11.17 - Loss:11.176488105796423
 Iter:5 - Alpha:0.047 - Batch 208/249 - Min Loss:11.16 - Loss:11.16998610985578
 Iter:5 - Alpha:0.047 - Batch 209/249 - Min Loss:11.16 - Loss:11.162463575946273
 Iter:5 - Alpha:0.047 - Batch 210/249 - Min Loss:11.15 - Loss:11.155308867982049
 Iter:5 - Alpha:0.047 - Batch 211/249 - Min Loss:11.13 - Loss:11.136266258724019
 Iter:5 - Alpha:0.047 - Batch 212/249 - Min Loss:11.13 - Loss:11.130234130439776
 Iter:5 - Alpha:0.047 - Batch 213/249 - Min Loss:11.12 - Loss:11.123176314521636
 Iter:5 - Alpha:0.047 - Batch 214/249 - Min Loss:11.12 - Loss:11.120840694554088
 Iter:5 - Alpha:0.047 - Batch 215/249 - Min Loss:11.11 - Loss:11.110450434101342
 Iter:5 - Alpha:0.047 - Batch 217/249 - Min Loss:11.10 - Loss:11.103688330628701
 Iter:5 - Alpha:0.047 - Batch 218/249 - Min Loss:11.09 - Loss:11.096441322469296
 Iter:5 - Alpha:0.047 - Batch 223/249 - Min Loss:11.08 - Loss:11.086439649957739
 Iter:5 - Alpha:0.047 - Batch 224/249 - Min Loss:11.08 - Loss:11.082898614386089
 Iter:5 - Alpha:0.047 - Batch 225/249 - Min Loss:11.07 - Loss:11.077012105195037
 Iter:5 - Alpha:0.047 - Batch 226/249 - Min Loss:11.07 - Loss:11.073326538260593
 Iter:5 - Alpha:0.047 - Batch 227/249 - Min Loss:11.06 - Loss:11.068675672742167
 Iter:5 - Alpha:0.047 - Batch 249/249 - Min Loss:11.06 - Loss:11.081552878678924
 Iter:6 - Alpha:0.047 - Batch 1/249 - Min Loss:11.59 - Loss:11.597973696732243 - heres, and seesteres, and seesteres, and seesteres, and seesteres, and
 Iter:6 - Alpha:0.047 - Batch 2/249 - Min Loss:11.45 - Loss:11.450347821473407
 Iter:6 - Alpha:0.047 - Batch 3/249 - Min Loss:11.41 - Loss:11.418771097742296
 Iter:6 - Alpha:0.047 - Batch 4/249 - Min Loss:11.38 - Loss:11.381117731838811
 Iter:6 - Alpha:0.047 - Batch 25/249 - Min Loss:11.19 - Loss:11.259641659226372
 Iter:6 - Alpha:0.047 - Batch 34/249 - Min Loss:11.14 - Loss:11.143286302001574
 Iter:6 - Alpha:0.047 - Batch 35/249 - Min Loss:11.11 - Loss:11.110166237945121
 Iter:6 - Alpha:0.047 - Batch 36/249 - Min Loss:11.09 - Loss:11.09810755108634
 Iter:6 - Alpha:0.047 - Batch 37/249 - Min Loss:11.08 - Loss:11.088589748259052
 Iter:6 - Alpha:0.047 - Batch 101/249 - Min Loss:11.06 - Loss:11.068781178870975
 Iter:6 - Alpha:0.047 - Batch 102/249 - Min Loss:11.05 - Loss:11.050673855064446
 Iter:6 - Alpha:0.047 - Batch 103/249 - Min Loss:11.04 - Loss:11.045905269326687
 Iter:6 - Alpha:0.047 - Batch 104/249 - Min Loss:11.04 - Loss:11.040134059471336
 Iter:6 - Alpha:0.047 - Batch 105/249 - Min Loss:11.03 - Loss:11.03689865720998
 Iter:6 - Alpha:0.047 - Batch 106/249 - Min Loss:11.01 - Loss:11.017797878614465
 Iter:6 - Alpha:0.047 - Batch 107/249 - Min Loss:11.01 - Loss:11.016352707652024
 Iter:6 - Alpha:0.047 - Batch 130/249 - Min Loss:11.01 - Loss:11.022947147397362
 Iter:6 - Alpha:0.047 - Batch 131/249 - Min Loss:11.00 - Loss:11.007783207970942
 Iter:6 - Alpha:0.047 - Batch 132/249 - Min Loss:10.98 - Loss:10.98843334580342
 Iter:6 - Alpha:0.047 - Batch 133/249 - Min Loss:10.98 - Loss:10.986836850666375
 Iter:6 - Alpha:0.047 - Batch 135/249 - Min Loss:10.97 - Loss:10.976116641090075
 Iter:6 - Alpha:0.047 - Batch 137/249 - Min Loss:10.96 - Loss:10.964092181788672
 Iter:6 - Alpha:0.047 - Batch 138/249 - Min Loss:10.95 - Loss:10.950881825804778
 Iter:6 - Alpha:0.047 - Batch 152/249 - Min Loss:10.94 - Loss:10.953430037846198
 Iter:6 - Alpha:0.047 - Batch 153/249 - Min Loss:10.94 - Loss:10.945277883322511
 Iter:6 - Alpha:0.047 - Batch 192/249 - Min Loss:10.93 - Loss:10.947356709103463
 Iter:6 - Alpha:0.047 - Batch 197/249 - Min Loss:10.93 - Loss:10.937629168918932
 Iter:6 - Alpha:0.047 - Batch 198/249 - Min Loss:10.92 - Loss:10.928869991563216
 Iter:6 - Alpha:0.047 - Batch 199/249 - Min Loss:10.92 - Loss:10.92529456554725
 Iter:6 - Alpha:0.047 - Batch 201/249 - Min Loss:10.92 - Loss:10.920550668481589
 Iter:6 - Alpha:0.047 - Batch 202/249 - Min Loss:10.91 - Loss:10.916404992841002
 Iter:6 - Alpha:0.047 - Batch 203/249 - Min Loss:10.91 - Loss:10.916145261852648
 Iter:6 - Alpha:0.047 - Batch 205/249 - Min Loss:10.91 - Loss:10.916028403877382
 Iter:6 - Alpha:0.047 - Batch 206/249 - Min Loss:10.91 - Loss:10.912509604664942
 Iter:6 - Alpha:0.047 - Batch 207/249 - Min Loss:10.90 - Loss:10.909694867969744
 Iter:6 - Alpha:0.047 - Batch 208/249 - Min Loss:10.90 - Loss:10.903563446426856
 Iter:6 - Alpha:0.047 - Batch 209/249 - Min Loss:10.89 - Loss:10.896369123228224
 Iter:6 - Alpha:0.047 - Batch 210/249 - Min Loss:10.88 - Loss:10.888517506120522
 Iter:6 - Alpha:0.047 - Batch 211/249 - Min Loss:10.87 - Loss:10.871189601418905
 Iter:6 - Alpha:0.047 - Batch 212/249 - Min Loss:10.86 - Loss:10.865862301623107
 Iter:6 - Alpha:0.047 - Batch 213/249 - Min Loss:10.85 - Loss:10.858556656474592
 Iter:6 - Alpha:0.047 - Batch 214/249 - Min Loss:10.85 - Loss:10.856731747008148
 Iter:6 - Alpha:0.047 - Batch 215/249 - Min Loss:10.84 - Loss:10.84734398306333
 Iter:6 - Alpha:0.047 - Batch 217/249 - Min Loss:10.84 - Loss:10.845245300885306
 Iter:6 - Alpha:0.047 - Batch 218/249 - Min Loss:10.83 - Loss:10.839035759257246
 Iter:6 - Alpha:0.047 - Batch 219/249 - Min Loss:10.82 - Loss:10.825552412095542
 Iter:6 - Alpha:0.047 - Batch 221/249 - Min Loss:10.82 - Loss:10.825542421485585
 Iter:6 - Alpha:0.047 - Batch 222/249 - Min Loss:10.81 - Loss:10.819645931641812
 Iter:6 - Alpha:0.047 - Batch 223/249 - Min Loss:10.81 - Loss:10.814707576451124
 Iter:6 - Alpha:0.047 - Batch 224/249 - Min Loss:10.81 - Loss:10.811010087637642
 Iter:6 - Alpha:0.047 - Batch 225/249 - Min Loss:10.80 - Loss:10.804957508040596
 Iter:6 - Alpha:0.047 - Batch 226/249 - Min Loss:10.80 - Loss:10.800522785343569
 Iter:6 - Alpha:0.047 - Batch 227/249 - Min Loss:10.79 - Loss:10.794763767417592
 Iter:6 - Alpha:0.047 - Batch 228/249 - Min Loss:10.79 - Loss:10.79133660216299
 Iter:6 - Alpha:0.047 - Batch 235/249 - Min Loss:10.79 - Loss:10.791267404818555
 Iter:6 - Alpha:0.047 - Batch 249/249 - Min Loss:10.78 - Loss:10.804802177668392
 Iter:7 - Alpha:0.046 - Batch 3/249 - Min Loss:10.95 - Loss:10.958138002915904 - herer, Thateres, and seest theres, and seest theres, and seest theres,
 Iter:7 - Alpha:0.046 - Batch 4/249 - Min Loss:10.91 - Loss:10.912158413953339
 Iter:7 - Alpha:0.046 - Batch 37/249 - Min Loss:10.81 - Loss:10.845710521162113
 Iter:7 - Alpha:0.046 - Batch 101/249 - Min Loss:10.81 - Loss:10.815927158513666
 Iter:7 - Alpha:0.046 - Batch 102/249 - Min Loss:10.80 - Loss:10.801599922161772
 Iter:7 - Alpha:0.046 - Batch 103/249 - Min Loss:10.79 - Loss:10.797403997724187
 Iter:7 - Alpha:0.046 - Batch 104/249 - Min Loss:10.79 - Loss:10.791964002711321
 Iter:7 - Alpha:0.046 - Batch 105/249 - Min Loss:10.78 - Loss:10.789082032551546
 Iter:7 - Alpha:0.046 - Batch 106/249 - Min Loss:10.77 - Loss:10.77262977482171
 Iter:7 - Alpha:0.046 - Batch 107/249 - Min Loss:10.77 - Loss:10.771765482758843
 Iter:7 - Alpha:0.046 - Batch 131/249 - Min Loss:10.76 - Loss:10.774375818563435
 Iter:7 - Alpha:0.046 - Batch 132/249 - Min Loss:10.75 - Loss:10.757656544589656
 Iter:7 - Alpha:0.046 - Batch 133/249 - Min Loss:10.75 - Loss:10.757439656006715
 Iter:7 - Alpha:0.046 - Batch 135/249 - Min Loss:10.74 - Loss:10.74578559884793
 Iter:7 - Alpha:0.046 - Batch 137/249 - Min Loss:10.73 - Loss:10.738444848209742
 Iter:7 - Alpha:0.046 - Batch 153/249 - Min Loss:10.72 - Loss:10.733584951882385
 Iter:7 - Alpha:0.046 - Batch 185/249 - Min Loss:10.72 - Loss:10.725213972248483
 Iter:7 - Alpha:0.046 - Batch 186/249 - Min Loss:10.72 - Loss:10.720002655168463
 Iter:7 - Alpha:0.046 - Batch 187/249 - Min Loss:10.71 - Loss:10.716043772091027
 Iter:7 - Alpha:0.046 - Batch 192/249 - Min Loss:10.70 - Loss:10.712271019995148
 Iter:7 - Alpha:0.046 - Batch 197/249 - Min Loss:10.70 - Loss:10.705776640082933
 Iter:7 - Alpha:0.046 - Batch 198/249 - Min Loss:10.69 - Loss:10.696172468204802
 Iter:7 - Alpha:0.046 - Batch 199/249 - Min Loss:10.69 - Loss:10.692679071384829
 Iter:7 - Alpha:0.046 - Batch 201/249 - Min Loss:10.68 - Loss:10.686741383464744
 Iter:7 - Alpha:0.046 - Batch 202/249 - Min Loss:10.68 - Loss:10.682114088442516
 Iter:7 - Alpha:0.046 - Batch 203/249 - Min Loss:10.68 - Loss:10.681021930751958
 Iter:7 - Alpha:0.046 - Batch 205/249 - Min Loss:10.67 - Loss:10.680475288982352
 Iter:7 - Alpha:0.046 - Batch 206/249 - Min Loss:10.67 - Loss:10.677022329608207
 Iter:7 - Alpha:0.046 - Batch 207/249 - Min Loss:10.67 - Loss:10.672824431768825
 Iter:7 - Alpha:0.046 - Batch 208/249 - Min Loss:10.66 - Loss:10.664322201316141
 Iter:7 - Alpha:0.046 - Batch 209/249 - Min Loss:10.65 - Loss:10.65685681422515
 Iter:7 - Alpha:0.046 - Batch 210/249 - Min Loss:10.64 - Loss:10.64912664278722
 Iter:7 - Alpha:0.046 - Batch 211/249 - Min Loss:10.63 - Loss:10.632799394208075
 Iter:7 - Alpha:0.046 - Batch 212/249 - Min Loss:10.62 - Loss:10.627907460921842
 Iter:7 - Alpha:0.046 - Batch 213/249 - Min Loss:10.62 - Loss:10.62063358210885
 Iter:7 - Alpha:0.046 - Batch 214/249 - Min Loss:10.61 - Loss:10.619918982994934
 Iter:7 - Alpha:0.046 - Batch 215/249 - Min Loss:10.61 - Loss:10.610925182846321
 Iter:7 - Alpha:0.046 - Batch 217/249 - Min Loss:10.60 - Loss:10.607207250963953
 Iter:7 - Alpha:0.046 - Batch 218/249 - Min Loss:10.60 - Loss:10.60160539057914
 Iter:7 - Alpha:0.046 - Batch 219/249 - Min Loss:10.58 - Loss:10.589719078889535
 Iter:7 - Alpha:0.046 - Batch 221/249 - Min Loss:10.58 - Loss:10.590980451266592
 Iter:7 - Alpha:0.046 - Batch 222/249 - Min Loss:10.58 - Loss:10.584968432681116
 Iter:7 - Alpha:0.046 - Batch 223/249 - Min Loss:10.57 - Loss:10.579886462866027
 Iter:7 - Alpha:0.046 - Batch 224/249 - Min Loss:10.57 - Loss:10.57619235909256
 Iter:7 - Alpha:0.046 - Batch 225/249 - Min Loss:10.56 - Loss:10.569902210237558
 Iter:7 - Alpha:0.046 - Batch 226/249 - Min Loss:10.56 - Loss:10.56536380209786
 Iter:7 - Alpha:0.046 - Batch 227/249 - Min Loss:10.55 - Loss:10.559292663208273
 Iter:7 - Alpha:0.046 - Batch 249/249 - Min Loss:10.55 - Loss:10.571853863175985
 Iter:8 - Alpha:0.046 - Batch 1/249 - Min Loss:10.57 - Loss:10.572758342133278 - heres, and seeres, and seeres, and seeres, and seeres, and seeres, and
 Iter:8 - Alpha:0.046 - Batch 4/249 - Min Loss:10.56 - Loss:10.582174496351056
 Iter:8 - Alpha:0.046 - Batch 200/249 - Min Loss:10.51 - Loss:10.515691862712735
 Iter:8 - Alpha:0.046 - Batch 201/249 - Min Loss:10.51 - Loss:10.515218097873685
 Iter:8 - Alpha:0.046 - Batch 202/249 - Min Loss:10.51 - Loss:10.510057847269692
 Iter:8 - Alpha:0.046 - Batch 203/249 - Min Loss:10.50 - Loss:10.508747905206338
 Iter:8 - Alpha:0.046 - Batch 206/249 - Min Loss:10.50 - Loss:10.505347378669848
 Iter:8 - Alpha:0.046 - Batch 207/249 - Min Loss:10.50 - Loss:10.500805226866067
 Iter:8 - Alpha:0.046 - Batch 208/249 - Min Loss:10.49 - Loss:10.492010485700405
 Iter:8 - Alpha:0.046 - Batch 209/249 - Min Loss:10.48 - Loss:10.484308798765769
 Iter:8 - Alpha:0.046 - Batch 210/249 - Min Loss:10.47 - Loss:10.477023051869798
 Iter:8 - Alpha:0.046 - Batch 211/249 - Min Loss:10.46 - Loss:10.461178833897888
 Iter:8 - Alpha:0.046 - Batch 212/249 - Min Loss:10.45 - Loss:10.45588642646875
 Iter:8 - Alpha:0.046 - Batch 213/249 - Min Loss:10.44 - Loss:10.4496716892192
 Iter:8 - Alpha:0.046 - Batch 214/249 - Min Loss:10.44 - Loss:10.449592564468075
 Iter:8 - Alpha:0.046 - Batch 215/249 - Min Loss:10.44 - Loss:10.441406319762619
 Iter:8 - Alpha:0.046 - Batch 217/249 - Min Loss:10.43 - Loss:10.436393783056396
 Iter:8 - Alpha:0.046 - Batch 218/249 - Min Loss:10.43 - Loss:10.43036389040014
 Iter:8 - Alpha:0.046 - Batch 219/249 - Min Loss:10.41 - Loss:10.418624926777582
 Iter:8 - Alpha:0.046 - Batch 221/249 - Min Loss:10.41 - Loss:10.419791889450973
 Iter:8 - Alpha:0.046 - Batch 222/249 - Min Loss:10.41 - Loss:10.413381583912066
 Iter:8 - Alpha:0.046 - Batch 223/249 - Min Loss:10.40 - Loss:10.40631565750874
 Iter:8 - Alpha:0.046 - Batch 224/249 - Min Loss:10.40 - Loss:10.400804178283995
 Iter:8 - Alpha:0.046 - Batch 225/249 - Min Loss:10.39 - Loss:10.394420754827483
 Iter:8 - Alpha:0.046 - Batch 226/249 - Min Loss:10.39 - Loss:10.39019169384537
 Iter:8 - Alpha:0.046 - Batch 227/249 - Min Loss:10.38 - Loss:10.384406108412323
 Iter:8 - Alpha:0.046 - Batch 228/249 - Min Loss:10.38 - Loss:10.381457006837918
 Iter:8 - Alpha:0.046 - Batch 233/249 - Min Loss:10.38 - Loss:10.386117719638854
 Iter:8 - Alpha:0.046 - Batch 234/249 - Min Loss:10.37 - Loss:10.378070488719848
 Iter:8 - Alpha:0.046 - Batch 235/249 - Min Loss:10.37 - Loss:10.370196897537925
 Iter:8 - Alpha:0.046 - Batch 236/249 - Min Loss:10.36 - Loss:10.362743764476646
 Iter:8 - Alpha:0.046 - Batch 237/249 - Min Loss:10.35 - Loss:10.35515501437382
 Iter:8 - Alpha:0.046 - Batch 240/249 - Min Loss:10.34 - Loss:10.353640061613337
 Iter:8 - Alpha:0.046 - Batch 249/249 - Min Loss:10.34 - Loss:10.354719040189485
 Iter:9 - Alpha:0.045 - Batch 1/249 - Min Loss:10.34 - Loss:10.34364788168377 - heres, and seer That ther ther ther ther ther ther ther ther ther ther
 Iter:9 - Alpha:0.045 - Batch 218/249 - Min Loss:10.31 - Loss:10.313500662931768
 Iter:9 - Alpha:0.045 - Batch 221/249 - Min Loss:10.30 - Loss:10.303650479027542
 Iter:9 - Alpha:0.045 - Batch 222/249 - Min Loss:10.29 - Loss:10.296436311213158
 Iter:9 - Alpha:0.045 - Batch 223/249 - Min Loss:10.28 - Loss:10.287924917883068
 Iter:9 - Alpha:0.045 - Batch 224/249 - Min Loss:10.28 - Loss:10.281111397360338
 Iter:9 - Alpha:0.045 - Batch 225/249 - Min Loss:10.27 - Loss:10.275890389709376
 Iter:9 - Alpha:0.045 - Batch 226/249 - Min Loss:10.27 - Loss:10.272418221484491
 Iter:9 - Alpha:0.045 - Batch 235/249 - Min Loss:10.26 - Loss:10.273821973377212
 Iter:9 - Alpha:0.045 - Batch 236/249 - Min Loss:10.26 - Loss:10.266560342950875
 Iter:9 - Alpha:0.045 - Batch 237/249 - Min Loss:10.26 - Loss:10.262050545877333
 Iter:9 - Alpha:0.045 - Batch 248/249 - Min Loss:10.26 - Loss:10.269939957386935
 Iter:9 - Alpha:0.045 - Batch 249/249 - Min Loss:10.25 - Loss:10.259759471622285



#训练之后，进行预测
def generate_sample(n=30, init_char=' '):
    s = ""
    hidden = model.init_hidden(batch_size=1)
    input = Tensor(np.array([word2index[init_char]]))
    for i in range(n):
        rnn_input = embed.forward(input)
        output, hidden = model.forward(input=rnn_input, hidden=hidden)
        output.data *= 15
        temp_dist = output.softmax()
        temp_dist /= temp_dist.sum()

#         m = (temp_dist > np.random.rand()).argmax() # sample from predictions
        m = output.data.argmax() # take the max prediction
        c = vocab[m]
        input = Tensor(np.array([m]))
        s += c
    return s
print(generate_sample(n=500, init_char='\n'))

#结果（因为训练次数只有10次，所以结果不太好。书中是训练了100次，结果看起来比较好）
And will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will w
​
```

#  第十五章：在看不见的数据上做深度学习：联邦学习导论

##  一、深度学习的隐私问题

1.深度学习作为机器学习的子领域，主要关于从数据中学习。

2.深度学习最主要的天然资源是训练数据（人工合成的或自然存在的）。没有数据的深度学习不能学习；并且因为最具有价值的用例经常与私人的数据集有关。

3.如果不把训练数据的语料集中到一个地方做训练，而是一生成就把它传到数据那边会有什么效果？---------这个问题就是机器学习的一个崭新的子领域----->联邦学习。

4.对于3中的问题，在医疗保健、个人管理和其它敏感领域的重要模型，可以在不必要求任何人披露他们自己的信息的情况下训练出来。

##  二、联邦学习

1.对于联邦学习，你不必访问数据集就能在它上面学习。

2.联邦学习主要讨论的问题是，如何让模型进入一个安全环境中并学习如何解决问题，与此同时，不需要把数据移动到任何地方。

##  三、学习检测垃圾邮件

1.Step_1_Plan Ole Fashioned Deep Learning (Email Spam Detection).ipynb------（在Enron（安然）数据集上，来训练检测垃圾邮件的模型）

```python
import numpy as np

class Tensor (object):
    
    def __init__(self,data,
                 autograd=False,
                 creators=None,
                 creation_op=None,
                 id=None):
        
        self.data = np.array(data)
        self.autograd = autograd
        self.grad = None

        if(id is None):
            self.id = np.random.randint(0,1000000000)
        else:
            self.id = id
        
        self.creators = creators
        self.creation_op = creation_op
        self.children = {}
        
        if(creators is not None):
            for c in creators:
                if(self.id not in c.children):
                    c.children[self.id] = 1
                else:
                    c.children[self.id] += 1

    def all_children_grads_accounted_for(self):
        for id,cnt in self.children.items():
            if(cnt != 0):
                return False
        return True 
        
    def backward(self,grad=None, grad_origin=None):
        if(self.autograd):
 
            if(grad is None):
                grad = Tensor(np.ones_like(self.data))

            if(grad_origin is not None):
                if(self.children[grad_origin.id] == 0):
                    return
                    print(self.id)
                    print(self.creation_op)
                    print(len(self.creators))
                    for c in self.creators:
                        print(c.creation_op)
                    raise Exception("cannot backprop more than once")
                else:
                    self.children[grad_origin.id] -= 1

            if(self.grad is None):
                self.grad = grad
            else:
                self.grad += grad
            
            # grads must not have grads of their own
            assert grad.autograd == False
            
            # only continue backpropping if there's something to
            # backprop into and if all gradients (from children)
            # are accounted for override waiting for children if
            # "backprop" was called on this variable directly
            if(self.creators is not None and 
               (self.all_children_grads_accounted_for() or 
                grad_origin is None)):

                if(self.creation_op == "add"):
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
                    
                if(self.creation_op == "sub"):
                    self.creators[0].backward(Tensor(self.grad.data), self)
                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)

                if(self.creation_op == "mul"):
                    new = self.grad * self.creators[1]
                    self.creators[0].backward(new , self)
                    new = self.grad * self.creators[0]
                    self.creators[1].backward(new, self)                    
                    
                if(self.creation_op == "mm"):
                    c0 = self.creators[0]
                    c1 = self.creators[1]
                    new = self.grad.mm(c1.transpose())
                    c0.backward(new)
                    new = self.grad.transpose().mm(c0).transpose()
                    c1.backward(new)
                    
                if(self.creation_op == "transpose"):
                    self.creators[0].backward(self.grad.transpose())

                if("sum" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.expand(dim,
                                                               self.creators[0].data.shape[dim]))

                if("expand" in self.creation_op):
                    dim = int(self.creation_op.split("_")[1])
                    self.creators[0].backward(self.grad.sum(dim))
                    
                if(self.creation_op == "neg"):
                    self.creators[0].backward(self.grad.__neg__())
                    
                if(self.creation_op == "sigmoid"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (self * (ones - self)))
                
                if(self.creation_op == "tanh"):
                    ones = Tensor(np.ones_like(self.grad.data))
                    self.creators[0].backward(self.grad * (ones - (self * self)))
                
                if(self.creation_op == "index_select"):
                    new_grad = np.zeros_like(self.creators[0].data)
                    indices_ = self.index_select_indices.data.flatten()
                    grad_ = grad.data.reshape(len(indices_), -1)
                    for i in range(len(indices_)):
                        new_grad[indices_[i]] += grad_[i]
                    self.creators[0].backward(Tensor(new_grad))
                    
                if(self.creation_op == "cross_entropy"):
                    dx = self.softmax_output - self.target_dist
                    self.creators[0].backward(Tensor(dx))
                    
    def __add__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data + other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="add")
        return Tensor(self.data + other.data)

    def __neg__(self):
        if(self.autograd):
            return Tensor(self.data * -1,
                          autograd=True,
                          creators=[self],
                          creation_op="neg")
        return Tensor(self.data * -1)
    
    def __sub__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data - other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="sub")
        return Tensor(self.data - other.data)
    
    def __mul__(self, other):
        if(self.autograd and other.autograd):
            return Tensor(self.data * other.data,
                          autograd=True,
                          creators=[self,other],
                          creation_op="mul")
        return Tensor(self.data * other.data)    

    def sum(self, dim):
        if(self.autograd):
            return Tensor(self.data.sum(dim),
                          autograd=True,
                          creators=[self],
                          creation_op="sum_"+str(dim))
        return Tensor(self.data.sum(dim))
    
    def expand(self, dim,copies):

        trans_cmd = list(range(0,len(self.data.shape)))
        trans_cmd.insert(dim,len(self.data.shape))
        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)
        
        if(self.autograd):
            return Tensor(new_data,
                          autograd=True,
                          creators=[self],
                          creation_op="expand_"+str(dim))
        return Tensor(new_data)
    
    def transpose(self):
        if(self.autograd):
            return Tensor(self.data.transpose(),
                          autograd=True,
                          creators=[self],
                          creation_op="transpose")
        
        return Tensor(self.data.transpose())
    
    def mm(self, x):
        if(self.autograd):
            return Tensor(self.data.dot(x.data),
                          autograd=True,
                          creators=[self,x],
                          creation_op="mm")
        return Tensor(self.data.dot(x.data))
    
    def sigmoid(self):
        if(self.autograd):
            return Tensor(1 / (1 + np.exp(-self.data)),
                          autograd=True,
                          creators=[self],
                          creation_op="sigmoid")
        return Tensor(1 / (1 + np.exp(-self.data)))

    def tanh(self):
        if(self.autograd):
            return Tensor(np.tanh(self.data),
                          autograd=True,
                          creators=[self],
                          creation_op="tanh")
        return Tensor(np.tanh(self.data))
    
    def index_select(self, indices):

        if(self.autograd):
            new = Tensor(self.data[indices.data],
                         autograd=True,
                         creators=[self],
                         creation_op="index_select")
            new.index_select_indices = indices
            return new
        return Tensor(self.data[indices.data])
    
    def softmax(self):
        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        return softmax_output
    
    def cross_entropy(self, target_indices):

        temp = np.exp(self.data)
        softmax_output = temp / np.sum(temp,
                                       axis=len(self.data.shape)-1,
                                       keepdims=True)
        
        t = target_indices.data.flatten()
        p = softmax_output.reshape(len(t),-1)
        target_dist = np.eye(p.shape[1])[t]
        loss = -(np.log(p) * (target_dist)).sum(1).mean()
    
        if(self.autograd):
            out = Tensor(loss,
                         autograd=True,
                         creators=[self],
                         creation_op="cross_entropy")
            out.softmax_output = softmax_output
            out.target_dist = target_dist
            return out

        return Tensor(loss)
        
    
    def __repr__(self):
        return str(self.data.__repr__())
    
    def __str__(self):
        return str(self.data.__str__())  

class Layer(object):
    
    def __init__(self):
        self.parameters = list()
        
    def get_parameters(self):
        return self.parameters

    
class SGD(object):
    
    def __init__(self, parameters, alpha=0.1):
        self.parameters = parameters
        self.alpha = alpha
    
    def zero(self):
        for p in self.parameters:
            p.grad.data *= 0
        
    def step(self, zero=True):
        
        for p in self.parameters:
            
            p.data -= p.grad.data * self.alpha
            
            if(zero):
                p.grad.data *= 0


class Linear(Layer):

    def __init__(self, n_inputs, n_outputs, bias=True):
        super().__init__()
        
        self.use_bias = bias
        
        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))
        self.weight = Tensor(W, autograd=True)
        if(self.use_bias):
            self.bias = Tensor(np.zeros(n_outputs), autograd=True)
        
        self.parameters.append(self.weight)
        
        if(self.use_bias):        
            self.parameters.append(self.bias)

    def forward(self, input):
        if(self.use_bias):
            return input.mm(self.weight)+self.bias.expand(0,len(input.data))
        return input.mm(self.weight)


class Sequential(Layer):
    
    def __init__(self, layers=list()):
        super().__init__()
        
        self.layers = layers
    
    def add(self, layer):
        self.layers.append(layer)
        
    def forward(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input
    
    def get_parameters(self):
        params = list()
        for l in self.layers:
            params += l.get_parameters()
        return params


class Embedding(Layer):
    
    def __init__(self, vocab_size, dim):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        
        # this random initialiation style is just a convention from word2vec
        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)
        
        self.parameters.append(self.weight)
    
    def forward(self, input):
        return self.weight.index_select(input)


class Tanh(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.tanh()


class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        return input.sigmoid()
    

class CrossEntropyLoss(object):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, input, target):
        return input.cross_entropy(target)

class MSELoss(object):
    
    def __init__(self):
        super().__init__()
    
    def forward(self, input, target):
        dif = input - target
        return (dif * dif).sum(0)
    
class RNNCell(Layer):
    
    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):
        super().__init__()

        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.n_output = n_output
        
        if(activation == 'sigmoid'):
            self.activation = Sigmoid()
        elif(activation == 'tanh'):
            self.activation == Tanh()
        else:
            raise Exception("Non-linearity not found")

        self.w_ih = Linear(n_inputs, n_hidden)
        self.w_hh = Linear(n_hidden, n_hidden)
        self.w_ho = Linear(n_hidden, n_output)
        
        self.parameters += self.w_ih.get_parameters()
        self.parameters += self.w_hh.get_parameters()
        self.parameters += self.w_ho.get_parameters()        
    
    def forward(self, input, hidden):
        from_prev_hidden = self.w_hh.forward(hidden)
        combined = self.w_ih.forward(input) + from_prev_hidden
        new_hidden = self.activation.forward(combined)
        output = self.w_ho.forward(new_hidden)
        return output, new_hidden
    
    def init_hidden(self, batch_size=1):
        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
    
class LSTMCell(Layer):
    
    def __init__(self, n_inputs, n_hidden, n_output):
        super().__init__()

        self.n_inputs = n_inputs
        self.n_hidden = n_hidden
        self.n_output = n_output

        self.xf = Linear(n_inputs, n_hidden)
        self.xi = Linear(n_inputs, n_hidden)
        self.xo = Linear(n_inputs, n_hidden)        
        self.xc = Linear(n_inputs, n_hidden)        
        
        self.hf = Linear(n_hidden, n_hidden, bias=False)
        self.hi = Linear(n_hidden, n_hidden, bias=False)
        self.ho = Linear(n_hidden, n_hidden, bias=False)
        self.hc = Linear(n_hidden, n_hidden, bias=False)        
        
        self.w_ho = Linear(n_hidden, n_output, bias=False)
        
        self.parameters += self.xf.get_parameters()
        self.parameters += self.xi.get_parameters()
        self.parameters += self.xo.get_parameters()
        self.parameters += self.xc.get_parameters()

        self.parameters += self.hf.get_parameters()
        self.parameters += self.hi.get_parameters()        
        self.parameters += self.ho.get_parameters()        
        self.parameters += self.hc.get_parameters()                
        
        self.parameters += self.w_ho.get_parameters()        
    
    def forward(self, input, hidden):
        
        prev_hidden = hidden[0]        
        prev_cell = hidden[1]
        
        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()
        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()
        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        
        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        
        c = (f * prev_cell) + (i * g)

        h = o * c.tanh()
        
        output = self.w_ho.forward(h)
        return output, (h, c)
    
    def init_hidden(self, batch_size=1):
        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)
        init_hidden.data[:,0] += 1
        init_cell.data[:,0] += 1
        return (init_hidden, init_cell)


import numpy as np
from collections import Counter
import random
import sys
np.random.seed(12345)

# dataset from http://www2.aueb.gr/users/ion/data/enron-spam/

import codecs
with codecs.open('F:\DL\Spam_Ham\spam.txt', "r",encoding='utf-8', errors='ignore') as fdata:
    raw = fdata.readlines()

vocab = set()
    
spam = list()
for row in raw:
    spam.append(set(row[:-2].split(" ")))
    for word in spam[-1]:
        vocab.add(word)
    
import codecs
with codecs.open('F:\DL\Spam_Ham\ham.txt', "r",encoding='utf-8', errors='ignore') as fdata:
    raw = fdata.readlines()

ham = list()
for row in raw:
    ham.append(set(row[:-2].split(" ")))
    for word in ham[-1]:
        vocab.add(word)
        
vocab.add("<unk>")

vocab = list(vocab)
w2i = {}
for i,w in enumerate(vocab):
    w2i[w] = i
    
def to_indices(input, l=500):
    indices = list()
    for line in input:
        if(len(line) < l):
            line = list(line) + ["<unk>"] * (l - len(line))
            idxs = list()
            for word in line:
                idxs.append(w2i[word])
            indices.append(idxs)
    return indices
            
spam_idx = to_indices(spam)
ham_idx = to_indices(ham)

train_spam_idx = spam_idx[0:-1000]
train_ham_idx = ham_idx[0:-1000]

test_spam_idx = spam_idx[-1000:]
test_ham_idx = ham_idx[-1000:]

train_data = list()
train_target = list()

test_data = list()
test_target = list()

for i in range(max(len(train_spam_idx),len(train_ham_idx))):
    train_data.append(train_spam_idx[i%len(train_spam_idx)])
    train_target.append([1])
    
    train_data.append(train_ham_idx[i%len(train_ham_idx)])
    train_target.append([0])
    
for i in range(max(len(test_spam_idx),len(test_ham_idx))):
    test_data.append(test_spam_idx[i%len(test_spam_idx)])
    test_target.append([1])
    
    test_data.append(test_ham_idx[i%len(test_ham_idx)])
    test_target.append([0])


#训练
def train(model, input_data, target_data, batch_size=500, iterations=5):
    
    criterion = MSELoss()
    optim = SGD(parameters=model.get_parameters(), alpha=0.01)
    
    n_batches = int(len(input_data) / batch_size)
    for iter in range(iterations):
        iter_loss = 0
        for b_i in range(n_batches):

            # padding token should stay at 0
            model.weight.data[w2i['<unk>']] *= 0 
            input = Tensor(input_data[b_i*batch_size:(b_i+1)*batch_size], autograd=True)
            target = Tensor(target_data[b_i*batch_size:(b_i+1)*batch_size], autograd=True)

            pred = model.forward(input).sum(1).sigmoid()
            loss = criterion.forward(pred,target)
            loss.backward()
            optim.step()

            iter_loss += loss.data[0] / batch_size

            sys.stdout.write("\r\tLoss:" + str(iter_loss / (b_i+1)))
        print()
    return model
    
    
#测试
def test(model, test_input, test_output):
    
    model.weight.data[w2i['<unk>']] *= 0 
    
    input = Tensor(test_input, autograd=True)
    target = Tensor(test_output, autograd=True)

    pred = model.forward(input).sum(1).sigmoid()
    return ((pred.data > 0.5) == target.data).mean()
    
    

model = Embedding(vocab_size=len(vocab), dim=1)
model.weight.data *= 0
criterion = MSELoss()
optim = SGD(parameters=model.get_parameters(), alpha=0.01)


for i in range(3):
    model = train(model, train_data, train_target, iterations=1)
    print("% Correct on Test Set: " + str(test(model, test_data, test_target)*100))
    
    
#结果（在测试集上达到99.45%的分类准确率）
Loss:0.037140416860871446
% Correct on Test Set: 98.65
	Loss:0.011258669226059114
% Correct on Test Set: 99.15
	Loss:0.008068268387986223
% Correct on Test Set: 99.45
```

2.在1中是普通的深度学习，下面开始介绍隐私保护。

```python
#续1中代码

#模拟多个不同邮件集合的联邦学习环境
bob = (train_data[0:1000], train_target[0:1000])
alice = (train_data[1000:2000], train_target[1000:2000])
sue = (train_data[2000:], train_target[2000:])

model = Embedding(vocab_size=len(vocab), dim=1)
model.weight.data *= 0

for i in range(3):
    print("Starting Training Round...")
    print("\tStep 1: send the model to Bob")
    bob_model = train(copy.deepcopy(model), bob[0], bob[1], iterations=1)
    
    print("\n\tStep 2: send the model to Alice")
    alice_model = train(copy.deepcopy(model), alice[0], alice[1], iterations=1)
    
    print("\n\tStep 3: Send the model to Sue")
    sue_model = train(copy.deepcopy(model), sue[0], sue[1], iterations=1)
    
    print("\n\tAverage Everyone's New Models")
    model.weight.data = (bob_model.weight.data + \
                         alice_model.weight.data + \
                         sue_model.weight.data)/3
    
    print("\t% Correct on Test Set: " + \
          str(test(model, test_data, test_target)*100))
    
    print("\nRepeat!!\n")
    
#结果（模型学到与之前几乎同样的性能）
Starting Training Round...
	Step 1: send the model to Bob
	Loss:0.21908166249699718

	Step 2: send the model to Alice
	Loss:0.2937106899184867

	Step 3: Send the model to Sue
	Loss:0.033339966977175894

	Average Everyone's New Models
	% Correct on Test Set: 84.05

Repeat!!

Starting Training Round...
	Step 1: send the model to Bob
	Loss:0.06625367483630413

	Step 2: send the model to Alice
	Loss:0.09595374225556821

	Step 3: Send the model to Sue
	Loss:0.020290247881140743

	Average Everyone's New Models
	% Correct on Test Set: 92.25

Repeat!!

Starting Training Round...
	Step 1: send the model to Bob
	Loss:0.030819682914453826

	Step 2: send the model to Alice
	Loss:0.03580324891736099

	Step 3: Send the model to Sue
	Loss:0.015368461608470256

	Average Everyone's New Models
	% Correct on Test Set: 98.8

Repeat!!

```



##  四、深入联邦学习

1.联邦学习面临的量大挑战：性能和隐私。（可能会大部分时间会花在把模型来回传输上，只有相当少的时间花在训练上（尤其是当模型特别大的时候））

2.续三中代码 -------------------这里是来通过这个例子介绍如何在训练数据集上学习。

```python
import copy

bobs_email = ["my", "computer", "password", "is", "pizza"]

bob_input = np.array([[w2i[x] for x in bobs_email]])
bob_target = np.array([[0]])

model = Embedding(vocab_size=len(vocab), dim=1)
model.weight.data *= 0

bobs_model = train(copy.deepcopy(model), bob_input, bob_target, iterations=1, batch_size=1)

for i, v in enumerate(bobs_model.weight.data - model.weight.data):
    if(v != 0):
        print(vocab[i])
        
      
#结果：
	Loss:0.25
my
password
computer
pizza
is
```



##  五、安全聚合

1.通过合理否认的隐私保护：具体答案来自随机噪声而不是个人的随机性，通过允许人们做出合理否认，保护了他们的隐私。这构成了安全聚合的基础，并且更一般的，可以说是构成了差分隐私的大部分内容。

2.在联邦学习的场景下，你可以根据需要添加大量噪声，但这会影响训练效果。替代的方法是，首先通过某种方式把所有人的梯度求和，但是每个人只能看到他们自己的梯度。研究这种特性的一类问题叫做安全聚合；而为了实现他，需要一种工具：同态加密。

##  六、同态加密

1.同态加密可以让你不用解密就在加密的值上做运算。

2.omomorphic_Encryption_02.ipynb----------------同态加密例子（公匙让你加密数字；私匙让你解密数字；加密的值叫密文；未加密的值叫明文。）

```python
#pip install phe
import phe

public_key, private_key = phe.generate_paillier_keypair(n_length=1024)

# encrypt the number "5"
x = public_key.encrypt(5)

# encrypt the number "3"
y = public_key.encrypt(3)

# add the two encrypted values（把两个加密值相加）
z = x + y

# decrypt the result
z_ = private_key.decrypt(z)
print("The Answer: " + str(z_))

#结果
The Answer: 8
```

##  七、同态加密联邦学习

1.续三的1代码 ---------------------使用同态加密保护被聚合打的梯度

```python
#使用同态加密保护被聚合打的梯度

model = Embedding(vocab_size=len(vocab), dim=1)
model.weight.data *= 0

# note that in production the n_length should be at least 1024
public_key, private_key = phe.generate_paillier_keypair(n_length=128)

def train_and_encrypt(model, input, target, pubkey):
    new_model = train(copy.deepcopy(model), input, target, iterations=1)

    encrypted_weights = list()
    for val in new_model.weight.data[:,0]:
        encrypted_weights.append(public_key.encrypt(val))
    ew = np.array(encrypted_weights).reshape(new_model.weight.data.shape)
    
    return ew




for i in range(3):
    print("\nStarting Training Round...")
    print("\tStep 1: send the model to Bob")
    bob_encrypted_model = train_and_encrypt(copy.deepcopy(model), 
                                            bob[0], bob[1], public_key)

    print("\n\tStep 2: send the model to Alice")
    alice_encrypted_model = train_and_encrypt(copy.deepcopy(model), 
                                              alice[0], alice[1], public_key)

    print("\n\tStep 3: Send the model to Sue")
    sue_encrypted_model = train_and_encrypt(copy.deepcopy(model), 
                                            sue[0], sue[1], public_key)

    print("\n\tStep 4: Bob, Alice, and Sue send their")
    print("\tencrypted models to each other.")
    aggregated_model = bob_encrypted_model + \
                       alice_encrypted_model + \
                       sue_encrypted_model

    print("\n\tStep 5: only the aggregated model")
    print("\tis sent back to the model owner who")
    print("\t can decrypt it.")
    raw_values = list()
    for val in sue_encrypted_model.flatten():
        raw_values.append(private_key.decrypt(val))
    model.weight.data = np.array(raw_values).reshape(model.weight.data.shape)/3

    print("\t% Correct on Test Set: " + \
              str(test(model, test_data, test_target)*100))
    
    
#结果（Alice、Bob、Sue先把他们同态加密过得模型相加，然后发回。）
Starting Training Round...
	Step 1: send the model to Bob
	Loss:0.21908166249699718

	Step 2: send the model to Alice
	Loss:0.2937106899184867

	Step 3: Send the model to Sue
	Loss:0.033339966977175894

	Step 4: Bob, Alice, and Sue send their
	encrypted models to each other.

	Step 5: only the aggregated model
	is sent back to the model owner who
	 can decrypt it.
	% Correct on Test Set: 98.75

Starting Training Round...
	Step 1: send the model to Bob
	Loss:0.06366414053035604

	Step 2: send the model to Alice
	Loss:0.06100035791351305

	Step 3: Send the model to Sue
	Loss:0.025483920416627266

	Step 4: Bob, Alice, and Sue send their
	encrypted models to each other.

	Step 5: only the aggregated model
	is sent back to the model owner who
	 can decrypt it.
	% Correct on Test Set: 99.05000000000001

Starting Training Round...
	Step 1: send the model to Bob
	Loss:0.058477976535441636

	Step 2: send the model to Alice
	Loss:0.05987976552444443

	Step 3: Send the model to Sue
	Loss:0.024763428511034752

	Step 4: Bob, Alice, and Sue send their
	encrypted models to each other.

	Step 5: only the aggregated model
	is sent back to the model owner who
	 can decrypt it.
	% Correct on Test Set: 99.15
```

2.联邦学习是深度学习最重要的突破之一。它会在未来几年改变深度学习的图景。它会解锁之前因为过于敏感而不能使用的新数据集，并通过由此产生的新的创业机会，创造巨大的社会效益。这是加密算法与人工智能研究之间更广泛融合的一部分。

#  第十六章：往哪里去：简要指引

第一步：开始学习Pytorch

- 选用一门用这个框架讲授深度学习的深度学习课程（来调动之前熟悉的概念的记忆，同时向你展示它们在Pytorch的什么地方）
- https://py/torch.org/tutorials   和   https://github.com/pytorch/examples

第二步：开始另一个深度学习课程

- 找一些来自擅长深度学习教学的大学或AI实验室的在线课程。

第三步：找一本偏数学的深度学习建材

- Deep Learning (MIT出版社，2016年)

第四步：开设博客讲授深度学习

第五步：Twitter

第六步：实现学术论文

第七步：设法使用GPU

- GPU可以让训练速度加快10~100倍
- Google Colab notebooks 上可以访问免费的K80s（但是需要外网）

第八步：成为从业者

第九步：参与开源项目

第十步：发展你的本地社区
