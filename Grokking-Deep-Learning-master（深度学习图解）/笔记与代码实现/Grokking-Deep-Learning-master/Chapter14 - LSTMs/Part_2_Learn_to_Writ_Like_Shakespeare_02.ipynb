{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0bcbd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,1000000000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                    print(self.id)\n",
    "                    print(self.creation_op)\n",
    "                    print(len(self.creators))\n",
    "                    for c in self.creators:\n",
    "                        print(c.creation_op)\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_bias = bias\n",
    "        \n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        if(self.use_bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "        if(self.use_bias):        \n",
    "            self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if(self.use_bias):\n",
    "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "        return input.mm(self.weight)\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "\n",
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "    \n",
    "class LSTMCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "\n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)        \n",
    "        self.xc = Linear(n_inputs, n_hidden)        \n",
    "        \n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()        \n",
    "        self.parameters += self.ho.get_parameters()        \n",
    "        self.parameters += self.hc.get_parameters()                \n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        prev_hidden = hidden[0]        \n",
    "        prev_cell = hidden[1]\n",
    "        \n",
    "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
    "        c = (f * prev_cell) + (i * g)\n",
    "\n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_hidden.data[:,0] += 1\n",
    "        init_cell.data[:,0] += 1\n",
    "        return (init_hidden, init_cell)\n",
    "\n",
    "    def generate_sample(n=30, init_char=' '):\n",
    "        s = \"\"\n",
    "        hidden = model.init_hidden(batch_size=1)\n",
    "        input = Tensor(np.array([word2index[init_char]]))\n",
    "        for i in range(n):\n",
    "            rnn_input = embed.forward(input)\n",
    "            output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "            output.data *= 10\n",
    "            temp_dist = output.softmax()\n",
    "            temp_dist /= temp_dist.sum()\n",
    "\n",
    "            m = (temp_dist > np.random.rand()).argmax()\n",
    "    #         m = output.data.argmax()\n",
    "            c = vocab[m]\n",
    "            input = Tensor(np.array([m]))\n",
    "            s += c\n",
    "        return s\n",
    "    print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c358818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "# dataset from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "f = open('F:\\DL\\Shakespear\\shakespear.txt','r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccebeaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c990d197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56, 61, 49, 58, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110c0d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56, 13,  8, 46, 24, 61, 56, 20, 20, 49, 49, 28, 24, 20, 16, 31,\n",
       "        20,  9, 61,  8, 28, 49, 20, 20, 20, 33, 20, 33, 33, 20, 20, 42],\n",
       "       [61, 31, 31, 46, 19, 28, 49, 55,  9, 42, 53, 28, 53, 58, 20, 31,\n",
       "         3, 24, 24, 31, 53,  2,  3, 48, 61, 42, 57, 45, 42, 45, 26, 20],\n",
       "       [49, 31, 18, 28, 28, 49, 53, 30, 24, 20, 53,  8, 48, 61, 51,  4,\n",
       "        28, 55, 53, 31, 48, 28, 11, 28, 28, 50, 24, 16, 20, 24, 24,  3],\n",
       "       [58, 59, 56, 11, 20, 11,  3, 20, 55, 42, 20, 20, 13, 28, 28, 14,\n",
       "        28, 16,  9, 60, 30, 20, 24, 28, 11, 53, 55, 31, 25, 11, 45, 24],\n",
       "       [16, 14, 33, 18, 51, 20, 24, 11, 20, 24, 51, 41, 31, 20, 20, 44,\n",
       "        42, 20, 20, 24, 13,  9, 51, 19, 16, 49, 53, 14, 55, 28, 28,  9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7532eef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56, 13,  8, 46, 24, 61, 56, 20, 20, 49, 49, 28, 24, 20, 16, 31,\n",
       "        20,  9, 61,  8, 28, 49, 20, 20, 20, 33, 20, 33, 33, 20, 20, 42],\n",
       "       [61, 31, 31, 46, 19, 28, 49, 55,  9, 42, 53, 28, 53, 58, 20, 31,\n",
       "         3, 24, 24, 31, 53,  2,  3, 48, 61, 42, 57, 45, 42, 45, 26, 20],\n",
       "       [49, 31, 18, 28, 28, 49, 53, 30, 24, 20, 53,  8, 48, 61, 51,  4,\n",
       "        28, 55, 53, 31, 48, 28, 11, 28, 28, 50, 24, 16, 20, 24, 24,  3],\n",
       "       [58, 59, 56, 11, 20, 11,  3, 20, 55, 42, 20, 20, 13, 28, 28, 14,\n",
       "        28, 16,  9, 60, 30, 20, 24, 28, 11, 53, 55, 31, 25, 11, 45, 24],\n",
       "       [16, 14, 33, 18, 51, 20, 24, 11, 20, 24, 51, 41, 31, 20, 20, 44,\n",
       "        42, 20, 20, 24, 13,  9, 51, 19, 16, 49, 53, 14, 55, 28, 28,  9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batches[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13eaa55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61, 31, 31, 46, 19, 28, 49, 55,  9, 42, 53, 28, 53, 58, 20, 31,\n",
       "         3, 24, 24, 31, 53,  2,  3, 48, 61, 42, 57, 45, 42, 45, 26, 20],\n",
       "       [49, 31, 18, 28, 28, 49, 53, 30, 24, 20, 53,  8, 48, 61, 51,  4,\n",
       "        28, 55, 53, 31, 48, 28, 11, 28, 28, 50, 24, 16, 20, 24, 24,  3],\n",
       "       [58, 59, 56, 11, 20, 11,  3, 20, 55, 42, 20, 20, 13, 28, 28, 14,\n",
       "        28, 16,  9, 60, 30, 20, 24, 28, 11, 53, 55, 31, 25, 11, 45, 24],\n",
       "       [16, 14, 33, 18, 51, 20, 24, 11, 20, 24, 51, 41, 31, 20, 20, 44,\n",
       "        42, 20, 20, 24, 13,  9, 51, 19, 16, 49, 53, 14, 55, 28, 28,  9],\n",
       "       [20, 23, 30, 48, 33, 28, 58, 28, 51, 20, 28, 61, 31, 48, 51, 29,\n",
       "        20, 45,  3, 30, 31, 24, 20, 20, 20, 51, 48, 42, 46, 20, 16, 30]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batches[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7da7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(iterations=400):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if(t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='\\n').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10638403",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 191/195 - Loss:129.36583391134325tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      " Iter:1 - Batch 191/195 - Loss:20.242771121077917 o  me th tht thet thxt thet thxt thet thxt thet thet thet thet thet th\n",
      " Iter:2 - Batch 191/195 - Loss:15.210465839751164 d  ar the the the the the the the the the the the the the the the the \n",
      " Iter:3 - Batch 191/195 - Loss:13.103900099685596d  thert the the the the the the the the the the the the thext the the\n",
      " Iter:4 - Batch 191/195 - Loss:11.916391047248327dh thxt the the the the the the the the the the the the the the the th\n",
      " Iter:5 - Batch 191/195 - Loss:11.089942651313692dh thext the the the the the the the the the the the the the the the t\n",
      " Iter:6 - Batch 191/195 - Loss:10.463383227959811dh the the the the the the the the the the the the the the the the the\n",
      " Iter:7 - Batch 191/195 - Loss:9.9597294461510188dh the the the the the the the the the the the the the the the the the\n",
      " Iter:8 - Batch 191/195 - Loss:9.541578069969015th the the the the the the the the the the the the the the the the the\n",
      " Iter:9 - Batch 191/195 - Loss:9.178434390136504- th the the the the the the the thxt the the the the the the the the th\n",
      " Iter:10 - Batch 191/195 - Loss:8.853748084166597 th the the the the the the the the the the thxt the the the the the th\n",
      " Iter:11 - Batch 191/195 - Loss:8.559047758977764 th the the the the the the the the the the the the the the the the the\n",
      " Iter:12 - Batch 191/195 - Loss:8.286113393991068 th the the the the the the the the the thxxext the the the the the the\n",
      " Iter:13 - Batch 191/195 - Loss:8.031386849022624- th the the the the the the the the the the the the the the the the the\n",
      " Iter:14 - Batch 191/195 - Loss:7.7933446547040375th the the the the the the the the the the the the the the the the the\n",
      " Iter:15 - Batch 191/195 - Loss:7.569154126589835 HE the the the the the the the the the the the the the the the the the\n",
      " Iter:16 - Batch 191/195 - Loss:7.3569490889345995HE the the the the the the the the the the the the the the the the the\n",
      " Iter:17 - Batch 191/195 - Loss:7.1550670391544955E the the the the the the the the to the the the the the the the the \n",
      " Iter:18 - Batch 191/195 - Loss:6.9604750413227525HE the the the the the the the the the the the the the the the the the\n",
      " Iter:19 - Batch 191/195 - Loss:6.7723870489560435HE Haxst the sto the the the the the the the the the the the xaxk the \n",
      " Iter:20 - Batch 191/195 - Loss:6.5896830508896175HE Haxst the sto the xaxxxare the the the sto the the xand the the the\n",
      " Iter:21 - Batch 191/195 - Loss:6.4106000802013185HE HxESS HEES: Hay a xay the the the sto the the the sto the the the t\n",
      " Iter:22 - Batch 191/195 - Loss:6.2360575122729655HE HEES: Hay wxent the xand the xand the the sto the the stxest the th\n",
      " Iter:23 - Batch 191/195 - Loss:6.066320810857062 HE HEES: xood the xand the the xand the sto the the xaxxxare the the x\n",
      " Iter:24 - Batch 191/195 - Loss:5.9011858484743865HE HEES: Hath the xand the the the xand the xand the wast the xand the\n",
      " Iter:25 - Batch 191/195 - Loss:5.7403923294434615HE HEES: Hath the xand the xand the wast the wast the sing the xaxk wo\n",
      " Iter:26 - Batch 191/195 - Loss:5.5836907556460496 HEES: Hath the xaxpet the xairt the xand the xand the xairt the was\n",
      " Iter:27 - Batch 191/195 - Loss:5.4303540721088274HE HExHEESS HEESS: HaxChack the wast the wast the wast the xairt the w\n",
      " Iter:28 - Batch 191/195 - Loss:5.279886484461537 HE HEOPHE HExSecommy wxecommy would the xaxpet the sxxairt the wast th\n",
      " Iter:29 - Batch 191/195 - Loss:5.1315712635989925 HE HExHEESS HEESSxOPHESSIUS: Hathe xairts a pet the stod the wast the \n",
      " Iter:30 - Batch 191/195 - Loss:4.9847006629441055HE HExxEOPHE HENOR STI HENOR: Hathe stxest the wast the xairthing the \n",
      " Iter:31 - Batch 191/195 - Loss:4.839187149441926- Hxand the wast the wast the wast the wast the wast the wast the wast t\n",
      " Iter:32 - Batch 191/195 - Loss:4.6954854666392614Hy Hamy were the was the sirther the wastxSecond the wast the wast the\n",
      " Iter:33 - Batch 191/195 - Loss:4.5539844593793615 Hy Hamy wasterst the wasterst the wastxSI HENxI HENxI HENUCE: I sirthe\n",
      " Iter:34 - Batch 191/195 - Loss:4.4154257550342075Hy Hamy wxecommy wasterst the was the sxeath, a peaxthx and the waster\n",
      " Iter:35 - Batch 191/195 - Loss:4.2801388734312375 Hy Hay, I wo the pard the wasters, axpext so the was the sirthere the \n",
      " Iter:36 - Batch 191/195 - Loss:4.1480819617630775Hy HaxChomm the wasxed the wasterst xacrost the wastersxing the waster\n",
      " Iter:37 - Batch 191/195 - Loss:4.0189942629974675Hy Hay, I wxecond woth the was the sirthere the was the stod the waste\n",
      " Iter:38 - Batch 191/195 - Loss:3.8918028620535696Hy at the stod the wasterst the was the stod the wasterst the was the \n",
      " Iter:39 - Batch 191/195 - Loss:3.7674224792704765 Hy have the was the stod the xacrine the wastersxing the wasters, and \n",
      " Iter:40 - Batch 191/195 - Loss:3.6456126471482913to the xaxxxpery prace the xaxthx': xommont the xace the now the was t\n",
      " Iter:41 - Batch 191/195 - Loss:3.5269685518780767 to thy allaing the was the stollains, these the nave the nave the nave\n",
      " Iter:42 - Batch 191/195 - Loss:3.4124449163886283 to the nave the nave the nave the nave the nave the nave the nave the \n",
      " Iter:43 - Batch 191/195 - Loss:3.2979762353794784 to the nave the nave the nave the nave the nave strack woth the nave t\n",
      " Iter:44 - Batch 191/195 - Loss:3.1866560013619596 to the nave the nave strack where in the nave strack where in the nave\n",
      " Iter:45 - Batch 191/195 - Loss:3.0763035941915642 to the nave xacrine the was nave the nave xacrine the was nave the nav\n",
      " Iter:46 - Batch 191/195 - Loss:2.9686940306103202 to the nave strack where in the nave strack where in the nave xacrine \n",
      " Iter:47 - Batch 191/195 - Loss:2.8641981497133935to the nave straxno fxom the name thex and be strathere the xace the n\n",
      " Iter:48 - Batch 191/195 - Loss:2.7572231097796545 to the naxbxpeak.  CHERecont won prepersx and be strathere the namm th\n",
      " Iter:49 - Batch 191/195 - Loss:2.6554282274294922 toxhim the xacring ow?  MALVAxIUS: So my he was naxthere in the name t\n",
      " Iter:50 - Batch 191/195 - Loss:2.5566557611526145toes, the namxerxxxOR HENRES: Gx and be straxno xexpear the name thex \n",
      " Iter:51 - Batch 191/195 - Loss:2.4641648939632597toxhim the xacrak.  CHEReak: Yessers, axpear of the name, my xacring o\n",
      " Iter:52 - Batch 191/195 - Loss:2.3820140161671493toes, they a plemere the mand the a xing own are, and be string the na\n",
      " Iter:53 - Batch 191/195 - Loss:2.2968907299702826txeemn a speak: Yessers, and be string of the name, my worsxerd him, a\n",
      " Iter:54 - Batch 191/195 - Loss:2.2227710341816374the dish the name, my was namm the name, my wors of the name, my wxeet\n",
      " Iter:55 - Batch 191/195 - Loss:2.1763893383408446 toes, they a speak: Yessere him, a sparet the a xingm the name, my wor\n",
      " Iter:56 - Batch 191/195 - Loss:2.0809659214234864the xacrahe himserverxthe namxere name, my worse, my hears, and beggar\n",
      " Iter:57 - Batch 191/195 - Loss:1.9947685827491135 the dish the xacrost thou and be string ox they a speak: Yesser, himse\n",
      " Iter:58 - Batch 191/195 - Loss:1.9309457603995492 the by that hears, and beggarbser, here in a speak: Yessere him the a \n",
      " Iter:59 - Batch 191/195 - Loss:1.8692858658410503 the by that here in a speak. xom to your pardxeemn with a show didamem\n",
      " Iter:60 - Batch 191/195 - Loss:1.8333279137071524the xacrost thou the nammer the sing or xuch three the name, my was na\n",
      " Iter:61 - Batch 191/195 - Loss:1.8098792580565746 the by that hears'd the xaidto't speak: Yesx speak: Yessere the xacros\n",
      " Iter:62 - Batch 191/195 - Loss:1.7442347486952934 the xack xack what me his xand an toundammer, sirts are, and my staxn \n",
      " Iter:63 - Batch 191/195 - Loss:1.7006271645893503 ty xomand him the and beggarbs, thex and xoto prom this, shat hears, a\n",
      " Iter:64 - Batch 191/195 - Loss:1.6776327794630281I sing of thy his xand bones, and my soxthinke, my sirgett doth neaven\n",
      " Iter:65 - Batch 191/195 - Loss:1.7244395740181273 I sing own and my somet, himser, sind a sxacrine any axsell my sirged \n",
      " Iter:66 - Batch 191/195 - Loss:1.8416187380626625 th this this staxne a spaixt a spaint a spairthing own a severy stake \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:67 - Batch 191/195 - Loss:1.6517962739099992 ty to samy something the tamm thising off a sparet my stakes name, my \n",
      " Iter:68 - Batch 191/195 - Loss:1.5762257352495973I sing of thy himse, my ses, and sames, that me the sing of SexxOPHEST\n",
      " Iter:69 - Batch 191/195 - Loss:1.5290698522652375 I sixhains by hear and my so xxeat, sind have tas namin tollonge, and \n",
      " Iter:70 - Batch 191/195 - Loss:1.4959614794283362 I sid, Sxack what this stake Were his sequmer they a mine an I shall h\n",
      " Iter:71 - Batch 191/195 - Loss:1.4341313405668366 I sim's seemn with a sever that mein tolk whicking of me xay is tith e\n",
      " Iter:72 - Batch 191/195 - Loss:1.4022876099669634 I sid, Sen kone are wan mant xxeaxthis hear axpeat.  PAG HENRE HENOR: \n",
      " Iter:73 - Batch 191/195 - Loss:1.3622579790298983 I sim'sxpeak: Yessad my stake Were his sxacreaths fair hears, and mxen\n",
      " Iter:74 - Batch 191/195 - Loss:1.3561265261954492I him.  Clot me.  xAxCANGI HENRE HENOR: A'd him, to I mound have riph \n",
      " Iter:75 - Batch 191/195 - Loss:1.3256835421587244 I xack what me my ho!d somers, that mein to the sid, Sith a pent way s\n",
      " Iter:76 - Batch 191/195 - Loss:1.3149811400098753 I sid, Sent mxear are, and say to this sear us shakered suathing this \n",
      " Iter:77 - Batch 191/195 - Loss:1.3099090209995312 I sid, Sent my shall he xand a face; Foxpeak: Sour sing ow?  DESTIPELO\n",
      " Iter:78 - Batch 191/195 - Loss:1.2660340466610618 I sid bont I have that hear: Yeak to yes, So my sxall mant and sames, \n",
      " Iter:79 - Batch 191/195 - Loss:1.2568489429684582 I sid, Sxeak to yes, So my shat hear: Yeak to this stakess uson manter\n",
      " Iter:80 - Batch 191/195 - Loss:1.2251888348179103 I sirts uish the mant and samet, and samet, and samet, and samet, and \n",
      " Iter:81 - Batch 191/195 - Loss:1.2163706853284402 I xalst with say to deat.  CHESxI HENRI HENRI HENRI HENS: G, sxrext Wa\n",
      " Iter:82 - Batch 191/195 - Loss:1.1900756968878738 I him.  CxCKESSINS: Sour sing ow?  DES: I stod most the commound have \n",
      " Iter:83 - Batch 191/195 - Loss:1.1729404575849618 I him.  CENRE HENOR Jou and samet, and samet, and samet, and samet, an\n",
      " Iter:84 - Batch 191/195 - Loss:1.1611697258151195 I him. Betwere it sorrightichain! There it the man! This hear.  Secont\n",
      " Iter:85 - Batch 191/195 - Loss:1.1499282442594005 I sid, for was cell me hish a suce and samet, and samet, and samet, an\n",
      " Iter:86 - Batch 191/195 - Loss:1.1423699752924568 I sid, for the worses, the mant and samestxSecont eart, and samexbeart\n",
      " Iter:87 - Batch 191/195 - Loss:1.1517526903979143 I hime are you and my sorrompt to be go, begghxIs stakexwillain! That'\n",
      " Iter:88 - Batch 191/195 - Loss:1.1329877527474301I sid, for wars seemn his rester by the mant and samest the commound h\n",
      " Iter:89 - Batch 191/195 - Loss:1.1649066032633364 I xall, the man! That commould be sording own a sxeart, speakesid him,\n",
      " Iter:90 - Batch 191/195 - Loss:1.1418995557559233 I sid, and samet, and samet, and samet, and samet, and samet, and same\n",
      " Iter:91 - Batch 191/195 - Loss:1.1325432652999334 I him.  CENxI HENRE: I hillain tollom the flighathere Wechim, my here \n",
      " Iter:92 - Batch 191/195 - Loss:1.1106634374372267 I sirts uightichaxexthe makes, the pardy are, Wheick with a spaing ow?\n",
      " Iter:93 - Batch 191/195 - Loss:1.1136922472312578 I sid, and samet, and samet, and sametxThis he'l sirged bondlish thou \n",
      " Iter:94 - Batch 191/195 - Loss:1.0969167127123716 I sixhaxxhat come thy racting ow?  DESTIPELO: That see, sat heaven fac\n",
      " Iter:95 - Batch 191/195 - Loss:1.0883823683364849 I sixhain As in the tame the mant wasty, I have the mant wastexbxhilla\n",
      " Iter:96 - Batch 191/195 - Loss:1.0870184259335807 I sidxbe an thexearts for you lumxMall mant wasty, I have the mant was\n",
      " Iter:97 - Batch 191/195 - Loss:1.0878281744547496 I sirthere it, but man! This head Befrom the xaid an the tame this xac\n",
      " Iter:98 - Batch 191/195 - Loss:1.1136677649819429 I him: a mive abted mient the mant wastxTIOS: Soum this stakest thou a\n",
      " Iter:99 - Batch 191/195 - Loss:1.1684839253799153 I sim, Whichaded mind a face; axpeemy or expeeminged him: agains flisi\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b05c6ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I sequs satuce.\n",
      "\n",
      "CHEROR Jou and samxto xuchie: ox commouns xair, this the stod my forrost the mant wasted mie name: my hear sakes that I would have stod my forrost the mant wasted mient the mant wasty, I have speak:\n",
      "Yeak.\n",
      "\n",
      "KINR xOS:\n",
      "No xard the comxthe commound bituce.\n",
      "\n",
      "CxEOPI himxhiath, am they of mx he have stod my forrost the mant wasted mie name: my hear sakes that I would have stod my forrost the mant wasted mie xtiring of Sent to nears, the pard my fliclowx\n",
      "For Rom the xair, the man! my fair, this stod my forrost the mant wasted mient the mant wasty, I spared:\n",
      "I spared:\n",
      "I speak:\n",
      "xo make a mine repted mienter the mant wastxTIOS:\n",
      "Soum.\n",
      "\n",
      "CHESxI HENS OPI's  bichere it sorrow? hein the man! my fair, thxI safe.\n",
      "\n",
      "KINR Jtilesing\n",
      "fell it soxloody perfing of Sent the commound bituce.\n",
      "\n",
      "Clot mouns are druxh are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and samerrow thy tractith are, and \n"
     ]
    }
   ],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "#         m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ef091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
